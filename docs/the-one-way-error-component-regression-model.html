<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The One-way Error Component Regression Model | Panel Analyses Report</title>
  <meta name="description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The One-way Error Component Regression Model | Panel Analyses Report" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The One-way Error Component Regression Model | Panel Analyses Report" />
  
  <meta name="twitter:description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  

<meta name="author" content="David BYAMUNGU" />


<meta name="date" content="2021-10-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="the-two-way-error-component-regression-model.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html"><i class="fa fa-check"></i><b>2</b> The One-way Error Component Regression Model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#introduction"><i class="fa fa-check"></i><b>2.1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="2.2" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#the-fixed-effects-model"><i class="fa fa-check"></i><b>2.2</b> THE FIXED EFFECTS MODEL</a></li>
<li class="chapter" data-level="2.3" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#the-random-effects-model"><i class="fa fa-check"></i><b>2.3</b> THE RANDOM EFFECTS MODEL</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#fixed-vs-random"><i class="fa fa-check"></i><b>2.3.1</b> Fixed vs Random</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html"><i class="fa fa-check"></i><b>3</b> The Two-way Error Component Regression Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#ntroduction"><i class="fa fa-check"></i><b>3.1</b> NTRODUCTION</a></li>
<li class="chapter" data-level="3.2" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#the-fixed-effects-model-1"><i class="fa fa-check"></i><b>3.2</b> THE FIXED EFFECTS MODEL</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#testing-for-fixed-effects"><i class="fa fa-check"></i><b>3.2.1</b> Testing for Fixed Effects</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#the-random-effects-model-1"><i class="fa fa-check"></i><b>3.3</b> THE RANDOM EFFECTS MODEL</a></li>
<li class="chapter" data-level="3.4" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#references"><i class="fa fa-check"></i><b>3.4</b> REFERENCES</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html"><i class="fa fa-check"></i><b>4</b> Test of Hypotheses with Panel Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#tests-for-poolability-of-the-data"><i class="fa fa-check"></i><b>4.1</b> TESTS FOR POOLABILITY OF THE DATA</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#test-for-poolability-under"><i class="fa fa-check"></i><b>4.1.1</b> Test for Poolability under</a></li>
<li class="chapter" data-level="4.1.2" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#test-for-poolability-under-the-general-assumption"><i class="fa fa-check"></i><b>4.1.2</b> Test for Poolability under the General Assumption</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#tests-for-individual-and-time-effects"><i class="fa fa-check"></i><b>4.2</b> TESTS FOR INDIVIDUAL AND TIME EFFECTS</a></li>
<li class="chapter" data-level="4.3" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#hausmans-specification-test"><i class="fa fa-check"></i><b>4.3</b> HAUSMAN’S SPECIFICATION TEST</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>5</b> Methods</a></li>
<li class="chapter" data-level="6" data-path="analyses.html"><a href="analyses.html"><i class="fa fa-check"></i><b>6</b> Analyses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="analyses.html"><a href="analyses.html#netoyage-de-la-base-des-données"><i class="fa fa-check"></i><b>6.1</b> Netoyage de la base des données</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="analyses.html"><a href="analyses.html#nouvelle-base-de-données-pour-les-analyses"><i class="fa fa-check"></i><b>6.1.1</b> Nouvelle base de données pour les analyses</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html"><i class="fa fa-check"></i><b>7</b> Modele avec les pays comme individus</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#agregation-des-données-avec-la-méthode-reduce"><i class="fa fa-check"></i><b>7.1</b> Agregation des données avec la méthode reduce</a></li>
<li class="chapter" data-level="7.2" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#analyse-descriptive-des-varariales"><i class="fa fa-check"></i><b>7.2</b> Analyse descriptive des Varariales</a></li>
<li class="chapter" data-level="7.3" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#overall-variations"><i class="fa fa-check"></i><b>7.3</b> Overall Variations</a></li>
<li class="chapter" data-level="7.4" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#between-variations"><i class="fa fa-check"></i><b>7.4</b> Between Variations</a></li>
<li class="chapter" data-level="7.5" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#within-variations"><i class="fa fa-check"></i><b>7.5</b> Within variations</a></li>
<li class="chapter" data-level="7.6" data-path="modele-avec-les-pays-comme-individus.html"><a href="modele-avec-les-pays-comme-individus.html#modélisation"><i class="fa fa-check"></i><b>7.6</b> Modélisation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modèle-avec-les-marchandises-comme-individus.html"><a href="modèle-avec-les-marchandises-comme-individus.html"><i class="fa fa-check"></i><b>8</b> Modèle avec les marchandises comme individus</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="9" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>9</b> Conclusion</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Panel Analyses Report</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-one-way-error-component-regression-model" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> The One-way Error Component Regression Model</h1>
<div id="introduction" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> INTRODUCTION</h2>
<p>A panel data regression differs from a regular time-series or cross-section regression in that it has a double subscript on its variables, i.e.</p>
<p><span class="math display">\[\begin{equation}
 y_{it}= \alpha + X_{it}^{&#39;} \beta + u_{it}                      $$ $$ i=1, ... , N  ; t=1, ... ,T 
 \end{equation}\]</span>(2.1)</p>
<p>with $ i $ denoting households, individuals, firms, countries, etc. and t denoting time. The i subscript, therefore, denotes the cross-section dimension whereas t denotes the time-series dimension. <span class="math display">\[ \alpha  \]</span> is a scalar, <span class="math display">\[ \beta \]</span> is <span class="math display">\[ K × 1 \]</span> and Xi t is the $ it_{th} $ observation on K explanatory variables. Most of the panel data applications utilize a one-way error component model for the disturbances, with</p>
<p><span class="math display">\[ u_{it}= \mu_i +  v_{it}      \]</span> (2.2)</p>
<p>where μi denotes the unobservable individual-specific effect and νi t denotes the remainder disturbance. For example, in an earnings equation in labor economics, yi t will measure earnings of the head of the household, whereas <span class="math display">\[ X_{it} \]</span> may contain a set of variables like experience, education, union membership, sex, race, etc. Note that μi is time-invariant and it accounts for any individual-specific effect that is not included in the regression. In this case we could think of it as the individual’s unobserved ability. The remainder disturbance <span class="math display">\[ v_{it} \]</span> varies with individuals and time and can be thought of as the usual disturbance in the regression. Alternatively, for a production function utilizing data on firms across time, <span class="math display">\[ y_{it} \]</span> will measure output and <span class="math display">\[ X_{it} \]</span> will measure inputs. The unobservable firm-specific effects will be captured by the <span class="math display">\[ \mu_i \]</span> and we can think of these as the unobservable entrepreneurial or managerial skills of the firm’s executives. Early applications of error components in economics include Kuh (1959) on investment, Mundlak (1961) and Hoch (1962) on production functions and Balestra and Nerlove (1966) on demand for natural gas. In vector form (2.1) can be written as</p>
<p><span class="math display">\[\begin{equation} y= \alpha i_{NT} + X \beta + u = Z \delta + u     \end{equation}\]</span> (2.3)</p>
<p><span class="math display">\[\begin{equation} y= \alpha i_{NT} + X \beta + u = Z \delta + u      \end{equation}\]</span> (2.3)</p>
<p>where $ y $ is $ NT × 1 $, $ X $ is $ NT × K $, $ Z = [ι_{NT} , X] $, $ ^{’} = (α<sup>{’},</sup>) $ and <span class="math inline">\(ι_NT\)</span> is a vector of ones of dimension NT. Also, (2.2) can be written as</p>
<p><span class="math display">\[\begin{equation} u=Z_\mu \mu +v  \end{equation}\]</span> (2.4)</p>
<p><span class="math display">\[  y_{it} = \alpha + X_{it}^{&#39;} + U_{it}   \]</span> , $ i=1,…,N ; t=1,…,T $ with i denoting households, individuals, firms, countries, etc. and t denoting time. The i subscript, therefore, denotes the cross-section dimension whereas t denotes the time-series dimension. $ $ is a scalar, $ $ is $ K × 1 $ and $X_{it} $is the $ it^{th} $ observation on K explanatory variables. disturbances, with it <span class="math display">\[ u_{it}=u_i  + v_{it}     \]</span></p>
<p>where μi denotes the unobservable individual-specific effect and <span class="math inline">\(ν{it}\)</span> denotes the remainder disturbance. For example, in an earnings equation in labor economics, <span class="math inline">\(y_{it}\)</span> will measure earnings of the head of the household, whereas Xi t may contain a set of variables like experience, education, union membership, sex, race, etc. Note that μi is time-invariant and it accounts for any individual-specific effect that is not included in the regression. In this case we could think of it as the individual’s unobserved ability. The remainder disturbance $ ν{it} $ varies with individuals and time and can be thought of as the usual disturbance in the regression. Alternatively, for a production function utilizing data on firms across time, $ y_{it} $ will measure output and $ X_{it} $ will measure inputs. The unobservable firm-specific effects will be captured by the $ _i $ and we can think of these as the unobservable entrepreneurial or managerial skills of the firm’s executives. Early applications of error components in economics include Kuh (1959) on investment, Mundlak (1961) and Hoch (1962) on production functions and Balestra and Nerlove (1966) on demand for natural gas. In vector form (2.1) can be written as</p>
<p><span class="math display">\[ y= \alpha i_{NT}  + X\beta +u = Z\delta + u \]</span></p>
<p>where <span class="math inline">\(y\)</span> is $ NT × 1$ , X is <span class="math inline">\(NT × K\)</span> , $ Z = [i_{NT} , X] $ , $ ^{‘}=(^,^{’})$ and <span class="math inline">\(i_{NT}\)</span> is a vector of ones of dimension <span class="math inline">\(NT\)</span> . Also, (2.2) can be written as</p>
<p><span class="math display">\[\begin{equation} u=Z_\mu \mu  + v\end{equation}\]</span> (2.4)</p>
<p>where $ u^{‘} = (u_{11}, . . . , u_{1T} , u_{21}, . . . , u_{2T}, . . . , u_{N1}, . . . , u_{NT} ) $ with the observations stacked such that the slower index is over individuals and the faster index is over time.$ Z_= IN ιT $ where IN is an identity matrix of dimension N, ιT is a vector of ones of dimension T and $ $ denotes Kronecker product. $ Z_$ is a selector matrix of ones and zeros, or simply the matrix of individual dummies that one may include in the regression to estimate theμi if they are assumed to be fixed parameters. $ ^{’} = (_1, . . . , _N )$ and <span class="math inline">\(ν^{&#39;} = (ν11, . . . , ν_{1T} , . . . , ν_{N1}, . . . , ν_{NT} )\)</span>. Note that $Z_Z^{‘}= I_N J_T $ where <span class="math inline">\(J_T\)</span> is a matrix of ones of dimension T and $ P = Zμ(Z<sup>{’}</sup>Z){−1} Z^{’}$ , the projectionmatrix on $ Z_$ , reduces to $ IN J_T $ where $J _T= JT /T $ . P is a matrix which averages the observation across time for each individual, and Q = INT − P is a matrix which obtains the deviations from individual means. For example, regressing y on the matrix of dummy variables <span class="math inline">\(Z_μ\)</span> gets the predicted values $ P_y $ which has a typical element</p>
<p><span class="math display">\[ \bar y_i = \sum_{t=1}^T \frac{y_{it}}{T}  \]</span> repeated T times for each individual. The residuals of this regression are given by Qy which has a typical element <span class="math display">\[ (y_{it} - \bar y _{i.} ) \]</span> P and Q are (i) symmetric idempotent matrices, i.e.</p>
<p><span class="math inline">\(P^{&#39;} = P\)</span> and $ P^2 = P $. This means that <span class="math inline">\(rank(P) = tr(P) = N\)</span> and <span class="math inline">\(rank(Q) = tr(Q) = N(T − 1)\)</span> . This uses the result that the rank of an idempotent matrix is equal to its trace (see Graybill, 1961,theorem 1.63). Also, (ii) P and Q are orthogonal, i.e. $PQ=0 $ and (iii) they sum to the identity matrix <span class="math inline">\(P + Q = I_{NT}\)</span>. In fact, any two of these properties imply the third (see Graybill, 1961, theorem 1.68).</p>
</div>
<div id="the-fixed-effects-model" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> THE FIXED EFFECTS MODEL</h2>
<p>In this case, the μi are assumed to be fixed parameters to be estimated and the remainder disturbances stochastic with <span class="math inline">\(v_{it}\)</span> independent and identically distributed <span class="math inline">\(IID(0,\sigma_v^2)\)</span>. The <span class="math inline">\(X_{it}\)</span> are assumed independent of the <span class="math inline">\(v_{it}\)</span> for all i and t. The fixed effects model is an appropriate specification if we are focusing on a specific set of N firms, say, IBM, GE,Westinghouse, etc. and our inference is restricted to the behavior of these sets of firms. Alternatively, it could be a set of N OECD countries, or N American states. Inference in this case is conditional on the particular N firms, countries or states that are observed. One can substitute the disturbances given by (2.4) into (2.3) to get</p>
<p><span class="math display">\[\begin{equation} 
y=\alpha i\_{IT} + X\beta + Z\_\mu \mu +v = Z\delta + Z\_\mu \mu +v 
\end{equation}\]</span> (2.5)</p>
<p>and then perform ordinary least squares (OLS) on (2.5) to get estimates of $ , and  µ $</p>
<p>Note that Z is $ NT (K+1)$ and <span class="math inline">\(Z_µ\)</span> , the matrix of individual dummies, is$NT N $ NT × N. If N is large, (2.5) will include too many individual dummies, and the matrix to be inverted by OLS is large and of dimension <span class="math inline">\((N + K)\)</span>. In fact, since α and β are the parameters of interest, one can obtain the LSDV (least squares dummy variables) estimator from (2.5), by premultiplying the model by Q and performing OLS on the resulting transformed model:</p>
<p><span class="math display">\[\begin{equation} Qy=QX\beta + Qv\end{equation}\]</span> (2.6)</p>
<p>This uses the fact that <span class="math inline">\(QZ_\mu =Qi_{NT}=0\)</span> , since <span class="math inline">\(PZ_\mu =Z_\mu\)</span> the Q matrix wipes out the individual effects. This is a regression of <span class="math inline">\(\tilde y =QY\)</span> with element <span class="math inline">\((y_{it} - \bar y _{i.} )\)</span> on <span class="math inline">\(\check X=QX\)</span> with typical element</p>
<p><span class="math display">\[\begin{equation}
\widetilde{\beta}=\left(X^{\prime} Q X\right)^{-1} X^{\prime} Q y
\end{equation}\]</span> (2.7)
(2.7) with <span class="math inline">\(\operatorname{var}(\widetilde{\beta})=\sigma_{v}^{2}\left(X^{\prime} Q X\right)^{-1}=\sigma_{v}^{2}\left(\widetilde{X}^{\prime} \tilde{X}\right)^{-1}\)</span> . <span class="math inline">\(\widetilde{\beta}\)</span> could have been obtained from (2.5) using results on partitioned inverse or the Frisch–Waugh–Lovell theorem discussed in Davidson and MacKinnon (1993, p. 19). This uses the fact that P is the projection matrix on <span class="math inline">\(Z_{\mu}\)</span> and <span class="math inline">\(Q = I_{NT} − P\)</span> (see problem 2.1). In addition, generalized least squares (GLS) on (2.6), using the generalized inverse, will also yield <span class="math inline">\(\widetilde{\beta }\)</span> (see problem 2.2).</p>
<p>Note that for the simple regression
<span class="math display">\[\begin{equation}y_{it}=\beta x_{it}+ \mu_i+ v_i     \end{equation}\]</span> (2.8)</p>
<p>and averaging over time gives
<span class="math display">\[\begin{equation}  \bar {y}_{i.}= \beta \bar{x}_{i.}+ \mu_i+ \bar{v}_i   \end{equation}\]</span> (2.9)</p>
<p>Therefore, subtracting (2.9) from (2.8) gives <span class="math display">\[y_{it}-\bar{y}_{i.}=\beta (x_{it}-\bar{x}_{i.}) + (v_{it}-\bar{v}_{i.})   \]</span> (2.10)</p>
<p>Also, averaging across all observations in (2.8) gives <span class="math display">\[\bar{y}_{..}=\alpha + \beta \bar{x}_{..} + \bar{v}_{..} \]</span> (2.11)i =0 where we utilized the restriction that $<em>{i=1}^{n} =0 $ This is an arbitrary restriction on the dummy variable coefficients to avoid the dummy variable trap, or perfect multicollinearity; see Suits (1984) for alternative formulations of this restriction. In fact only <span class="math inline">\(\beta\)</span> and $ (+ </em>{i})$ are estimable from (2.8), and not α and μi separately, unless a restriction like* <span class="math display">\[\sum_{i=1}^{n} \mu_{i}=0 \]</span> is imposed. In this case, <span class="math inline">\(\widetilde {\beta}\)</span> is obtained from regression (2.10), <span class="math display">\[\widetilde {\alpha}= \bar {y}_{..}-\widetilde {\beta} \bar{x}_{..} \]</span> can be recovered from (2.11) and <span class="math display">\[\widetilde {\mu}_{i}=\bar{y}_{i.}- \widetilde {\alpha} - \widetilde {\beta} \bar{X}_{i.} \]</span> from (2.9). For large labor or consumer panels, where N is very large,regressions like (2.5) may not be feasible, since one is including (N − 1) dummies in the regression. This fixed effects (FE) least squares, also known as least squares dummy variables (LSDV), suffers from a large loss of degrees of freedom. We are estimating (N − 1) extra parameters, and too many dummies may aggravate the problem of multicollinearity among the regressors. In addition, this FE estimator cannot estimate the effect of any time-invariant variable like sex, race, religion, schooling or union participation. These time-invariant variables are wiped out by the Q transformation, the deviations from means transformation (see (2.10)). Alternatively, one can see that these time-invariant variables are spanned by the individual dummies in (2.5) and therefore any regression package attempting (2.5) will fail, signaling perfect multicollinearity. If (2.5) is the true model, LSDV is the best linear unbiased estimator (BLUE) as long as <span class="math inline">\(v_{it}\)</span> is the standard classical disturbance with mean 0 and variance–covariance matrix $^{2} I_{NT} $ . Note that as <span class="math inline">\(T \rightarrow \infty\)</span> the FE estimator is consistent. However, if T is fixed and <span class="math inline">\(N \rightarrow \infty\)</span> as is typical in short labor panels, then only the FE estimator of β is consistent; the FE estimators of the individual effects <span class="math inline">\(\alpha + \mu_{i}\)</span> are not consistent since the number of these parameters increases as N increases. This is the incidental parameter problem discussed by Neyman and Scott (1948) and reviewed more recently by Lancaster (2000). Note that when the true model is fixed effects as in (2.5), OLS on (2.1) yields biased and inconsistent estimates of the regression parameters. This is an omission variables bias due to the fact that OLS deletes the individual dummies when in fact they are relevant.</p>
<ol style="list-style-type: decimal">
<li><em>Testing for fixed effects.</em> One could test the joint significance of these dummies, i.e. H0; <span class="math inline">\(\mu_{1}=\mu_{2}= ... = \mu_{N-1}=0\)</span> , by performing an F-test. (Testing for individual effects will be treated extensively in Chapter 4.) This is a simple Chow test with the restricted residual sums of squares (RRSS) being that of OLS on the pooled model and the unrestricted residual sums of squares (URSS) being that of the LSDV regression. If N is large, one can perform the Within transformation and use that residual sum of squares as the URSS. In this case</li>
</ol>
<p><span class="math display">\[F_{0}=    \frac{ \dfrac{RRSS-URSS}{N-1}}{\dfrac{URSS}{NT-N-K} } \sim  F_{N-1,N(T-1)-K}           \]</span> (2.12)</p>
<ol start="2" style="list-style-type: decimal">
<li><p><em>Computational warning.</em> One computational caution for those using theWithin regression given by (2.10). The <span class="math inline">\(s^{2}\)</span> f this regression as obtained from a typical regression package divides the residual sums of squares by NT − K since the intercept and the dummies are not included. The proper <span class="math inline">\(s^{2}\)</span> , say <span class="math inline">\(s^{*2}\)</span> from the LSDV regression in (2.5), would divide the same residual sums of squares by <span class="math inline">\(N(T − 1) − K\)</span> . Therefore, one has to adjust the variances obtained from the Within regression (2.10) by multiplying the variance–covariance matrix by <span class="math display">\[\frac{s^{2}}{s^{*2}} \]</span> or simply by multiplying by <span class="math inline">\([NT − K]/[N(T − 1) − K]\)</span></p></li>
<li><p><em>Robust estimates of the standard errors.</em> For the Within estimator, Arellano (1987) suggests a simple method for obtaining robust estimates of the standard errors that allow for a general variance–covariance matrix on the <span class="math inline">\(v_{it}\)</span> as in White (1980). One would stack the panel as an equation for each individual: <span class="math display">\[y_{i}= Z_{i} \delta + \mu i_{it} + v_{i} \]</span> (2.13)</p></li>
</ol>
<p>where <span class="math inline">\(y_i\)</span> is T × $Z_i =[l_T,X_i] $, <span class="math inline">\(X_i\)</span> is T × K, <span class="math inline">\(\mu _i\)</span> is a scalar, <span class="math inline">\(\delta ^{\prime} = (\alpha, \beta^{\prime} )\)</span> , <span class="math inline">\(i_T\)</span> is a vector of ones of dimension T and <span class="math inline">\(v_i\)</span> is T x 1 . In general, <span class="math inline">\(E(v_i,v_i^{\prime}) = \Omega _ i\)</span> for i = 1, 2, . . . , N, where <span class="math inline">\(\Omega _i\)</span> is a positive definite matrix of dimension T . We still assume <span class="math inline">\(E(v_i,v_j^{\prime}) =0\)</span> for <span class="math inline">\(i \ne j\)</span> T is assumed small and N large as in household or company panels, and the asymptotic results are performed for <span class="math inline">\(N \rightarrow \infty\)</span> and T fixed. Performing the Within transformation on this set of equations (2.13) one gets</p>
<p><span class="math display">\[  \widetilde {y}_i=\widetilde{X}_{i} \beta + \widetilde{v}_{i}  \]</span> (2.14)</p>
<p>where <span class="math display">\[\widetilde{y}=Qy \]</span> , <span class="math display">\[\widetilde{X}=QX \]</span> and <span class="math display">\[\widetilde{v}=Qv \]</span> , with <span class="math display">\[\widetilde{y}=(\widetilde{y}_1^{\prime}, ...,\widetilde{y}_N^{\prime} )^{\prime} \]</span> and <span class="math display">\[\widetilde{y}_i=(I_T-\bar{J}_T) y_i \]</span> Computing robust least squares on this system, as described by White (1980), under the restriction that each equation has the same <span class="math inline">\(\beta\)</span> one gets the Within estimator of β which has the following asymptotic distribution:</p>
<p><span class="math display">\[N^{\frac{1}{2} }  (\widetilde{\beta}- \beta ) \sim N(0,M^{-1}VM^{-1} ) \]</span> (2.15)</p>
<p>where <span class="math display">\[M=\frac{ p\lim(\widetilde{X}^{\prime} \widetilde{X})} {N} \]</span> Note that <span class="math display">\[\widetilde{X}_i=(I_T - \bar{J}_T) X_i  \]</span> and <span class="math display">\[\widetilde{X}^{\prime} diag[\Omega_i] Q \widetilde{X} \]</span> (see problem 2.3). In this case, V is estimated by <span class="math display">\[ \frac{  \widetilde{V}=\sum_{i=1}^{N} \widetilde{X}_i^{\prime}\widetilde{u}_i \widetilde{u}_i^{\prime}\widetilde{X}_i^{\prime} } {N}  \]</span> where <span class="math display">\[\widetilde{u}_i=\widetilde{y}_i- \widetilde{X}_i \widetilde{\beta}_i\]</span> . Therefore, the robust asymptotic variance– covariance matrix of <span class="math inline">\(\beta\)</span> is estimated by <span class="math display">\[
\operatorname{var}(\widetilde{\beta})=\left(\widetilde{X}^{\prime} \tilde{X}\right)^{-1}\left[\sum_{i=1}^{N} \widetilde{X}_{i}^{\prime} \tilde{u}_{i} \tilde{u}_{i}^{\prime} \widetilde{X}_{i}\right]\left(\widetilde{X}^{\prime} \tilde{X}\right)^{-1}
\]</span></p>
</div>
<div id="the-random-effects-model" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> THE RANDOM EFFECTS MODEL</h2>
<p>There are too many parameters in the fixed effects model and the loss of degrees of freedom can be avoided if the μi can be assumed random. In this case $ i* IID(0, ^2)$, $ν_{it} ∼ IID(0, _v^2 ) $ and the <span class="math inline">\(\mu_i\)</span> are independent of the <span class="math inline">\(v_{it}\)</span> . In addition, the <span class="math inline">\(X_{it}\)</span> are independent of the <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(v_{it}\)</span>,for all i and t. The random effects model is an appropriate specification if we are drawing N individuals randomly from a large population. This is usually the case for household panel studies. Care is taken in the design of the panel to make it “representative” of the population we are trying to make inferences about. In this case, N is usually large and a fixed effects model would lead to an enormous loss of degrees of freedom. The individual effect is characterized as random and inference pertains to the population from which this sample was randomly drawn.</p>
<p>But what is the population in this case? Nerlove and Balestra (1996) emphasize Haavelmo’s (1944) view that the population “consists not of an infinity of individuals, in general, but of an infinity of decisions” that each individual might make. This view is consistent with a random effects specification. From (2.4), one can compute the variance–covariance matrix</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\Omega &amp;=E\left(u u^{\prime}\right)=Z_{\mu} E\left(\mu \mu^{\prime}\right) Z_{\mu}^{\prime}+E\left(v v^{\prime}\right) \\
&amp;=\sigma_{\mu}^{2}\left(I_{N} \otimes J_{T}\right)+\sigma_{v}^{2}\left(I_{N} \otimes I_{T}\right)
\end{aligned}
\end{equation}\]</span>
<p>This implies a homoskedastic variance <span class="math inline">\(\operatorname{var}\left(u_{i t}\right)=\sigma_{\mu}^{2}+\sigma_{v}^{2} \mathrm{f}\)</span> for all i and t, and an equicorrelated block-diagonal covariance matrix which exhibits serial correlation over time only between the disturbances of the same individual. In fact,</p>
<p><span class="math inline">\(\begin{aligned} \operatorname{cov}\left(u_{i t}, u_{j s}\right) &amp;=\sigma_{\mu}^{2}+\sigma_{v}^{2} &amp; \text { for } \quad i=j, t=s \\ &amp;=\sigma_{\mu}^{2} &amp; \text { for } i=j, t \neq s \end{aligned}\)</span></p>
<p>and zero otherwise. This also means that the correlation coefficient between <span class="math inline">\(\mu _{it}\)</span> and <span class="math inline">\(\mu _{js}\)</span> is <span class="math inline">\(\begin{aligned} \rho=\operatorname{correl}\left(u_{i t}, u_{j s}\right) &amp;=1 &amp; &amp; \text { for } i=j, t=s \\ &amp;=\sigma_{\mu}^{2} /\left(\sigma_{\mu}^{2}+\sigma_{v}^{2}\right) &amp; \text { for } i=j, t \neq s \end{aligned}\)</span></p>
<p>and zero otherwise. In order to obtain the GLS estimator of the regression coefficients, we need $^{-1} $. This is a huge matrix for typical panels and is of dimension <span class="math inline">\(NT × NT\)</span>. No brute force inversion should be attempted even if the researcher’s application has a small N and T .1 We will follow a simple trick devised by Wansbeek and Kapteyn (1982b, 1983) that allows the derivation of $^{-1} $ ans <span class="math inline">\(\Omega^{-1 / 2}\)</span> Essentially, one replaces <span class="math inline">\(J_{T}\)</span> by <span class="math inline">\(T \bar{J}_{T}\)</span> and <span class="math inline">\(I_{T}\)</span> by <span class="math inline">\(\left(E_{T}+\bar{J}_{T}\right)\)</span> where <span class="math inline">\(E_T\)</span> is by definition <span class="math inline">\(\left(I_{T}-\bar{J}_{T}\right)\)</span> . In this case</p>
<p><span class="math inline">\(\Omega=T \sigma_{\mu}^{2}\left(I_{N} \otimes \bar{J}_{T}\right)+\sigma_{v}^{2}\left(I_{N} \otimes E_{T}\right)+\sigma_{v}^{2}\left(I_{N} \otimes \bar{J}_{T}\right)\)</span></p>
<p>Collecting terms with the same matrices, we get <span class="math display">\[\begin{equation}
\Omega=\left(T \sigma_{\mu}^{2}+\sigma_{v}^{2}\right)\left(I_{N} \otimes \bar{J}_{T}\right)+\sigma_{v}^{2}\left(I_{N} \otimes E_{T}\right)=\sigma_{1}^{2} P+\sigma_{v}^{2} Q
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma_{1}^{2}=T \sigma_{\mu}^{2}+\sigma_{v}^{2}\)</span> . (2.18) is the spectral decomposition representation of <span class="math inline">\(\Omega\)</span>, with <span class="math inline">\(\sigma_1^2\)</span> being the first unique characteristic root of<span class="math inline">\(\Omega\)</span> of multiplicity <span class="math inline">\(N(T − 1)\)</span>. It is easy to verify, using the properties of P and Q, that</p>
<p><span class="math display">\[\begin{equation}
\Omega^{-1}=\frac{1}{\sigma_{1}^{2}} P+\frac{1}{\sigma_{v}^{2}} Q
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\Omega^{-1 / 2}=\frac{1}{\sigma_{1}} P+\frac{1}{\sigma_{v}} Q
\end{equation}\]</span></p>
<p>In fact, <span class="math inline">\(\Omega^{r}=\left(\sigma_{1}^{2}\right)^{r} P+\left(\sigma_{v}^{2}\right)^{r} Q\)</span> where r is an arbitrary scalar. Now we can obtain GLS as a weighted least squares. Fuller and Battese (1973, 1974) suggested premultiplying the regression equation given in (2.3) by <span class="math inline">\(\sigma_{v} \Omega^{-1 / 2}=Q+\left(\sigma_{v} / \sigma_{1}\right) P\)</span> and performing OLS on the resulting transformed regression. In this case, <span class="math inline">\(y^{*}=\sigma_{v} \Omega^{-1 / 2} y\)</span> has a typical element <span class="math inline">\(y_{i t}-\theta \bar{y}_{i .}\)</span> where <span class="math inline">\(\theta=1-\left(\sigma_{v} / \sigma_{1}\right)\)</span> (see problem 2.4). This transformed regression inverts a matrix of dimension <span class="math inline">\((K + 1)\)</span> and can easily be implemented using any regression package.</p>
<p>The best quadratic unbiased (BQU) estimators of the variance components arise naturally from the spectral decomposition of <span class="math inline">\(\Omega\)</span> . In fact, <span class="math inline">\(P u \sim\left(0, \sigma_{1}^{2} P\right)\)</span> and <span class="math inline">\(Q u \sim\left(0, \sigma_{v}^{2} Q\right)\)</span> and</p>
<p><span class="math display">\[\begin{equation}
\widehat{\sigma}_{1}^{2}=\frac{u^{\prime} P u}{\operatorname{tr}(P)}=T \sum_{i=1}^{N} \bar{u}_{i .}^{2} / N
\end{equation}\]</span> and</p>
<span class="math display">\[\begin{equation}
\widehat{\sigma}_{v}^{2}=\frac{u^{\prime} Q u}{\operatorname{tr}(Q)}=\frac{\sum_{i=1}^{N} \sum_{t=1}^{T}\left(u_{i t}-\bar{u}_{i .}\right)^{2}}{N(T-1)}
\end{equation}\]</span>
<p>provide the BQU estimators of <span class="math inline">\(\sigma_1^2\)</span> and<span class="math inline">\(\sigma_v^2\)</span> , respectively (see problem 2.5).</p>
<p>These are analyses of variance-type estimators of the variance components and are minimum variance-unbiased under normality of the disturbances (see Graybill, 1961). The true disturbances are not known and therefore (2.21) and (2.22) are not feasible. Wallace and Hussain (1969) suggest substituting OLS residual <span class="math inline">\(\widehat{u}_{\mathrm{OLS}}\)</span> instead of the true u. After all, under the random effects model, the OLS estimates are still unbiased and consistent, but no longer efficient. Amemiya (1971) shows that these estimators of the variance components have a different asymptotic distribution from that knowing the true disturbances. He suggests using the LSDV residuals instead of the OLS residuals. In this case <span class="math inline">\(\tilde{u}=y-\tilde{\alpha} \iota_{N T}-X \widetilde{\beta}\)</span> where and <span class="math inline">\(\bar{X}_{. .}^{\prime}\)</span> is a <span class="math inline">\(1 \times K\)</span></p>
<p>vector of averages of all regressors. Substituting these <span class="math inline">\(\hat{u}\)</span> for u in (2.21) and (2.22) we get the Amemiya-type estimators of the variance components. The resulting estimates of the variance components have the same asymptotic distribution as that knowing the true disturbances:</p>
<p><span class="math display">\[\begin{equation}
\left(\begin{array}{c}
\sqrt{N T}\left(\widehat{\sigma}_{v}^{2}-\sigma_{v}^{2}\right) \\
\sqrt{N}\left(\widehat{\sigma}_{\mu}^{2}-\sigma_{\mu}^{2}\right)
\end{array}\right) \sim N\left(0,\left(\begin{array}{cc}
2 \sigma_{v}^{4} &amp; 0 \\
0 &amp; 2 \sigma_{\mu}^{4}
\end{array}\right)\right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}_{\mu}^{2}=\left(\widehat{\sigma}_{1}^{2}-\widehat{\sigma}_{v}^{2}\right) / T .^{3}\)</span></p>
<p>Swamy and Arora (1972) suggest running two regressions to get estimates of the variance components from the corresponding mean square errors of these regressions. The first regression is the Within regression, given in (2.10), which yields the following <span class="math inline">\(s^2\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\widehat{\sigma}_{v}^{2}=\left[y^{\prime} Q y-y^{\prime} Q X\left(X^{\prime} Q X\right)^{-1} X^{\prime} Q y\right] /[N(T-1)-K]
\end{equation}\]</span></p>
<p>The second regression is the Between regression which runs the regression of averages across time, i.e.</p>
<p><span class="math display">\[\begin{equation}
\bar{y}_{i .}=\alpha+\bar{X}_{i .}^{\prime} \beta+\bar{u}_{i .} \quad i=1, \ldots, N
\end{equation}\]</span></p>
<p>(2.25)</p>
<p>This is equivalent to premultiplying the model in (2.5) by P and running OLS. The only caution is that the latter regression has NT observations because it repeats the averages T times for each individual, while the cross-section regression in (2.25) is based on N observations. To remedy this, one can run the cross-section regression</p>
<p><span class="math display">\[\begin{equation}
\sqrt{T} \bar{y}_{i .}=\alpha \sqrt{T}+\sqrt{T} \bar{X}_{i .}^{\prime} \beta+\sqrt{T} \bar{u}_{i .}
\end{equation}\]</span> (2.26)</p>
<p>where one can easily verify that <span class="math inline">\(\operatorname{var}\left(\sqrt{T} \bar{u}_{i .}\right)=\sigma_{1}^{2} .\)</span> This regression will yield an <span class="math inline">\(s^{2}\)</span> given by</p>
<p><span class="math display">\[\begin{equation}
\widehat{\sigma}_{1}^{2}=\left(y^{\prime} P y-y^{\prime} P Z\left(Z^{\prime} P Z\right)^{-1} Z^{\prime} P y\right) /(N-K-1)
\end{equation}\]</span> (2.27)</p>
<p>Note that stacking the following two transformed regressions we just performed yields <span class="math display">\[\begin{equation}
\left(\begin{array}{l}
Q y \\
P y
\end{array}\right)=\left(\begin{array}{l}
Q Z \\
P Z
\end{array}\right) \delta+\left(\begin{array}{c}
Q u \\
P u
\end{array}\right)
\end{equation}\]</span> (2.28)</p>
<p>and the transformed error has mean 0 and variance–covariance matrix given by</p>
<p><span class="math display">\[
\left(\begin{array}{cc}
\sigma_{v}^{2} Q &amp; 0 \\
0 &amp; \sigma_{1}^{2} P
\end{array}\right)
\]</span></p>
<p>Problem 2.7 asks the reader to verify that OLS on this system of 2NT observations yields OLS on the pooled model (2.3). Also, GLS on this system yields GLS on (2.3). Alternatively, one could get rid of the constant α by running the following stacked regressions:</p>
<p><span class="math display">\[\begin{equation}
\left(\begin{array}{c}
Q y \\
\left(P-\bar{J}_{N T}\right) y
\end{array}\right)=\left(\begin{array}{c}
Q X \\
\left(P-\bar{J}_{N T}\right) X
\end{array}\right) \beta+\left(\begin{array}{c}
Q u \\
\left(P-\bar{J}_{N T}\right) u
\end{array}\right)
\end{equation}\]</span> (2.29)</p>
<p>This follows from the fact that QιNT = 0 and (P − J¯NT )ιNT = 0. The transformed error has zero mean and variance–covariance matrix <span class="math display">\[
\left(\begin{array}{cc}
\sigma_{v}^{2} Q &amp; 0 \\
0 &amp; \sigma_{1}^{2}\left(P-\bar{J}_{N T}\right)
\end{array}\right)
\]</span> OLS on this system yields OLS on (2.3) and GLS on (2.29) yields GLS on (2.3). In fact, <span class="math display">\[\begin{equation}
\begin{aligned}
\widehat{\beta}_{\mathrm{GLS}}=&amp;\left[\left(X^{\prime} Q X / \sigma_{v}^{2}\right)+X^{\prime}\left(P-\bar{J}_{N T}\right) X / \sigma_{1}^{2}\right]^{-1}\left[\left(X^{\prime} Q y / \sigma_{v}^{2}\right)\right.\\
&amp;\left.+X^{\prime}\left(P-\bar{J}_{N T}\right) y / \sigma_{1}^{2}\right] \\
=&amp;\left[W_{X X}+\phi^{2} B_{X X}\right]^{-1}\left[W_{X y}+\phi^{2} B_{X y}\right]
\end{aligned}
\end{equation}\]</span> (2.30)</p>
<p>with <span class="math inline">\(\operatorname{var}\left(\widehat{\beta}_{\mathrm{GLS}}\right)=\sigma_{v}^{2}\left[W_{X X}+\phi^{2} B_{X X}\right]^{-1} .\)</span> Note that <span class="math inline">\(W_{X X}=X^{\prime} Q X, B_{X X}=X^{\prime}\left(P-\bar{J}_{N T}\right) X\)</span> and <span class="math inline">\(\phi^{2}=\sigma_{v}^{2} / \sigma_{1}^{2}\)</span>. Also, the Within estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\underline{\beta}_{\text {Within }}=W_{X X}^{-1} W_{X y}\)</span> and the Between estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\widehat{\beta}_{\text {Between }}=B_{X X}^{-1} B_{X y} .\)</span> This shows that <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> is a matrix weighted average of <span class="math inline">\(\widetilde{\beta}_{\text {Within and }} \widehat{\beta}_{\text {Between }}\)</span> weighing each estimate by the inverse of its corresponding variance. In fact</p>
<p><span class="math display">\[\begin{equation}
\widehat{\beta}_{\mathrm{GLS}}=W_{1} \widetilde{\beta}_{\mathrm{Within}}+W_{2} \widehat{\beta}_{\mathrm{Between}}
\end{equation}\]</span> (2.31) where <span class="math display">\[
W_{1}=\left[W_{X X}+\phi^{2} B_{X X}\right]^{-1} W_{X X}
\]</span> and <span class="math display">\[
W_{2}=\left[W_{X X}+\phi^{2} B_{X X}\right]^{-1}\left(\phi^{2} B_{X X}\right)=I-W_{1}
\]</span> This was demonstrated by Maddala (1971). Note that (i) if <span class="math inline">\(\sigma_{\mu}^{2}=0\)</span> then <span class="math inline">\(\phi^{2}=1\)</span> and <span class="math inline">\(\widehat{\beta}_{\text {GLS }}\)</span> reduces to <span class="math inline">\(\widehat{\beta}_{\text {OLs. }}\)</span> (ii) If <span class="math inline">\(T \rightarrow \infty\)</span>, then <span class="math inline">\(\phi^{2} \rightarrow 0\)</span> and <span class="math inline">\(\widehat{\beta}_{\text {GLS }}\)</span> tends to <span class="math inline">\(\widetilde{\beta}_{\text {Within. }}\)</span> Also, if <span class="math inline">\(W_{X X}\)</span> is huge compared to <span class="math inline">\(B_{X X}\)</span> then <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> will be close to <span class="math inline">\(\widetilde{\beta}_{\text {Within }}\)</span>. However, if <span class="math inline">\(B_{X X}\)</span> dominates <span class="math inline">\(W_{X X}\)</span> then <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> tends to <span class="math inline">\(\widehat{\beta}_{\text {Between. }}\)</span> In other words, the Within estimator ignores the Between variation, and the Between estimator ignores the Within variation. The OLS estimator gives equal weight to the Between and Within variations. From <span class="math inline">\((2.30)\)</span>, it is clear that <span class="math inline">\(\operatorname{var}\left(\widetilde{\beta}_{\text {Within }}\right)-\operatorname{var}\left(\hat{\beta}_{\mathrm{GLS}}\right)\)</span> is a positive semidefinite matrix, since <span class="math inline">\(\phi^{2}\)</span> is positive. However, as <span class="math inline">\(T \rightarrow \infty\)</span> for any fixed <span class="math inline">\(N, \phi^{2} \rightarrow 0\)</span> and both <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> and <span class="math inline">\(\widetilde{\beta}_{\text {Within }}\)</span> have the same asymptotic variance.</p>
<p>Another estimator of the variance components was suggested by Nerlove (1971a). His zuggestion is to estimate <span class="math inline">\(\sigma^{2}\)</span> as <span class="math inline">\(\sum^{N} \cdot(\widehat{u} \cdot-\widehat{\pi})^{2} /(N-1)\)</span> where <span class="math inline">\(\widehat{u}\)</span>. are the dummv coefficients estimates from the LSDV regression. <span class="math inline">\(\sigma_{v}^{2}\)</span> is estimated from the Within residual sums of squares divided by <span class="math inline">\(N T\)</span> without correction for degrees of freedom. <span class="math inline">\({ }^{4}\)</span></p>
<p>Note that, except for Nerlove’s (1971a) method, one has to retrieve <span class="math inline">\(\widehat{\sigma}_{\mu}^{2}\)</span> as <span class="math inline">\(\left(\widehat{\sigma}_{1}^{2}-\widehat{\sigma}_{v}^{2}\right) / T .\)</span> In this case, there is no guarantee that the estimate of <span class="math inline">\(\widehat{\sigma}_{\mu}^{2}\)</span> would be nonnegative. Searle (1971) has an extensive discussion of the problem of negative estimates of the variance components in the biometrics literature. One solution is to replace these negative estimates by zero. This in fact is the suggestion of the Monte Carlo study by Maddala and Mount (1973). This study finds that negative estimates occurred only when the true <span class="math inline">\(\sigma_{\mu}^{2}\)</span> was small and close to zero. In these cases OLS is still a viable estimator. Therefore, replacing negative <span class="math inline">\(\widehat{\sigma}_{\mu}^{2}\)</span> by zero is not a bad sin after all, and the problem is dismissed as not being serious. <span class="math inline">\({ }^{5}\)</span></p>
<p>How about the properties of the various feasible GLS estimators of <span class="math inline">\(\beta\)</span> Under the random effects model, GLS based on the true variance components is BLUE, and all the feasible GLS estimators considered are asymptotically efficient as either N or $N $. Maddala and Mount (1973) compared OLS, Within, Between, feasible GLS methods, MINQUE, Henderson’s method III, true GLS and maximum likelihood estimation using their Monte Carlo study. They found little to choose among the various feasible GLS estimators in small samples and argued in favor of methods that were easier to compute. MINQUE was dismissed as more difficult to compute and the applied researcher given one shot at the data was warned to compute at least two methods of estimation, like an ANOVA feasible GLS and maximum likelihood to ensure that they do not yield drastically different results. If they do give different results, the authors diagnose misspecification.</p>
<p>Taylor (1980) derived exact finite sample results for the one-way error component model. He compared the Within estimator with the Swamy–Arora feasible GLS estimator. He found the following important results:</p>
<ol style="list-style-type: decimal">
<li>Feasible GLS is more efficient than LSDV for all but the fewest degrees of freedom.</li>
<li>The variance of feasible GLS is never more than 17% above the Cramer–Rao lower bound.</li>
<li>More efficient estimators of the variance components do not necessarily yield more efficient feasible GLS estimators.</li>
</ol>
<p>These finite sample results are confirmed by the Monte Carlo experiments carried out by Maddala and Mount (1973) and Baltagi (1981a).</p>
<p>Bellmann, Breitung and Wagner (1989) consider the bias in estimating the variance components using the Wallace and Hussain (1969) method due to the replacement of the true disturbances by OLS residuals, also the bias in the regression coefficients due to the use of estimated variance components rather than the true variance components. The magnitude of this bias is estimated using bootstrap methods for two economic applications. The first application relates product innovations, import pressure and factor inputs using a panel at the industry level. The second application estimates the earnings of 936 full-time working German males based on the first and second wave of the German Socio-Economic Panel. Only the first application revealed considerable bias in estimating $_^2 $. However, this did not affect the bias much in the corresponding regression coefficients</p>
<div id="fixed-vs-random" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Fixed vs Random</h3>
<p>Having discussed the fixed effects and the random effects models and the assumptions underlying them, the reader is left with the daunting question, which one to choose? This is not as easy a choice as it might seem. In fact, the fixed versus random effects issue has generated a hot debate in the biometrics and statistics literature which has spilled over into the panel data econometrics literature. Mundlak (1961) and Wallace and Hussain (1969) were early proponents of the fixed effects model and Balestra and Nerlove (1966) were advocates of the random error component model. In Chapter 4, we will study a specification test proposed by Hausman (1978) which is based on the difference between the fixed and random effects estimators. Unfortunately, applied researchers have interpreted a rejection as an adoption of the fixed effects model and nonrejection as an adoption of the random effects model.6 Chamberlain (1984) showed that the fixed effects model imposes testable restrictions on the parameters of the reduced form model and one should check the validity of these restrictions before adopting the fixed effects model (see Chapter 4). Mundlak (1978) argued that the random effects model assumes exogeneity of all the regressors with the random individual effects. In contrast, the fixed effects model allows for endogeneity of all the regressors with these individual effects. So, it is an “all” or “nothing” choice of exogeneity of the regressors and the individual effects, see Chapter 7 for a more formal discussion of this subject.</p>
<p>Hausman and Taylor (1981) allowed for some of the regressors to be correlated with the individual effects, as opposed to the all or nothing choice. These over-identification restrictions are testable using a Hausman-type test (see Chapter 7). For the applied researcher, performing fixed effects and random effects and the associated Hausman test reported in standard packages like Stata, LIMDEP, TSP, etc., the message is clear: Do not stop here. Test the restrictions implied by the fixed effects model derived by Chamberlain (1984) (see Chapter 4) and check whether a Hausman and Taylor (1981) specification might be a viable alternative (see Chapter 7).</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-two-way-error-component-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["panel analyses book.pdf", "panel analyses book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
