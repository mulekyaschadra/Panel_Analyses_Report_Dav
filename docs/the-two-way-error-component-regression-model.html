<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 The Two-way Error Component Regression Model | Panel Analyses Report</title>
  <meta name="description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 The Two-way Error Component Regression Model | Panel Analyses Report" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 The Two-way Error Component Regression Model | Panel Analyses Report" />
  
  <meta name="twitter:description" content="dans ce livre nous produisons le rapport complet d’analyses du panel des taxes des pays Africains." />
  

<meta name="author" content="David BYAMUNGU" />


<meta name="date" content="2021-09-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-one-way-error-component-regression-model.html"/>
<link rel="next" href="test-of-hypotheses-with-panel-data.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html"><i class="fa fa-check"></i><b>2</b> The One-way Error Component Regression Model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#introduction"><i class="fa fa-check"></i><b>2.1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="2.2" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#the-fixed-effects-model"><i class="fa fa-check"></i><b>2.2</b> THE FIXED EFFECTS MODEL</a></li>
<li class="chapter" data-level="2.3" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#the-random-effects-model"><i class="fa fa-check"></i><b>2.3</b> THE RANDOM EFFECTS MODEL</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-one-way-error-component-regression-model.html"><a href="the-one-way-error-component-regression-model.html#fixed-vs-random"><i class="fa fa-check"></i><b>2.3.1</b> Fixed vs Random</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html"><i class="fa fa-check"></i><b>3</b> The Two-way Error Component Regression Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#ntroduction"><i class="fa fa-check"></i><b>3.1</b> NTRODUCTION</a></li>
<li class="chapter" data-level="3.2" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#the-fixed-effects-model-1"><i class="fa fa-check"></i><b>3.2</b> THE FIXED EFFECTS MODEL</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#testing-for-fixed-effects"><i class="fa fa-check"></i><b>3.2.1</b> Testing for Fixed Effects</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#the-random-effects-model-1"><i class="fa fa-check"></i><b>3.3</b> THE RANDOM EFFECTS MODEL</a></li>
<li class="chapter" data-level="3.4" data-path="the-two-way-error-component-regression-model.html"><a href="the-two-way-error-component-regression-model.html#references"><i class="fa fa-check"></i><b>3.4</b> REFERENCES</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html"><i class="fa fa-check"></i><b>4</b> Test of Hypotheses with Panel Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#tests-for-poolability-of-the-data"><i class="fa fa-check"></i><b>4.1</b> TESTS FOR POOLABILITY OF THE DATA</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#test-for-poolability-under"><i class="fa fa-check"></i><b>4.1.1</b> Test for Poolability under</a></li>
<li class="chapter" data-level="4.1.2" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#test-for-poolability-under-the-general-assumption"><i class="fa fa-check"></i><b>4.1.2</b> Test for Poolability under the General Assumption</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#tests-for-individual-and-time-effects"><i class="fa fa-check"></i><b>4.2</b> TESTS FOR INDIVIDUAL AND TIME EFFECTS</a></li>
<li class="chapter" data-level="4.3" data-path="test-of-hypotheses-with-panel-data.html"><a href="test-of-hypotheses-with-panel-data.html#hausmans-specification-test"><i class="fa fa-check"></i><b>4.3</b> HAUSMAN’S SPECIFICATION TEST</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>5</b> Methods</a></li>
<li class="chapter" data-level="6" data-path="analyses.html"><a href="analyses.html"><i class="fa fa-check"></i><b>6</b> Analyses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="analyses.html"><a href="analyses.html#netoyage-de-la-base-des-données"><i class="fa fa-check"></i><b>6.1</b> Netoyage de la base des données</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="analyses.html"><a href="analyses.html#nouvelle-base-de-données-pour-les-analyses"><i class="fa fa-check"></i><b>6.1.1</b> Nouvelle base de données pour les analyses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="analyses.html"><a href="analyses.html#agregation-des-données-avec-la-méthode-reduce"><i class="fa fa-check"></i><b>6.2</b> Agregation des données avec la méthode reduce</a></li>
<li class="chapter" data-level="6.3" data-path="analyses.html"><a href="analyses.html#analyse-descriptive-des-varariales"><i class="fa fa-check"></i><b>6.3</b> Analyse descriptive des Varariales</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modele-poooling.html"><a href="modele-poooling.html"><i class="fa fa-check"></i><b>7</b> Modele poooling</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Panel Analyses Report</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-two-way-error-component-regression-model" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> The Two-way Error Component Regression Model</h1>
<div id="ntroduction" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> NTRODUCTION</h2>
<p>Wallace and Hussain (1969), Nerlove (1971b) and Amemiya (1971), among others, the regression model given by (2.1), but with two-way error components disturbances: <span class="math display">\[\begin{equation}
u_{i t}=\mu_{i}+\lambda_{t}+v_{i t} \quad i=1, \ldots, N ; t=1, \ldots, T
\end{equation}\]</span> (3.1)</p>
<p>where <span class="math inline">\(\mu_i\)</span> denotes the unobservable individual effect discussed in Chapter 2, <span class="math inline">\(\lambda_t\)</span> denotes the unobservable time effect and <span class="math inline">\(v_{it}\)</span> is the remainder stochastic disturbance term. Note that <span class="math inline">\(\lambda_t\)</span> is individual-invariant and it accounts for any time-specific effect that is not included in the regression. For example, it could account for strike year effects that disrupt production; oil embargo effects that disrupt the supply of oil and affect its price; Surgeon General reports on the ill-effects of smoking, or government laws restricting smoking in public places, all of which could affect consumption behavior. In vector form, (3.1) can be written as</p>
<p><span class="math display">\[\begin{equation}
u=Z_{\mu} \mu+Z_{\lambda} \lambda+v
\end{equation}\]</span> (3.2)</p>
<p>where <span class="math inline">\(Z_\mu, \mu\)</span> and <span class="math inline">\(v\)</span> were defined earlier. <span class="math inline">\(Z_\lambda = i_N \otimes I_T\)</span> is the matrix of time dummies that one may include in the regression to estimate the <span class="math inline">\(\lambda_t\)</span> if they are fixed parameters, and <span class="math inline">\(\lambda^{\prime}= (\lambda_1, ..., \lambda_T )\)</span> Note that <span class="math display">\[Z_\lambda Z_\lambda^ {\prime}=J_N \otimes I_T \]</span> and the projection on <span class="math inline">\(Z_{\lambda}\)</span> is <span class="math display">\[Z_{\lambda}\left(Z_{\lambda}^{\prime} Z_{\lambda}\right)^{-1} Z_{\lambda}^{\prime}=\bar{J}_{N} \otimes I_T  \]</span> This last matrix averages the data over individuals, i.e., if we regress y on <span class="math inline">\(Z_\lambda\)</span>, the predicted values are given by <span class="math display">\[ (\bar {J_N} \otimes I_T)y \]</span> which has typical element <span class="math inline">\(\bar{y}_{. t}=\sum_{i=1}^{N} y_{i t} / N\)</span> .</p>
</div>
<div id="the-fixed-effects-model-1" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> THE FIXED EFFECTS MODEL</h2>
<p>If the <span class="math inline">\(\mu_{i}\)</span> and <span class="math inline">\(\lambda_{t}\)</span> are assumed to be fixed parameters to be estimated and the remainder disturbances stochastic with <span class="math inline">\(v_{i t} \sim \operatorname{IID}\left(0, \sigma_{v}^{2}\right)\)</span>, then (3.1) represents a two-way fixed effects error component model. The <span class="math inline">\(X_{i t}\)</span> are assumed independent of the <span class="math inline">\(v_{i t}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>. Inference in this case is conditional on the particular <span class="math inline">\(N\)</span> individuals and over the specific time periods observed. Recall that <span class="math inline">\(Z_{\lambda}\)</span>, the matrix of time dummies, is <span class="math inline">\(N T \times T\)</span>. If <span class="math inline">\(N\)</span> or <span class="math inline">\(T\)</span> is large, there will be too many dummy variables in the regression <span class="math inline">\(\{(N-1)+(T-1)\}\)</span> of them, and this causes an enormous loss in degrees of freedom. In addition, this attenuates the problem of multicollinearity among the regressors. Rather than invert a large <span class="math inline">\((N+T+K-1)\)</span> matrix, one can obtain the fixed effects estimates of <span class="math inline">\(\beta\)</span> by performing the following Within transformation given by Wallace and Hussain (1969):</p>
<p><span class="math display">\[\begin{equation}
Q=E_{N} \otimes E_{T}=I_{N} \otimes I_{T}-I_{N} \otimes \bar{J}_{T}-\bar{J}_{N} \otimes I_{T}+\bar{J}_{N} \otimes \bar{J}_{T}
\end{equation}\]</span> (3.3)</p>
<p>where <span class="math inline">\(E_{N}=I_{N}-\bar{J}_{N}\)</span> and <span class="math inline">\(E_{T}=I_{T}-\bar{J}_{T}\)</span>. This transformation “sweeps” the <span class="math inline">\(\mu_{i}\)</span> and <span class="math inline">\(\lambda_{t}\)</span> effects. In fact, <span class="math inline">\(\tilde{y}=Q y\)</span> has a typical element <span class="math inline">\(\tilde{y}_{i t}=\left(y_{i t}-\bar{y}_{i .}-\bar{y}_{i t}+\bar{y} . .\right)\)</span> where <span class="math inline">\(\bar{y}_{. .}=\sum_{i} \sum_{t} y_{i t} /\)</span> <span class="math inline">\(N T\)</span>, and one would perform the regression of <span class="math inline">\(\tilde{y}=Q y\)</span> on <span class="math inline">\(\widetilde{X}=Q X\)</span> to get the Within estimator <span class="math inline">\(\widetilde{\beta}=\left(X^{\prime} Q X\right)^{-1} X^{\prime} Q y\)</span></p>
<p>Note that by averaging the simple regression given in (2.8) over individuals, we get</p>
<p><span class="math display">\[\begin{equation}
\bar{y}_{. t}=\alpha+\beta \bar{x}_{. t}+\lambda_{t}+\bar{v}_{. t}
\end{equation}\]</span> (3.4)</p>
<p>where we have utilized the restriction that <span class="math inline">\(\sum_i \mu_i=0\)</span> to avoid the dummy variable trap.
Similarly the averages defined in (2.9) and (2.11) still hold using <span class="math inline">\(\sum_t \lambda_t=0\)</span> and one can deduce that</p>
<p><span class="math display">\[\begin{equation}
\left(y_{i t}-\bar{y}_{i .}-\bar{y}_{. t}+\bar{y}_{. .}\right)=\left(x_{i t}-\bar{x}_{i .}-\bar{x}_{. t}+\bar{x}_{. .}\right) \beta+\left(v_{i t}-\bar{v}_{i .}-\bar{v}_{. t}+\bar{v}_{. .}\right)
\end{equation}\]</span> (3.5)</p>
<p>OLS on this model gives <span class="math inline">\(\widetilde {\beta}\)</span> the Within estimator for the two-way model. Once again, the
Within estimate of the intercept can be deduced from
<span class="math display">\[\widetilde {\alpha}=\bar{y}_..-\widetilde {\beta} \bar{x}  \]</span>
and those of <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(\lambda_i\)</span> are given by</p>
<p><span class="math display">\[\begin{equation}
\tilde{\mu}_{i}=\left(\bar{y}_{i .}-\bar{y}_{. .}\right)-\widetilde{\beta}\left(\bar{x}_{i .}-\bar{x}_{. .}\right)
\end{equation}\]</span> (3.6)</p>
<p><span class="math display">\[\begin{equation}
\tilde{\lambda}_{t}=\left(\bar{y}_{. t}-\bar{y}_{. .}\right)-\widetilde{\beta}\left(\bar{x}_{. t}-\bar{x}_{. .}\right)
\end{equation}\]</span> (3.7)</p>
<p>Note that the Within estimator cannot estimate the effect of time-invariant and individualinvariant variables because the Q transformation wipes out these variables. If the true model
is a two-way fixed effects model as in (3.2), then OLS on (2.1) yields biased and inconsistent estimates of the regression coefficients. OLS ignores both sets of dummy variables, whereas the one-way fixed effects estimator considered in Chapter 2 ignores only the time dummies. If
these time dummies are statistically significant, the one-way fixed effects estimator will also
suffer from omission bias.</p>
<div id="testing-for-fixed-effects" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Testing for Fixed Effects</h3>
<p>As in the one-way error component model case, one can test for joint significance of the dummy
variables:</p>
<p><span class="math display">\[
H_{0}: \mu_{1}=\ldots=\mu_{N-1}=0 \quad \text { and } \quad \lambda_{1}=\ldots=\lambda_{T-1}=0
\]</span>
The restricted residual sums of squares (RRSS) is that of pooled OLS and the unrestricted residual sums of squares (URSS) is that from the Within regression in (3.5). In this case,</p>
<p><span class="math display">\[\begin{equation}
F_{1}=\frac{(\mathrm{RRSS}-\mathrm{URSS}) /(N+T-2)}{\mathrm{URSS} /(N-1)(T-1)-K} \stackrel{H_{0}}{\sim} F_{(N+T-2),(N-1)(T-1)-K}
\end{equation}\]</span> (3.8)</p>
<p>Next, one can test for the existence of individual effects allowing for time effects, i.e.
<span class="math inline">\(H_{2}: \mu_{1}==u_{N}=0 \quad\)</span> allowing <span class="math inline">\(\quad \lambda \neq 0\)</span> for <span class="math inline">\(t=1 \ldots \quad T-1\)</span></p>
<p>The URSS is still the Within residual sum of squares. However, the RRSS is the regression with time-series dummies only, or the regression based upon</p>
<p><span class="math display">\[\begin{equation}
\left(y_{i t}-\bar{y}_{. t}\right)=\left(x_{i t}-\bar{x}_{. t}\right) \beta+\left(u_{i t}-\bar{u}_{. t}\right)
\end{equation}\]</span> (3.9)</p>
<p>In this case the resulting <span class="math inline">\(F\)</span> -statistic is <span class="math inline">\(F_{2} \stackrel{H_{0}}{\sim} F_{(N-1),(N-1)(T-1)-K}\)</span>. Note that <span class="math inline">\(F_{2}\)</span> differs from <span class="math inline">\(F_{0}\)</span> in ( <span class="math inline">\(2.12\)</span> ) in testing for <span class="math inline">\(\mu_{i}=0\)</span>. The latter tests <span class="math inline">\(H_{0}: \mu_{i}=0\)</span> assuming that <span class="math inline">\(\lambda_{t}=0\)</span>, whereas the former tests <span class="math inline">\(H_{2}: \mu_{i}=0\)</span> allowing <span class="math inline">\(\lambda_{t} \neq 0\)</span> for <span class="math inline">\(t=1, \ldots, T-1\)</span>. Similarly, one can test for the existence of time effects allowing for individual effects, i.e.</p>
<p><span class="math display">\[\begin{equation}
H_{3}: \lambda_{1}=\ldots=\lambda_{T-1}=0 \quad \text { allowing } \quad \mu_{i} \neq 0 ; i=1, \ldots,(N-1)
\end{equation}\]</span></p>
<p>The RRSS is given by the regression in (2.10), while the URSS is obtained from the regression
(3.5). In this case, the resulting F-statistic is <span class="math inline">\(F_{3} \sim^{H_{0}} F_{(T-1),(N-1)(T-1)-K}\)</span> .</p>
<p><em>Computational Warning</em></p>
<p>As in the one-way model, <span class="math inline">\(s^2\)</span> from the regression in (3.5) as obtained from any standard regression package has to be adjusted for loss of degrees of freedom. In this case, one divides by$ (N − 1)(T − 1) − K $ and multiplies by $(NT − K) $to get the proper variance–covariance matrix of the Within estimator.</p>
</div>
</div>
<div id="the-random-effects-model-1" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> THE RANDOM EFFECTS MODEL</h2>
<p>If <span class="math inline">\(\mu_{i} \sim \operatorname{IID}\left(0, \sigma_{\mu}^{2}\right), \lambda_{t} \sim \operatorname{IID}\left(0, \sigma_{\lambda}^{2}\right)\)</span> and <span class="math inline">\(v_{i t} \sim \operatorname{IID}\left(0, \sigma_{v}^{2}\right)\)</span> independent of each other, then this
is the two-way random effects model. In addition, <span class="math inline">\(X_{i t}\)</span> is independent of <span class="math inline">\(\mu_{i}, \lambda_{t}\)</span> and <span class="math inline">\(v_{i t}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>. Inference in this case pertains to the large population from which this sample was randomly drawn. From (3.2), one can compute the variance-covariance matrix</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\Omega &amp;=E\left(u u^{\prime}\right)=Z_{\mu} E\left(\mu \mu^{\prime}\right) Z_{\mu}^{\prime}+Z_{\lambda} E\left(\lambda \lambda^{\prime}\right) Z_{\lambda}^{\prime}+\sigma_{v}^{2} I_{N T} \\
&amp;=\sigma_{\mu}^{2}\left(I_{N} \otimes J_{T}\right)+\sigma_{\lambda}^{2}\left(J_{N} \otimes I_{T}\right)+\sigma_{v}^{2}\left(I_{N} \otimes I_{T}\right)
\end{aligned}
\end{equation}\]</span> (3.10)</p>
<p>The disturbances are homoskedastic with <span class="math inline">\(\operatorname{var}\left(u_{i t}\right)=\sigma_{\mu}^{2}+\sigma_{\lambda}^{2}+\sigma_{v}^{2}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{rl}
\operatorname{cov}\left(u_{i t}, u_{j s}\right)=\sigma_{\mu}^{2} &amp; i=j, t \neq s \\
=\sigma_{\lambda}^{2} &amp; i \neq j, t=s
\end{array}
\end{equation}\]</span> (3.11)</p>
<p>and zero otherwise. This means that the correlation coefficient</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\operatorname{correl}\left(u_{i t}, u_{j s}\right) &amp;=\sigma_{\mu}^{2} /\left(\sigma_{\mu}^{2}+\sigma_{\lambda}^{2}+\sigma_{v}^{2}\right) &amp; &amp; i=j, t \neq s \\
&amp;=\sigma_{\lambda}^{2} /\left(\sigma_{\mu}^{2}+\sigma_{\lambda}^{2}+\sigma_{v}^{2}\right) &amp; &amp; i \neq j, t=s \\
&amp;=1 &amp; &amp; i=j, t=s \\
&amp;=0 &amp; &amp; i \neq j, t \neq s
\end{aligned}
\end{equation}\]</span> (3.12)</p>
<p>In order to get <span class="math inline">\(\Omega^{-1}\)</span>, we replace <span class="math inline">\(J_{N}\)</span> by <span class="math inline">\(N \bar{J}_{N}, I_{N}\)</span> by <span class="math inline">\(E_{N}+\bar{J}_{N}, J_{T}\)</span> by <span class="math inline">\(T \bar{J}_{T}\)</span> and <span class="math inline">\(I_{T}\)</span> by <span class="math inline">\(E_{T}+\bar{J}_{T}\)</span> and collect terms with the same matrices. This gives</p>
<p><span class="math display">\[\begin{equation}
\Omega=\sum_{i=1}^{4} \lambda_{i} Q_{i}
\end{equation}\]</span> (3.13)</p>
<p>where <span class="math inline">\(\lambda_{1}=\sigma_{v}^{2}, \lambda_{2}=T \sigma_{\mu}^{2}+\sigma_{v}^{2}, \lambda_{3}=N \sigma_{\lambda}^{2}+\sigma_{v}^{2}\)</span> and <span class="math inline">\(\lambda_{4}=T \sigma_{\mu}^{2}+N \sigma_{\lambda}^{2}+\sigma_{v}^{2}\)</span>. Correspond-
ingly, <span class="math inline">\(Q_{1}=E_{N} \otimes E_{T}, Q_{2}=E_{N} \otimes \bar{J}_{T}, Q_{3}=\bar{J}_{N} \otimes E_{T}\)</span> and <span class="math inline">\(Q_{4}=\bar{J}_{N} \otimes \bar{J}_{T}\)</span>, respectively.
The <span class="math inline">\(\lambda_{i}\)</span> are the distinct characteristic roots of <span class="math inline">\(\Omega\)</span> and the <span class="math inline">\(Q_{i}\)</span> are the corresponding matrices of eigenprojectors. <span class="math inline">\(\lambda_{1}\)</span> is of multiplicity <span class="math inline">\((N-1)(T-1), \lambda_{2}\)</span> is of multiplicity <span class="math inline">\((N-1), \lambda_{3}\)</span> is of multiplicity <span class="math inline">\((T-1)\)</span> and <span class="math inline">\(\lambda_{4}\)</span> is of multiplicity <span class="math inline">\(1 .{ }^{1}\)</span> Each <span class="math inline">\(Q_{i}\)</span> is symmetric and idempotent with its rank equal to its trace. Moreover, the <span class="math inline">\(Q_{i}\)</span> are pairwise orthogonal and sum to the identity matrix. The advantages of this spectral decomposition are that</p>
<p><span class="math display">\[\begin{equation}
\Omega^{r} =\sum_{i=1}^{4} \lambda_{i}^{r} Q_{i}
\end{equation}\]</span> (3.14)</p>
<p>where <span class="math inline">\(r\)</span> is an arbitrary scalar so that</p>
<p><span class="math display">\[\begin{equation}
\sigma_{v} \Omega^{-1 / 2}=\sum_{i=1}^{4}\left(\sigma_{v} / \lambda_{i}^{1 / 2}\right) Q_{i}
\end{equation}\]</span> (3.15)</p>
<p>and the typical element of <span class="math inline">\(y^{*}=\sigma_{\nu} \Omega^{-1 / 2} y\)</span> is given by</p>
<p><span class="math display">\[\begin{equation}
y_{i t}^{*}=y_{i t}-\theta_{1} \bar{y}_{i .}-\theta_{2} \bar{y}_{. t}+\theta_{3} \bar{y}_{. .}
\end{equation}\]</span> (3.16)</p>
<p>where <span class="math inline">\(\theta_{1}=1-\left(\sigma_{v} / \lambda_{2}^{1 / 2}\right), \theta_{2}=1-\left(\sigma_{v} / \lambda_{3}^{1 / 2}\right)\)</span> and <span class="math inline">\(\theta_{3}=\theta_{1}+\theta_{2}+\left(\sigma_{v} / \lambda_{4}^{1 / 2}\right)-1\)</span>. As a result,
GLS can be obtained as OLS of <span class="math inline">\(y^{*}\)</span> on <span class="math inline">\(Z^{*}\)</span>, where <span class="math inline">\(Z^{*}=\sigma_{\nu} \Omega^{-1 / 2} Z\)</span>. This transformation was first derived by Fuller and Battese (1974), see also Baltagi (1993).</p>
<p>The best quadratic unbiased (BQU) estimators of the variance components arise naturally from the fact that <span class="math inline">\(Q_{i} u \sim\left(0, \lambda_{i} Q_{i}\right) .\)</span> Hence,</p>
<p><span class="math display">\[\begin{equation}
\widehat{\lambda}_{i}=u^{\prime} Q_{i} u / \operatorname{tr}\left(Q_{i}\right)
\end{equation}\]</span> (3.17)</p>
<p>is the BQU estimator of <span class="math inline">\(\lambda_{i}\)</span> for <span class="math inline">\(i=1,2,3\)</span>. These ANOVA estimators are minimum variance unbiased (MVU) under normality of the disturbances (see Graybill, 1961 ). As in the one-way error component model, one can obtain feasible estimates of the variance components by replacing the true disturbances by OLS residuals (see Wallace and Hussain, 1969 ). OLS is still an unbiased and consistent estimator under the random effects model, but it is inefficient and results in biased standard errors and <span class="math inline">\(t\)</span> -statistics. Alternatively, one could substitute the Within residuals with <span class="math inline">\(\tilde{u}=y-\widetilde{\alpha} \iota_{N T}-X \widetilde{\beta}\)</span>, where <span class="math inline">\(\tilde{\alpha}=\bar{y}_{. .}-\bar{X}_{. .}^{\prime} \tilde{\beta}\)</span> and <span class="math inline">\(\widetilde{\beta}\)</span> is obtained by the regression
in (3.5). This is the method proposed by Amemiya (1971). In fact, Amemiya (1971) shows that the Wallace and Hussain (1969) estimates of the variance components have a different asymptotic distribution from that knowing the true disturbances, while the Amemiya (1971) estimates of the variance components have the same asymptotic distribution as that knowing the true disturbances:</p>
<p><span class="math display">\[\begin{equation}
\left(\begin{array}{c}
\sqrt{N T}\left(\widehat{\sigma}_{v}^{2}-\sigma_{v}^{2}\right) \\
\sqrt{N}\left(\widehat{\sigma}_{\mu}^{2}-\sigma_{\mu}^{2}\right) \\
\sqrt{T}\left(\widehat{\sigma}^{2}-\sigma_{\mu}^{2}\right)
\end{array}\right) \sim N\left(0,\left(\begin{array}{ccc}
2 \sigma_{v}^{4} &amp; 0 &amp; 0 \\
0 &amp; 2 \sigma_{\mu}^{4} &amp; 0 \\
0 &amp; 0 &amp; 2 \sigma^{4}
\end{array}\right)\right)
\end{equation}\]</span> (3.18)</p>
<p>SubstitutingOLSorWithin residuals instead of the true disturbances in (3.17) introduces bias in
the corresponding estimates of the variance components. The degrees of freedom corrections
that make these estimates unbiased depend upon traces of matrices that involve the matrix
of regressors X. These corrections are given in Wallace and Hussain (1969) and Amemiya
(1971), respectively. Alternatively, one can infer these correction terms from the more general
unbalanced error component model considered in Chapter 9.
Swamy and Arora (1972) suggest running three least squares regressions and estimating
the variance components from the corresponding mean square errors of these regressions. The
first regression corresponds to the Within regression which transforms the original model by
<span class="math inline">\(Q_1=E_N \otimes E_T\)</span> . This is equivalent to the regression in (3.5), and yields the following estimate
of <span class="math inline">\(\sigma_v^2\)</span> :</p>
<p><span class="math display">\[\begin{equation}
\widehat{\lambda}_{1}=\widehat{\sigma}_{v}^{2}=\left[y^{\prime} Q_{1} y-y^{\prime} Q_{1} X\left(X^{\prime} Q_{1} X\right)^{-1} X^{\prime} Q_{1} y\right] /[(N-1)(T-1)-K]
\end{equation}\]</span> (3.19)<br />
( à arranger)</p>
<p>The second regression is the Between individuals regression which transforms the original
model by <span class="math display">\[Q_2=E_N \otimes \bar{J}_T \]</span> This is equivalent to the regression of <span class="math display">\[ (\bar{y}_{i.}- \bar{y}_{i..}) \]</span> on
<span class="math display">\[ (\bar{X}_{i.}- \bar{X}_{i..}) \]</span>
and yields the following estimate of
$ _2=T_v^2 + _v^2 $:</p>
<p><span class="math display">\[\begin{equation}
\widehat{\lambda}_{2}=\left[y^{\prime} Q_{2} y-y^{\prime} Q_{2} X\left(X^{\prime} Q_{2} X\right)^{-1} X^{\prime} Q_{2} y\right] /[(N-1)-K]
\end{equation}\]</span> (3.20)</p>
<p>from which one obtains <span class="math inline">\(\widehat{\sigma}_{\mu}^{2}=\left(\widehat{\lambda}_{2}-\widehat{\sigma}_{v}^{2}\right) / T .\)</span> The third regression is the Between time-periods regression which transforms the original model by <span class="math inline">\(Q_{3}=\bar{J}_{N} \otimes E_{T}\)</span>. This is equivalent to the regression of <span class="math inline">\(\left(\bar{y}_{. t}-\bar{y}_{. .}\right)\)</span> on <span class="math inline">\(\left(\bar{X}_{. t}-\bar{X}_{. .}\right)\)</span> and yields the following estimate of <span class="math inline">\(\lambda_{3}=N \sigma_{\lambda}^{2}+\sigma_{v}^{2}\)</span></p>
<p><span class="math display">\[\begin{equation}
\widehat{\lambda}_{3}=\left[y^{\prime} Q_{3} y-y^{\prime} Q_{3} X\left(X^{\prime} Q_{3} X\right)^{-1} X^{\prime} Q_{3} y\right] /[(T-1)-K]
\end{equation}\]</span> (3.21)</p>
<p>from which one obtains
<span class="math display">\[ (\widehat{\widehat {\sigma_\lambda^2} }= \widehat{\widehat {\lambda} }_3 -  \widehat{\widehat {\lambda} }_v)/N \]</span> Stacking the three transformed regressions just
performed yields</p>
<p><span class="math display">\[\begin{equation}
\left(\begin{array}{l}
Q_{1} y \\
Q_{2} y \\
Q_{3} y
\end{array}\right)=\left(\begin{array}{l}
Q_{1} X \\
Q_{2} X \\
Q_{3} X
\end{array}\right) \beta+\left(\begin{array}{l}
Q_{1} u \\
Q_{2} u \\
Q_{3} u
\end{array}\right)
\end{equation}\]</span> (3.22)</p>
<p>since <span class="math inline">\(Q_{i} \iota_{N T}=0\)</span> for <span class="math inline">\(i=1,2,3\)</span>, and the transformed error has mean 0 and variance-covariance matrix given by <span class="math inline">\(\operatorname{diag}\left[\lambda_{i} Q_{i}\right]\)</span> with <span class="math inline">\(i=1,2,3 .\)</span> Problem <span class="math inline">\(3.4\)</span> asks the reader to show that <span class="math inline">\(\mathrm{OLS}\)</span> on this system of <span class="math inline">\(3 N T\)</span> observations yields the same estimator of <span class="math inline">\(\beta\)</span> as OLS on the pooled model (2.3). Also, GLS on this system of equations (3.22) yields the same estimator of <span class="math inline">\(\beta\)</span> as GLS on (2.3). In fact,</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\widehat{\beta}_{\mathrm{GLS}}=&amp;\left[\left(X^{\prime} Q_{1} X\right) / \sigma_{v}^{2}+\left(X^{\prime} Q_{2} X\right) / \lambda_{2}+\left(X^{\prime} Q_{3} X\right) / \lambda_{3}\right]^{-1} \\
&amp; \times\left[\left(X^{\prime} Q_{1} y\right) / \sigma_{v}^{2}+\left(X^{\prime} Q_{2} y\right) / \lambda_{2}+\left(X^{\prime} Q_{3} y\right) / \lambda_{3}\right] \\
=&amp;\left[W_{X X}+\phi_{2}^{2} B_{X X}+\phi_{3}^{2} C_{X X}\right]^{-1}\left[W_{X y}+\phi_{2}^{2} B_{X y}+\phi_{3}^{2} C_{X y}\right]
\end{aligned}
\end{equation}\]</span> (3.23)</p>
<p>with <span class="math inline">\(\operatorname{var}\left(\widehat{\beta}_{\mathrm{GLS}}\right)=\sigma_{v}^{2}\left[W_{X X}+\phi_{2}^{2} B_{X X}+\phi_{3}^{2} C_{X X}\right]^{-1}\)</span>. Note that <span class="math inline">\(W_{X X}=X^{\prime} Q_{1} X, B_{X X}=X^{\prime} Q_{2} X\)</span>
and <span class="math inline">\(C_{X X}=X^{\prime} Q_{3} X\)</span> with <span class="math inline">\(\phi_{2}^{2}=\sigma_{v}^{2} / \lambda_{2}, \phi_{3}^{2}=\sigma_{v}^{2} / \lambda_{3} .\)</span> Also, the Within estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\widetilde{\beta}_{W}=\)</span>
<span class="math inline">\(W_{X X}^{-1} W_{X y}\)</span>, the Between individuals estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\widehat{\beta}_{B}=B_{X X}^{-1} B_{X y}\)</span> and the Between timeperiods estimator of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\widehat{\beta}_{C}=C_{X X}^{-1} C_{X y}\)</span>. This shows that <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> is a matrix-weighted average of <span class="math inline">\(\widetilde{\beta}_{W}, \widehat{\beta}_{B}\)</span> and <span class="math inline">\(\widehat{\beta}_{C}\)</span>. In fact,</p>
<p><span class="math display">\[\begin{equation}
\widehat{\beta}_{\mathrm{GLS}}=W_{1} \widetilde{\beta}_{W}+W_{2} \widehat{\beta}_{B}+W_{3} \widehat{\beta}_{C}
\end{equation}\]</span> (3.24)</p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;W_{1}=\left[W_{X X}+\phi_{2}^{2} B_{X X}+\phi_{3}^{2} C_{X X}\right]^{-1} W_{X X} \\
&amp;W_{2}=\left[W_{X X}+\phi_{2}^{2} B_{X X}+\phi_{3}^{2} C_{X X}\right]^{-1}\left(\phi_{2}^{2} B_{X X}\right) \\
&amp;W_{3}=\left[W_{X X}+\phi_{2}^{2} B_{X X}+\phi_{3}^{2} C_{X X}\right]^{-1}\left(\phi_{3}^{2} C_{X X}\right)
\end{aligned}
\end{equation}\]</span></p>
<p>This was demonstrated by Maddala (1971). Note that (i) if <span class="math inline">\(\sigma_{\mu}^{2}=\sigma_{\lambda}^{2}=0, \phi_{2}^{2}=\phi_{3}^{2}=1\)</span> and <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> reduces to <span class="math inline">\(\widehat{\beta}_{\mathrm{OLs}}\)</span>; (ii) as <span class="math inline">\(T\)</span> and <span class="math inline">\(N \rightarrow \infty, \phi_{2}^{2}\)</span> and <span class="math inline">\(\phi_{3}^{2} \rightarrow 0\)</span> and <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> tends to <span class="math inline">\(\widetilde{\beta}_{W}\)</span>; (iii) if
<span class="math inline">\(\phi_{2}^{2} \rightarrow \infty\)</span> with <span class="math inline">\(\phi_{3}^{2}\)</span> finite, then <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> tends to <span class="math inline">\(\widehat{\beta}_{B}\)</span>; (iv) if <span class="math inline">\(\phi_{3}^{2} \rightarrow \infty\)</span> with <span class="math inline">\(\phi_{2}^{2}\)</span> finite, then <span class="math inline">\(\widehat{\beta}_{\text {GLS }}\)</span> tends to <span class="math inline">\(\widehat{\beta}_{C}\)</span>.
Wallace and Hussain (1969) compare <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> and <span class="math inline">\(\widetilde{\beta}_{\text {Within }}\)</span> in the case of nonstochastic (repetitive) <span class="math inline">\(X\)</span> and find that both are (i) asymptotically normal, (ii) consistent and unbiased and that</p>
<ol start="3" style="list-style-type: lower-roman">
<li><span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> has a smaller generalized variance (i.e. more efficient) in finite samples. In the case of nonstochastic (nonrepetitive) <span class="math inline">\(X\)</span> they find that both <span class="math inline">\(\widehat{\beta}_{\mathrm{GLS}}\)</span> and <span class="math inline">\(\widetilde{\beta}_{\text {Within }}\)</span> are consistent, asymptotically unbiased and have equivalent asymptotic variance-covariance matrices, as both <span class="math inline">\(N\)</span> and <span class="math inline">\(T \rightarrow \infty\)</span>. The last statement can be proved as follows: the limiting variance of the GLS estimator is</li>
</ol>
<p><span class="math display">\[\begin{equation}
\frac{1}{N T} \lim _{N \rightarrow \infty \atop T \rightarrow \infty}\left(X^{\prime} \Omega^{-1} X / N T\right)^{-1}=\frac{1}{N T} \lim _{N \rightarrow \infty \atop T \rightarrow \infty}\left[\sum_{i=1}^{3} \frac{1}{\lambda_{i}}\left(X^{\prime} Q_{i} X / N T\right)\right]^{-1}
\end{equation}\]</span> (3.25)</p>
<p>but the limit of the inverse is the inverse of the limit, and</p>
<p><span class="math display">\[\begin{equation}
\lim _{N \rightarrow \infty \atop T \rightarrow \infty} \frac{X^{\prime} Q_{i} X}{N T} \quad \text { for } i=1,2,3
\end{equation}\]</span> (3.26)</p>
<p>all exist and are positive semidefinite, since <span class="math display">\[\lim _{N \rightarrow \infty \atop T \rightarrow \infty}\left(X^{\prime} X / N T\right)\]</span> is assumed finite and positive definite. Hence</p>
<p><span class="math display">\[
\lim _{N \rightarrow \infty \atop T \rightarrow \infty} \frac{1}{\left(N \sigma_{\lambda}^{2}+\sigma_{v}^{2}\right)}\left(\frac{X^{\prime} Q_{3} X}{N T}\right)=0
\]</span></p>
<p>and
<span class="math display">\[
\lim _{N \rightarrow \infty \atop T \rightarrow \infty} \frac{1}{\left(T \sigma_{\mu}^{2}+\sigma_{v}^{2}\right)}\left(\frac{X^{\prime} Q_{2} X}{N T}\right)=0
\]</span>
Therefore the limiting variance of the GLS estimator becomes
<span class="math display">\[
\frac{1}{N T} \lim _{N \rightarrow \infty \atop T \rightarrow \infty} \sigma_{v}^{2}\left(\frac{X^{\prime} Q_{1} X}{N T}\right)^{-1}
\]</span></p>
<p>which is the limiting variance of the Within estimator. One can extend Nerlove’s (1971a) method for the one-way model, by estimating <span class="math inline">\(\sigma_{\mu}^{2}\)</span> as <span class="math inline">\(\sum_{i=1}^{N}\left(\widehat{\mu}_{i}-\widehat{\mu}\right)^{2} /(N-1)\)</span> and <span class="math inline">\(\sigma_{\lambda}^{2}\)</span> as <span class="math inline">\(\sum_{t=1}^{T}\left(\widehat{\lambda}_{t}-\bar{\lambda}\right)^{2} /(T-1)\)</span> where the <span class="math inline">\(\widehat{\mu}_{i}\)</span> and <span class="math inline">\(\widehat{\lambda}_{t}\)</span> are obtained
as coefficients from the least squares dummy variables regression (LSDV). <span class="math inline">\(\sigma_{v}^{2}\)</span> is estimated from the Within residual sums of squares divided by <span class="math inline">\(N T\)</span>. Baltagi (1995, appendix 3) develops two other methods of estimating the variance components. The first is Rao’s (1970) minimum norm quadratic unbiased estimation (MINQUE) and the second is Henderson’s method III as described by Fuller and Battese (1973). These methods require more notation and development and may be skipped in a brief course on this subject. Chapter 9 studies these estimation methods in the context of an unbalanced error component model.</p>
<p>Baltagi (1981a) performed a Monte Carlo study on a simple regression equation with twoway error component disturbances and studied the properties of the following estimators:
OLS, the Within estimator and six feasible GLS estimators denoted by WALHUS, AMEMIYA, SWAR, MINQUE, FUBA and NERLOVE corresponding to the methods developed by Wallace and Hussain (1969), Amemiya (1971), Swamy and Arora (1972), Rao (1972), Fuller and Battese (1974) and Nerlove (1971a), respectively. The mean square error of these estimators was computed relative to that of true GLS, i.e. GLS knowing the true variance components. To review some of the properties of these estimators: OLS is unbiased, but asymptotically inefficient, and its standard errors are biased; see Moulton (1986) for the extent of this bias in empirical applications. In contrast, the Within estimator is unbiased whether or not prior</p>
<p>information about the variance components is available. It is also asymptotically equivalent to the GLS estimator in case of weakly nonstochastic exogenous variables. Early in the literature, Wallace and Hussain (1969) recommended the Within estimator for the practical researcher, based on theoretical considerations but more importantly for its ease of computation. In Wallace and Hussain’s (1969, p. 66) words the “covariance estimators come off with a surprisingly clear bill of health.” True GLS is BLUE, but the variance components are usually not known and have to be estimated. All of the feasible GLS estimators considered are asymptotically efficient. In fact, Prucha (1984) showed that as long as the estimate of <span class="math inline">\(\sigma_{v}^{2}\)</span> is consistent, and the probability limits of the estimates <span class="math inline">\(\sigma_{\mu}^{2}\)</span> and <span class="math inline">\(\sigma_{\lambda}^{2}\)</span> are finite, the corresponding feasible GLS estimator is asymptotically efficient. Also, Swamy and Arora (1972) proved the existence of a family of asymptotically efficient two-stage feasible GLS estimators of the regression coefficients. Therefore, based on asymptotics only, one cannot differentiate among these twostage GLS estimators. This leaves undecided the question of which estimator is the best to use. Some analytical results were obtained by Swamy (1971) and Swamy and Arora (1972). These studies derived the relative efficiencies of (i) SWAR with respect to OLS, (ii) SWAR with respect to Within and (iii) Within with respect to OLS. Then, for various values of <span class="math inline">\(N, T\)</span>, the variance components, the Between groups, Between time-periods and Within groups sums of squares of the independent variable, they tabulated these relative efficiency values (see Swamy, 1971 , chapters II and III; Swamy and Arora, 1972, p. 272). Among their basic findings is the fact that, for small samples, SWAR is less efficient than OLS if <span class="math inline">\(\sigma_{\mu}^{2}\)</span> and <span class="math inline">\(\sigma_{\lambda}^{2}\)</span> are small. Also, SWAR is less efficient than Within if <span class="math inline">\(\sigma_{\mu}^{2}\)</span> and <span class="math inline">\(\sigma_{\lambda}^{2}\)</span> are large. The latter result is disconcerting, since Within, which uses only a part of the available data, is more efficient than SWAR, a feasible GLS estimator, which uses all of the available data.</p>
</div>
<div id="references" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> REFERENCES</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-one-way-error-component-regression-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="test-of-hypotheses-with-panel-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["panel analyses book.pdf", "panel analyses book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
