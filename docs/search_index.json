[["intro.html", "Panel Analyses Report Chapter 1 Introduction", " Panel Analyses Report David BYAMUNGU 2021-09-26 Chapter 1 Introduction Dans ce document nous cherchons à modéliser les taxes perçues dans différents pays , formant un panel dont la période est 10 ans. Ainsi, nous expliquerons la variable taxe par: Le poids des Marchandises La qualité des Marchandises Le but est darbitrer entre: le modèle pooling le modele à éffet fixe et le modèle à éffet aléatoire et en fin produire un modèle dynamique permettant dexpliquer la variation du taxe au cours du temps, avec comme variable dépendante additionnelle le taxe décalé Nous expliquons Notre méthodologie dans la partie suivante. "],["the-one-way-error-component-regression-model.html", "Chapter 2 The One-way Error Component Regression Model 2.1 INTRODUCTION 2.2 THE FIXED EFFECTS MODEL 2.3 THE RANDOM EFFECTS MODEL", " Chapter 2 The One-way Error Component Regression Model 2.1 INTRODUCTION A panel data regression differs from a regular time-series or cross-section regression in that it has a double subscript on its variables, i.e. \\[\\begin{equation} y_{it}= \\alpha + X_{it}^{&#39;} \\beta + u_{it} $$ $$ i=1, ... , N ; t=1, ... ,T \\end{equation}\\](2.1) with $ i $ denoting households, individuals, firms, countries, etc. and t denoting time. The i subscript, therefore, denotes the cross-section dimension whereas t denotes the time-series dimension. \\[ \\alpha \\] is a scalar, \\[ \\beta \\] is \\[ K × 1 \\] and Xi t is the $ it_{th} $ observation on K explanatory variables. Most of the panel data applications utilize a one-way error component model for the disturbances, with \\[ u_{it}= \\mu_i + v_{it} \\] (2.2) where i denotes the unobservable individual-specific effect and i t denotes the remainder disturbance. For example, in an earnings equation in labor economics, yi t will measure earnings of the head of the household, whereas \\[ X_{it} \\] may contain a set of variables like experience, education, union membership, sex, race, etc. Note that i is time-invariant and it accounts for any individual-specific effect that is not included in the regression. In this case we could think of it as the individuals unobserved ability. The remainder disturbance \\[ v_{it} \\] varies with individuals and time and can be thought of as the usual disturbance in the regression. Alternatively, for a production function utilizing data on firms across time, \\[ y_{it} \\] will measure output and \\[ X_{it} \\] will measure inputs. The unobservable firm-specific effects will be captured by the \\[ \\mu_i \\] and we can think of these as the unobservable entrepreneurial or managerial skills of the firms executives. Early applications of error components in economics include Kuh (1959) on investment, Mundlak (1961) and Hoch (1962) on production functions and Balestra and Nerlove (1966) on demand for natural gas. In vector form (2.1) can be written as \\[\\begin{equation} y= \\alpha i_{NT} + X \\beta + u = Z \\delta + u \\end{equation}\\] (2.3) \\[\\begin{equation} y= \\alpha i_{NT} + X \\beta + u = Z \\delta + u \\end{equation}\\] (2.3) where $ y $ is $ NT × 1 $, $ X $ is $ NT × K $, $ Z = [_{NT} , X] $, $ ^{} = ({},) $ and \\(_NT\\) is a vector of ones of dimension NT. Also, (2.2) can be written as \\[\\begin{equation} u=Z_\\mu \\mu +v \\end{equation}\\] (2.4) \\[ y_{it} = \\alpha + X_{it}^{&#39;} + U_{it} \\] , $ i=1,,N ; t=1,,T $ with i denoting households, individuals, firms, countries, etc. and t denoting time. The i subscript, therefore, denotes the cross-section dimension whereas t denotes the time-series dimension. $ $ is a scalar, $ $ is $ K × 1 $ and $X_{it} $is the $ it^{th} $ observation on K explanatory variables. disturbances, with it \\[ u_{it}=u_i + v_{it} \\] where i denotes the unobservable individual-specific effect and \\({it}\\) denotes the remainder disturbance. For example, in an earnings equation in labor economics, \\(y_{it}\\) will measure earnings of the head of the household, whereas Xi t may contain a set of variables like experience, education, union membership, sex, race, etc. Note that i is time-invariant and it accounts for any individual-specific effect that is not included in the regression. In this case we could think of it as the individuals unobserved ability. The remainder disturbance $ {it} $ varies with individuals and time and can be thought of as the usual disturbance in the regression. Alternatively, for a production function utilizing data on firms across time, $ y_{it} $ will measure output and $ X_{it} $ will measure inputs. The unobservable firm-specific effects will be captured by the $ _i $ and we can think of these as the unobservable entrepreneurial or managerial skills of the firms executives. Early applications of error components in economics include Kuh (1959) on investment, Mundlak (1961) and Hoch (1962) on production functions and Balestra and Nerlove (1966) on demand for natural gas. In vector form (2.1) can be written as \\[ y= \\alpha i_{NT} + X\\beta +u = Z\\delta + u \\] where \\(y\\) is $ NT × 1$ , X is \\(NT × K\\) , $ Z = [i_{NT} , X] $ , $ ^{}=(^,^{})$ and \\(i_{NT}\\) is a vector of ones of dimension \\(NT\\) . Also, (2.2) can be written as \\[\\begin{equation} u=Z_\\mu \\mu + v\\end{equation}\\] (2.4) where $ u^{} = (u_{11}, . . . , u_{1T} , u_{21}, . . . , u_{2T}, . . . , u_{N1}, . . . , u_{NT} ) $ with the observations stacked such that the slower index is over individuals and the faster index is over time.$ Z_= IN T $ where IN is an identity matrix of dimension N, T is a vector of ones of dimension T and $ $ denotes Kronecker product. $ Z_$ is a selector matrix of ones and zeros, or simply the matrix of individual dummies that one may include in the regression to estimate thei if they are assumed to be fixed parameters. $ ^{} = (_1, . . . , _N )$ and \\(^{&#39;} = (11, . . . , _{1T} , . . . , _{N1}, . . . , _{NT} )\\). Note that $Z_Z^{}= I_N J_T $ where \\(J_T\\) is a matrix of ones of dimension T and $ P = Z(Z{}Z){1} Z^{}$ , the projectionmatrix on $ Z_$ , reduces to $ IN J_T $ where $J _T= JT /T $ . P is a matrix which averages the observation across time for each individual, and Q = INT  P is a matrix which obtains the deviations from individual means. For example, regressing y on the matrix of dummy variables \\(Z_\\) gets the predicted values $ P_y $ which has a typical element \\[ \\bar y_i = \\sum_{t=1}^T \\frac{y_{it}}{T} \\] repeated T times for each individual. The residuals of this regression are given by Qy which has a typical element \\[ (y_{it} - \\bar y _{i.} ) \\] P and Q are (i) symmetric idempotent matrices, i.e. \\(P^{&#39;} = P\\) and $ P^2 = P $. This means that \\(rank(P) = tr(P) = N\\) and \\(rank(Q) = tr(Q) = N(T  1)\\) . This uses the result that the rank of an idempotent matrix is equal to its trace (see Graybill, 1961,theorem 1.63). Also, (ii) P and Q are orthogonal, i.e. $PQ=0 $ and (iii) they sum to the identity matrix \\(P + Q = I_{NT}\\). In fact, any two of these properties imply the third (see Graybill, 1961, theorem 1.68). 2.2 THE FIXED EFFECTS MODEL In this case, the i are assumed to be fixed parameters to be estimated and the remainder disturbances stochastic with \\(v_{it}\\) independent and identically distributed \\(IID(0,\\sigma_v^2)\\). The \\(X_{it}\\) are assumed independent of the \\(v_{it}\\) for all i and t. The fixed effects model is an appropriate specification if we are focusing on a specific set of N firms, say, IBM, GE,Westinghouse, etc. and our inference is restricted to the behavior of these sets of firms. Alternatively, it could be a set of N OECD countries, or N American states. Inference in this case is conditional on the particular N firms, countries or states that are observed. One can substitute the disturbances given by (2.4) into (2.3) to get \\[\\begin{equation} y=\\alpha i\\_{IT} + X\\beta + Z\\_\\mu \\mu +v = Z\\delta + Z\\_\\mu \\mu +v \\end{equation}\\] (2.5) and then perform ordinary least squares (OLS) on (2.5) to get estimates of $ , and µ $ Note that Z is $ NT (K+1)$ and \\(Z_µ\\) , the matrix of individual dummies, is$NT N $ NT × N. If N is large, (2.5) will include too many individual dummies, and the matrix to be inverted by OLS is large and of dimension \\((N + K)\\). In fact, since  and  are the parameters of interest, one can obtain the LSDV (least squares dummy variables) estimator from (2.5), by premultiplying the model by Q and performing OLS on the resulting transformed model: \\[\\begin{equation} Qy=QX\\beta + Qv\\end{equation}\\] (2.6) This uses the fact that \\(QZ_\\mu =Qi_{NT}=0\\) , since \\(PZ_\\mu =Z_\\mu\\) the Q matrix wipes out the individual effects. This is a regression of \\(\\tilde y =QY\\) with element \\((y_{it} - \\bar y _{i.} )\\) on \\(\\check X=QX\\) with typical element \\[\\begin{equation} \\widetilde{\\beta}=\\left(X^{\\prime} Q X\\right)^{-1} X^{\\prime} Q y \\end{equation}\\] (2.7) (2.7) with \\(\\operatorname{var}(\\widetilde{\\beta})=\\sigma_{v}^{2}\\left(X^{\\prime} Q X\\right)^{-1}=\\sigma_{v}^{2}\\left(\\widetilde{X}^{\\prime} \\tilde{X}\\right)^{-1}\\) . \\(\\widetilde{\\beta}\\) could have been obtained from (2.5) using results on partitioned inverse or the FrischWaughLovell theorem discussed in Davidson and MacKinnon (1993, p. 19). This uses the fact that P is the projection matrix on \\(Z_{\\mu}\\) and \\(Q = I_{NT}  P\\) (see problem 2.1). In addition, generalized least squares (GLS) on (2.6), using the generalized inverse, will also yield \\(\\widetilde{\\beta }\\) (see problem 2.2). Note that for the simple regression \\[\\begin{equation}y_{it}=\\beta x_{it}+ \\mu_i+ v_i \\end{equation}\\] (2.8) and averaging over time gives \\[\\begin{equation} \\bar {y}_{i.}= \\beta \\bar{x}_{i.}+ \\mu_i+ \\bar{v}_i \\end{equation}\\] (2.9) Therefore, subtracting (2.9) from (2.8) gives \\[y_{it}-\\bar{y}_{i.}=\\beta (x_{it}-\\bar{x}_{i.}) + (v_{it}-\\bar{v}_{i.}) \\] (2.10) Also, averaging across all observations in (2.8) gives \\[\\bar{y}_{..}=\\alpha + \\beta \\bar{x}_{..} + \\bar{v}_{..} \\] (2.11)i =0 where we utilized the restriction that ${i=1}^{n} =0 $ This is an arbitrary restriction on the dummy variable coefficients to avoid the dummy variable trap, or perfect multicollinearity; see Suits (1984) for alternative formulations of this restriction. In fact only \\(\\beta\\) and $ (+ {i})$ are estimable from (2.8), and not  and i separately, unless a restriction like* \\[\\sum_{i=1}^{n} \\mu_{i}=0 \\] is imposed. In this case, \\(\\widetilde {\\beta}\\) is obtained from regression (2.10), \\[\\widetilde {\\alpha}= \\bar {y}_{..}-\\widetilde {\\beta} \\bar{x}_{..} \\] can be recovered from (2.11) and \\[\\widetilde {\\mu}_{i}=\\bar{y}_{i.}- \\widetilde {\\alpha} - \\widetilde {\\beta} \\bar{X}_{i.} \\] from (2.9). For large labor or consumer panels, where N is very large,regressions like (2.5) may not be feasible, since one is including (N  1) dummies in the regression. This fixed effects (FE) least squares, also known as least squares dummy variables (LSDV), suffers from a large loss of degrees of freedom. We are estimating (N  1) extra parameters, and too many dummies may aggravate the problem of multicollinearity among the regressors. In addition, this FE estimator cannot estimate the effect of any time-invariant variable like sex, race, religion, schooling or union participation. These time-invariant variables are wiped out by the Q transformation, the deviations from means transformation (see (2.10)). Alternatively, one can see that these time-invariant variables are spanned by the individual dummies in (2.5) and therefore any regression package attempting (2.5) will fail, signaling perfect multicollinearity. If (2.5) is the true model, LSDV is the best linear unbiased estimator (BLUE) as long as \\(v_{it}\\) is the standard classical disturbance with mean 0 and variancecovariance matrix $^{2} I_{NT} $ . Note that as \\(T \\rightarrow \\infty\\) the FE estimator is consistent. However, if T is fixed and \\(N \\rightarrow \\infty\\) as is typical in short labor panels, then only the FE estimator of  is consistent; the FE estimators of the individual effects \\(\\alpha + \\mu_{i}\\) are not consistent since the number of these parameters increases as N increases. This is the incidental parameter problem discussed by Neyman and Scott (1948) and reviewed more recently by Lancaster (2000). Note that when the true model is fixed effects as in (2.5), OLS on (2.1) yields biased and inconsistent estimates of the regression parameters. This is an omission variables bias due to the fact that OLS deletes the individual dummies when in fact they are relevant. Testing for fixed effects. One could test the joint significance of these dummies, i.e. H0; \\(\\mu_{1}=\\mu_{2}= ... = \\mu_{N-1}=0\\) , by performing an F-test. (Testing for individual effects will be treated extensively in Chapter 4.) This is a simple Chow test with the restricted residual sums of squares (RRSS) being that of OLS on the pooled model and the unrestricted residual sums of squares (URSS) being that of the LSDV regression. If N is large, one can perform the Within transformation and use that residual sum of squares as the URSS. In this case \\[F_{0}= \\frac{ \\dfrac{RRSS-URSS}{N-1}}{\\dfrac{URSS}{NT-N-K} } \\sim F_{N-1,N(T-1)-K} \\] (2.12) Computational warning. One computational caution for those using theWithin regression given by (2.10). The \\(s^{2}\\) f this regression as obtained from a typical regression package divides the residual sums of squares by NT  K since the intercept and the dummies are not included. The proper \\(s^{2}\\) , say \\(s^{*2}\\) from the LSDV regression in (2.5), would divide the same residual sums of squares by \\(N(T  1)  K\\) . Therefore, one has to adjust the variances obtained from the Within regression (2.10) by multiplying the variancecovariance matrix by \\[\\frac{s^{2}}{s^{*2}} \\] or simply by multiplying by \\([NT  K]/[N(T  1)  K]\\) Robust estimates of the standard errors. For the Within estimator, Arellano (1987) suggests a simple method for obtaining robust estimates of the standard errors that allow for a general variancecovariance matrix on the \\(v_{it}\\) as in White (1980). One would stack the panel as an equation for each individual: \\[y_{i}= Z_{i} \\delta + \\mu i_{it} + v_{i} \\] (2.13) where \\(y_i\\) is T × $Z_i =[l_T,X_i] $, \\(X_i\\) is T × K, \\(\\mu _i\\) is a scalar, \\(\\delta ^{\\prime} = (\\alpha, \\beta^{\\prime} )\\) , \\(i_T\\) is a vector of ones of dimension T and \\(v_i\\) is T x 1 . In general, \\(E(v_i,v_i^{\\prime}) = \\Omega _ i\\) for i = 1, 2, . . . , N, where \\(\\Omega _i\\) is a positive definite matrix of dimension T . We still assume \\(E(v_i,v_j^{\\prime}) =0\\) for \\(i \\ne j\\) T is assumed small and N large as in household or company panels, and the asymptotic results are performed for \\(N \\rightarrow \\infty\\) and T fixed. Performing the Within transformation on this set of equations (2.13) one gets \\[ \\widetilde {y}_i=\\widetilde{X}_{i} \\beta + \\widetilde{v}_{i} \\] (2.14) where \\[\\widetilde{y}=Qy \\] , \\[\\widetilde{X}=QX \\] and \\[\\widetilde{v}=Qv \\] , with \\[\\widetilde{y}=(\\widetilde{y}_1^{\\prime}, ...,\\widetilde{y}_N^{\\prime} )^{\\prime} \\] and \\[\\widetilde{y}_i=(I_T-\\bar{J}_T) y_i \\] Computing robust least squares on this system, as described by White (1980), under the restriction that each equation has the same \\(\\beta\\) one gets the Within estimator of  which has the following asymptotic distribution: \\[N^{\\frac{1}{2} } (\\widetilde{\\beta}- \\beta ) \\sim N(0,M^{-1}VM^{-1} ) \\] (2.15) where \\[M=\\frac{ p\\lim(\\widetilde{X}^{\\prime} \\widetilde{X})} {N} \\] Note that \\[\\widetilde{X}_i=(I_T - \\bar{J}_T) X_i \\] and \\[\\widetilde{X}^{\\prime} diag[\\Omega_i] Q \\widetilde{X} \\] (see problem 2.3). In this case, V is estimated by \\[ \\frac{ \\widetilde{V}=\\sum_{i=1}^{N} \\widetilde{X}_i^{\\prime}\\widetilde{u}_i \\widetilde{u}_i^{\\prime}\\widetilde{X}_i^{\\prime} } {N} \\] where \\[\\widetilde{u}_i=\\widetilde{y}_i- \\widetilde{X}_i \\widetilde{\\beta}_i\\] . Therefore, the robust asymptotic variance covariance matrix of \\(\\beta\\) is estimated by \\[ \\operatorname{var}(\\widetilde{\\beta})=\\left(\\widetilde{X}^{\\prime} \\tilde{X}\\right)^{-1}\\left[\\sum_{i=1}^{N} \\widetilde{X}_{i}^{\\prime} \\tilde{u}_{i} \\tilde{u}_{i}^{\\prime} \\widetilde{X}_{i}\\right]\\left(\\widetilde{X}^{\\prime} \\tilde{X}\\right)^{-1} \\] 2.3 THE RANDOM EFFECTS MODEL There are too many parameters in the fixed effects model and the loss of degrees of freedom can be avoided if the i can be assumed random. In this case $ i* IID(0, ^2)$, $_{it}  IID(0, _v^2 ) $ and the \\(\\mu_i\\) are independent of the \\(v_{it}\\) . In addition, the \\(X_{it}\\) are independent of the \\(\\mu_i\\) and \\(v_{it}\\),for all i and t. The random effects model is an appropriate specification if we are drawing N individuals randomly from a large population. This is usually the case for household panel studies. Care is taken in the design of the panel to make it representative of the population we are trying to make inferences about. In this case, N is usually large and a fixed effects model would lead to an enormous loss of degrees of freedom. The individual effect is characterized as random and inference pertains to the population from which this sample was randomly drawn. But what is the population in this case? Nerlove and Balestra (1996) emphasize Haavelmos (1944) view that the population consists not of an infinity of individuals, in general, but of an infinity of decisions that each individual might make. This view is consistent with a random effects specification. From (2.4), one can compute the variancecovariance matrix \\[\\begin{equation} \\begin{aligned} \\Omega &amp;=E\\left(u u^{\\prime}\\right)=Z_{\\mu} E\\left(\\mu \\mu^{\\prime}\\right) Z_{\\mu}^{\\prime}+E\\left(v v^{\\prime}\\right) \\\\ &amp;=\\sigma_{\\mu}^{2}\\left(I_{N} \\otimes J_{T}\\right)+\\sigma_{v}^{2}\\left(I_{N} \\otimes I_{T}\\right) \\end{aligned} \\end{equation}\\] This implies a homoskedastic variance \\(\\operatorname{var}\\left(u_{i t}\\right)=\\sigma_{\\mu}^{2}+\\sigma_{v}^{2} \\mathrm{f}\\) for all i and t, and an equicorrelated block-diagonal covariance matrix which exhibits serial correlation over time only between the disturbances of the same individual. In fact, \\(\\begin{aligned} \\operatorname{cov}\\left(u_{i t}, u_{j s}\\right) &amp;=\\sigma_{\\mu}^{2}+\\sigma_{v}^{2} &amp; \\text { for } \\quad i=j, t=s \\\\ &amp;=\\sigma_{\\mu}^{2} &amp; \\text { for } i=j, t \\neq s \\end{aligned}\\) and zero otherwise. This also means that the correlation coefficient between \\(\\mu _{it}\\) and \\(\\mu _{js}\\) is \\(\\begin{aligned} \\rho=\\operatorname{correl}\\left(u_{i t}, u_{j s}\\right) &amp;=1 &amp; &amp; \\text { for } i=j, t=s \\\\ &amp;=\\sigma_{\\mu}^{2} /\\left(\\sigma_{\\mu}^{2}+\\sigma_{v}^{2}\\right) &amp; \\text { for } i=j, t \\neq s \\end{aligned}\\) and zero otherwise. In order to obtain the GLS estimator of the regression coefficients, we need $^{-1} $. This is a huge matrix for typical panels and is of dimension \\(NT × NT\\). No brute force inversion should be attempted even if the researchers application has a small N and T .1 We will follow a simple trick devised by Wansbeek and Kapteyn (1982b, 1983) that allows the derivation of $^{-1} $ ans \\(\\Omega^{-1 / 2}\\) Essentially, one replaces \\(J_{T}\\) by \\(T \\bar{J}_{T}\\) and \\(I_{T}\\) by \\(\\left(E_{T}+\\bar{J}_{T}\\right)\\) where \\(E_T\\) is by definition \\(\\left(I_{T}-\\bar{J}_{T}\\right)\\) . In this case \\(\\Omega=T \\sigma_{\\mu}^{2}\\left(I_{N} \\otimes \\bar{J}_{T}\\right)+\\sigma_{v}^{2}\\left(I_{N} \\otimes E_{T}\\right)+\\sigma_{v}^{2}\\left(I_{N} \\otimes \\bar{J}_{T}\\right)\\) Collecting terms with the same matrices, we get \\[\\begin{equation} \\Omega=\\left(T \\sigma_{\\mu}^{2}+\\sigma_{v}^{2}\\right)\\left(I_{N} \\otimes \\bar{J}_{T}\\right)+\\sigma_{v}^{2}\\left(I_{N} \\otimes E_{T}\\right)=\\sigma_{1}^{2} P+\\sigma_{v}^{2} Q \\end{equation}\\] where \\(\\sigma_{1}^{2}=T \\sigma_{\\mu}^{2}+\\sigma_{v}^{2}\\) . (2.18) is the spectral decomposition representation of \\(\\Omega\\), with \\(\\sigma_1^2\\) being the first unique characteristic root of\\(\\Omega\\) of multiplicity \\(N(T  1)\\). It is easy to verify, using the properties of P and Q, that \\[\\begin{equation} \\Omega^{-1}=\\frac{1}{\\sigma_{1}^{2}} P+\\frac{1}{\\sigma_{v}^{2}} Q \\end{equation}\\] and \\[\\begin{equation} \\Omega^{-1 / 2}=\\frac{1}{\\sigma_{1}} P+\\frac{1}{\\sigma_{v}} Q \\end{equation}\\] In fact, \\(\\Omega^{r}=\\left(\\sigma_{1}^{2}\\right)^{r} P+\\left(\\sigma_{v}^{2}\\right)^{r} Q\\) where r is an arbitrary scalar. Now we can obtain GLS as a weighted least squares. Fuller and Battese (1973, 1974) suggested premultiplying the regression equation given in (2.3) by \\(\\sigma_{v} \\Omega^{-1 / 2}=Q+\\left(\\sigma_{v} / \\sigma_{1}\\right) P\\) and performing OLS on the resulting transformed regression. In this case, \\(y^{*}=\\sigma_{v} \\Omega^{-1 / 2} y\\) has a typical element \\(y_{i t}-\\theta \\bar{y}_{i .}\\) where \\(\\theta=1-\\left(\\sigma_{v} / \\sigma_{1}\\right)\\) (see problem 2.4). This transformed regression inverts a matrix of dimension \\((K + 1)\\) and can easily be implemented using any regression package. The best quadratic unbiased (BQU) estimators of the variance components arise naturally from the spectral decomposition of \\(\\Omega\\) . In fact, \\(P u \\sim\\left(0, \\sigma_{1}^{2} P\\right)\\) and \\(Q u \\sim\\left(0, \\sigma_{v}^{2} Q\\right)\\) and \\[\\begin{equation} \\widehat{\\sigma}_{1}^{2}=\\frac{u^{\\prime} P u}{\\operatorname{tr}(P)}=T \\sum_{i=1}^{N} \\bar{u}_{i .}^{2} / N \\end{equation}\\] and \\[\\begin{equation} \\widehat{\\sigma}_{v}^{2}=\\frac{u^{\\prime} Q u}{\\operatorname{tr}(Q)}=\\frac{\\sum_{i=1}^{N} \\sum_{t=1}^{T}\\left(u_{i t}-\\bar{u}_{i .}\\right)^{2}}{N(T-1)} \\end{equation}\\] provide the BQU estimators of \\(\\sigma_1^2\\) and\\(\\sigma_v^2\\) , respectively (see problem 2.5). These are analyses of variance-type estimators of the variance components and are minimum variance-unbiased under normality of the disturbances (see Graybill, 1961). The true disturbances are not known and therefore (2.21) and (2.22) are not feasible. Wallace and Hussain (1969) suggest substituting OLS residual \\(\\widehat{u}_{\\mathrm{OLS}}\\) instead of the true u. After all, under the random effects model, the OLS estimates are still unbiased and consistent, but no longer efficient. Amemiya (1971) shows that these estimators of the variance components have a different asymptotic distribution from that knowing the true disturbances. He suggests using the LSDV residuals instead of the OLS residuals. In this case \\(\\tilde{u}=y-\\tilde{\\alpha} \\iota_{N T}-X \\widetilde{\\beta}\\) where and \\(\\bar{X}_{. .}^{\\prime}\\) is a \\(1 \\times K\\) vector of averages of all regressors. Substituting these \\(\\hat{u}\\) for u in (2.21) and (2.22) we get the Amemiya-type estimators of the variance components. The resulting estimates of the variance components have the same asymptotic distribution as that knowing the true disturbances: \\[\\begin{equation} \\left(\\begin{array}{c} \\sqrt{N T}\\left(\\widehat{\\sigma}_{v}^{2}-\\sigma_{v}^{2}\\right) \\\\ \\sqrt{N}\\left(\\widehat{\\sigma}_{\\mu}^{2}-\\sigma_{\\mu}^{2}\\right) \\end{array}\\right) \\sim N\\left(0,\\left(\\begin{array}{cc} 2 \\sigma_{v}^{4} &amp; 0 \\\\ 0 &amp; 2 \\sigma_{\\mu}^{4} \\end{array}\\right)\\right) \\end{equation}\\] where \\(\\widehat{\\sigma}_{\\mu}^{2}=\\left(\\widehat{\\sigma}_{1}^{2}-\\widehat{\\sigma}_{v}^{2}\\right) / T .^{3}\\) Swamy and Arora (1972) suggest running two regressions to get estimates of the variance components from the corresponding mean square errors of these regressions. The first regression is the Within regression, given in (2.10), which yields the following \\(s^2\\): \\[\\begin{equation} \\widehat{\\sigma}_{v}^{2}=\\left[y^{\\prime} Q y-y^{\\prime} Q X\\left(X^{\\prime} Q X\\right)^{-1} X^{\\prime} Q y\\right] /[N(T-1)-K] \\end{equation}\\] The second regression is the Between regression which runs the regression of averages across time, i.e. \\[\\begin{equation} \\bar{y}_{i .}=\\alpha+\\bar{X}_{i .}^{\\prime} \\beta+\\bar{u}_{i .} \\quad i=1, \\ldots, N \\end{equation}\\] (2.25) This is equivalent to premultiplying the model in (2.5) by P and running OLS. The only caution is that the latter regression has NT observations because it repeats the averages T times for each individual, while the cross-section regression in (2.25) is based on N observations. To remedy this, one can run the cross-section regression \\[\\begin{equation} \\sqrt{T} \\bar{y}_{i .}=\\alpha \\sqrt{T}+\\sqrt{T} \\bar{X}_{i .}^{\\prime} \\beta+\\sqrt{T} \\bar{u}_{i .} \\end{equation}\\] (2.26) where one can easily verify that \\(\\operatorname{var}\\left(\\sqrt{T} \\bar{u}_{i .}\\right)=\\sigma_{1}^{2} .\\) This regression will yield an \\(s^{2}\\) given by \\[\\begin{equation} \\widehat{\\sigma}_{1}^{2}=\\left(y^{\\prime} P y-y^{\\prime} P Z\\left(Z^{\\prime} P Z\\right)^{-1} Z^{\\prime} P y\\right) /(N-K-1) \\end{equation}\\] (2.27) Note that stacking the following two transformed regressions we just performed yields \\[\\begin{equation} \\left(\\begin{array}{l} Q y \\\\ P y \\end{array}\\right)=\\left(\\begin{array}{l} Q Z \\\\ P Z \\end{array}\\right) \\delta+\\left(\\begin{array}{c} Q u \\\\ P u \\end{array}\\right) \\end{equation}\\] (2.28) and the transformed error has mean 0 and variancecovariance matrix given by \\[ \\left(\\begin{array}{cc} \\sigma_{v}^{2} Q &amp; 0 \\\\ 0 &amp; \\sigma_{1}^{2} P \\end{array}\\right) \\] Problem 2.7 asks the reader to verify that OLS on this system of 2NT observations yields OLS on the pooled model (2.3). Also, GLS on this system yields GLS on (2.3). Alternatively, one could get rid of the constant  by running the following stacked regressions: \\[\\begin{equation} \\left(\\begin{array}{c} Q y \\\\ \\left(P-\\bar{J}_{N T}\\right) y \\end{array}\\right)=\\left(\\begin{array}{c} Q X \\\\ \\left(P-\\bar{J}_{N T}\\right) X \\end{array}\\right) \\beta+\\left(\\begin{array}{c} Q u \\\\ \\left(P-\\bar{J}_{N T}\\right) u \\end{array}\\right) \\end{equation}\\] (2.29) This follows from the fact that QNT = 0 and (P  J¯NT )NT = 0. The transformed error has zero mean and variancecovariance matrix \\[ \\left(\\begin{array}{cc} \\sigma_{v}^{2} Q &amp; 0 \\\\ 0 &amp; \\sigma_{1}^{2}\\left(P-\\bar{J}_{N T}\\right) \\end{array}\\right) \\] OLS on this system yields OLS on (2.3) and GLS on (2.29) yields GLS on (2.3). In fact, \\[\\begin{equation} \\begin{aligned} \\widehat{\\beta}_{\\mathrm{GLS}}=&amp;\\left[\\left(X^{\\prime} Q X / \\sigma_{v}^{2}\\right)+X^{\\prime}\\left(P-\\bar{J}_{N T}\\right) X / \\sigma_{1}^{2}\\right]^{-1}\\left[\\left(X^{\\prime} Q y / \\sigma_{v}^{2}\\right)\\right.\\\\ &amp;\\left.+X^{\\prime}\\left(P-\\bar{J}_{N T}\\right) y / \\sigma_{1}^{2}\\right] \\\\ =&amp;\\left[W_{X X}+\\phi^{2} B_{X X}\\right]^{-1}\\left[W_{X y}+\\phi^{2} B_{X y}\\right] \\end{aligned} \\end{equation}\\] (2.30) with \\(\\operatorname{var}\\left(\\widehat{\\beta}_{\\mathrm{GLS}}\\right)=\\sigma_{v}^{2}\\left[W_{X X}+\\phi^{2} B_{X X}\\right]^{-1} .\\) Note that \\(W_{X X}=X^{\\prime} Q X, B_{X X}=X^{\\prime}\\left(P-\\bar{J}_{N T}\\right) X\\) and \\(\\phi^{2}=\\sigma_{v}^{2} / \\sigma_{1}^{2}\\). Also, the Within estimator of \\(\\beta\\) is \\(\\underline{\\beta}_{\\text {Within }}=W_{X X}^{-1} W_{X y}\\) and the Between estimator of \\(\\beta\\) is \\(\\widehat{\\beta}_{\\text {Between }}=B_{X X}^{-1} B_{X y} .\\) This shows that \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) is a matrix weighted average of \\(\\widetilde{\\beta}_{\\text {Within and }} \\widehat{\\beta}_{\\text {Between }}\\) weighing each estimate by the inverse of its corresponding variance. In fact \\[\\begin{equation} \\widehat{\\beta}_{\\mathrm{GLS}}=W_{1} \\widetilde{\\beta}_{\\mathrm{Within}}+W_{2} \\widehat{\\beta}_{\\mathrm{Between}} \\end{equation}\\] (2.31) where \\[ W_{1}=\\left[W_{X X}+\\phi^{2} B_{X X}\\right]^{-1} W_{X X} \\] and \\[ W_{2}=\\left[W_{X X}+\\phi^{2} B_{X X}\\right]^{-1}\\left(\\phi^{2} B_{X X}\\right)=I-W_{1} \\] This was demonstrated by Maddala (1971). Note that (i) if \\(\\sigma_{\\mu}^{2}=0\\) then \\(\\phi^{2}=1\\) and \\(\\widehat{\\beta}_{\\text {GLS }}\\) reduces to \\(\\widehat{\\beta}_{\\text {OLs. }}\\) (ii) If \\(T \\rightarrow \\infty\\), then \\(\\phi^{2} \\rightarrow 0\\) and \\(\\widehat{\\beta}_{\\text {GLS }}\\) tends to \\(\\widetilde{\\beta}_{\\text {Within. }}\\) Also, if \\(W_{X X}\\) is huge compared to \\(B_{X X}\\) then \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) will be close to \\(\\widetilde{\\beta}_{\\text {Within }}\\). However, if \\(B_{X X}\\) dominates \\(W_{X X}\\) then \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) tends to \\(\\widehat{\\beta}_{\\text {Between. }}\\) In other words, the Within estimator ignores the Between variation, and the Between estimator ignores the Within variation. The OLS estimator gives equal weight to the Between and Within variations. From \\((2.30)\\), it is clear that \\(\\operatorname{var}\\left(\\widetilde{\\beta}_{\\text {Within }}\\right)-\\operatorname{var}\\left(\\hat{\\beta}_{\\mathrm{GLS}}\\right)\\) is a positive semidefinite matrix, since \\(\\phi^{2}\\) is positive. However, as \\(T \\rightarrow \\infty\\) for any fixed \\(N, \\phi^{2} \\rightarrow 0\\) and both \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) and \\(\\widetilde{\\beta}_{\\text {Within }}\\) have the same asymptotic variance. Another estimator of the variance components was suggested by Nerlove (1971a). His zuggestion is to estimate \\(\\sigma^{2}\\) as \\(\\sum^{N} \\cdot(\\widehat{u} \\cdot-\\widehat{\\pi})^{2} /(N-1)\\) where \\(\\widehat{u}\\). are the dummv coefficients estimates from the LSDV regression. \\(\\sigma_{v}^{2}\\) is estimated from the Within residual sums of squares divided by \\(N T\\) without correction for degrees of freedom. \\({ }^{4}\\) Note that, except for Nerloves (1971a) method, one has to retrieve \\(\\widehat{\\sigma}_{\\mu}^{2}\\) as \\(\\left(\\widehat{\\sigma}_{1}^{2}-\\widehat{\\sigma}_{v}^{2}\\right) / T .\\) In this case, there is no guarantee that the estimate of \\(\\widehat{\\sigma}_{\\mu}^{2}\\) would be nonnegative. Searle (1971) has an extensive discussion of the problem of negative estimates of the variance components in the biometrics literature. One solution is to replace these negative estimates by zero. This in fact is the suggestion of the Monte Carlo study by Maddala and Mount (1973). This study finds that negative estimates occurred only when the true \\(\\sigma_{\\mu}^{2}\\) was small and close to zero. In these cases OLS is still a viable estimator. Therefore, replacing negative \\(\\widehat{\\sigma}_{\\mu}^{2}\\) by zero is not a bad sin after all, and the problem is dismissed as not being serious. \\({ }^{5}\\) How about the properties of the various feasible GLS estimators of \\(\\beta\\) Under the random effects model, GLS based on the true variance components is BLUE, and all the feasible GLS estimators considered are asymptotically efficient as either N or $N $. Maddala and Mount (1973) compared OLS, Within, Between, feasible GLS methods, MINQUE, Hendersons method III, true GLS and maximum likelihood estimation using their Monte Carlo study. They found little to choose among the various feasible GLS estimators in small samples and argued in favor of methods that were easier to compute. MINQUE was dismissed as more difficult to compute and the applied researcher given one shot at the data was warned to compute at least two methods of estimation, like an ANOVA feasible GLS and maximum likelihood to ensure that they do not yield drastically different results. If they do give different results, the authors diagnose misspecification. Taylor (1980) derived exact finite sample results for the one-way error component model. He compared the Within estimator with the SwamyArora feasible GLS estimator. He found the following important results: Feasible GLS is more efficient than LSDV for all but the fewest degrees of freedom. The variance of feasible GLS is never more than 17% above the CramerRao lower bound. More efficient estimators of the variance components do not necessarily yield more efficient feasible GLS estimators. These finite sample results are confirmed by the Monte Carlo experiments carried out by Maddala and Mount (1973) and Baltagi (1981a). Bellmann, Breitung and Wagner (1989) consider the bias in estimating the variance components using the Wallace and Hussain (1969) method due to the replacement of the true disturbances by OLS residuals, also the bias in the regression coefficients due to the use of estimated variance components rather than the true variance components. The magnitude of this bias is estimated using bootstrap methods for two economic applications. The first application relates product innovations, import pressure and factor inputs using a panel at the industry level. The second application estimates the earnings of 936 full-time working German males based on the first and second wave of the German Socio-Economic Panel. Only the first application revealed considerable bias in estimating $_^2 $. However, this did not affect the bias much in the corresponding regression coefficients 2.3.1 Fixed vs Random Having discussed the fixed effects and the random effects models and the assumptions underlying them, the reader is left with the daunting question, which one to choose? This is not as easy a choice as it might seem. In fact, the fixed versus random effects issue has generated a hot debate in the biometrics and statistics literature which has spilled over into the panel data econometrics literature. Mundlak (1961) and Wallace and Hussain (1969) were early proponents of the fixed effects model and Balestra and Nerlove (1966) were advocates of the random error component model. In Chapter 4, we will study a specification test proposed by Hausman (1978) which is based on the difference between the fixed and random effects estimators. Unfortunately, applied researchers have interpreted a rejection as an adoption of the fixed effects model and nonrejection as an adoption of the random effects model.6 Chamberlain (1984) showed that the fixed effects model imposes testable restrictions on the parameters of the reduced form model and one should check the validity of these restrictions before adopting the fixed effects model (see Chapter 4). Mundlak (1978) argued that the random effects model assumes exogeneity of all the regressors with the random individual effects. In contrast, the fixed effects model allows for endogeneity of all the regressors with these individual effects. So, it is an all or nothing choice of exogeneity of the regressors and the individual effects, see Chapter 7 for a more formal discussion of this subject. Hausman and Taylor (1981) allowed for some of the regressors to be correlated with the individual effects, as opposed to the all or nothing choice. These over-identification restrictions are testable using a Hausman-type test (see Chapter 7). For the applied researcher, performing fixed effects and random effects and the associated Hausman test reported in standard packages like Stata, LIMDEP, TSP, etc., the message is clear: Do not stop here. Test the restrictions implied by the fixed effects model derived by Chamberlain (1984) (see Chapter 4) and check whether a Hausman and Taylor (1981) specification might be a viable alternative (see Chapter 7). "],["the-two-way-error-component-regression-model.html", "Chapter 3 The Two-way Error Component Regression Model 3.1 NTRODUCTION 3.2 THE FIXED EFFECTS MODEL 3.3 THE RANDOM EFFECTS MODEL 3.4 REFERENCES", " Chapter 3 The Two-way Error Component Regression Model 3.1 NTRODUCTION Wallace and Hussain (1969), Nerlove (1971b) and Amemiya (1971), among others, the regression model given by (2.1), but with two-way error components disturbances: \\[\\begin{equation} u_{i t}=\\mu_{i}+\\lambda_{t}+v_{i t} \\quad i=1, \\ldots, N ; t=1, \\ldots, T \\end{equation}\\] (3.1) where \\(\\mu_i\\) denotes the unobservable individual effect discussed in Chapter 2, \\(\\lambda_t\\) denotes the unobservable time effect and \\(v_{it}\\) is the remainder stochastic disturbance term. Note that \\(\\lambda_t\\) is individual-invariant and it accounts for any time-specific effect that is not included in the regression. For example, it could account for strike year effects that disrupt production; oil embargo effects that disrupt the supply of oil and affect its price; Surgeon General reports on the ill-effects of smoking, or government laws restricting smoking in public places, all of which could affect consumption behavior. In vector form, (3.1) can be written as \\[\\begin{equation} u=Z_{\\mu} \\mu+Z_{\\lambda} \\lambda+v \\end{equation}\\] (3.2) where \\(Z_\\mu, \\mu\\) and \\(v\\) were defined earlier. \\(Z_\\lambda = i_N \\otimes I_T\\) is the matrix of time dummies that one may include in the regression to estimate the \\(\\lambda_t\\) if they are fixed parameters, and \\(\\lambda^{\\prime}= (\\lambda_1, ..., \\lambda_T )\\) Note that \\[Z_\\lambda Z_\\lambda^ {\\prime}=J_N \\otimes I_T \\] and the projection on \\(Z_{\\lambda}\\) is \\[Z_{\\lambda}\\left(Z_{\\lambda}^{\\prime} Z_{\\lambda}\\right)^{-1} Z_{\\lambda}^{\\prime}=\\bar{J}_{N} \\otimes I_T \\] This last matrix averages the data over individuals, i.e., if we regress y on \\(Z_\\lambda\\), the predicted values are given by \\[ (\\bar {J_N} \\otimes I_T)y \\] which has typical element \\(\\bar{y}_{. t}=\\sum_{i=1}^{N} y_{i t} / N\\) . 3.2 THE FIXED EFFECTS MODEL If the \\(\\mu_{i}\\) and \\(\\lambda_{t}\\) are assumed to be fixed parameters to be estimated and the remainder disturbances stochastic with \\(v_{i t} \\sim \\operatorname{IID}\\left(0, \\sigma_{v}^{2}\\right)\\), then (3.1) represents a two-way fixed effects error component model. The \\(X_{i t}\\) are assumed independent of the \\(v_{i t}\\) for all \\(i\\) and \\(t\\). Inference in this case is conditional on the particular \\(N\\) individuals and over the specific time periods observed. Recall that \\(Z_{\\lambda}\\), the matrix of time dummies, is \\(N T \\times T\\). If \\(N\\) or \\(T\\) is large, there will be too many dummy variables in the regression \\(\\{(N-1)+(T-1)\\}\\) of them, and this causes an enormous loss in degrees of freedom. In addition, this attenuates the problem of multicollinearity among the regressors. Rather than invert a large \\((N+T+K-1)\\) matrix, one can obtain the fixed effects estimates of \\(\\beta\\) by performing the following Within transformation given by Wallace and Hussain (1969): \\[\\begin{equation} Q=E_{N} \\otimes E_{T}=I_{N} \\otimes I_{T}-I_{N} \\otimes \\bar{J}_{T}-\\bar{J}_{N} \\otimes I_{T}+\\bar{J}_{N} \\otimes \\bar{J}_{T} \\end{equation}\\] (3.3) where \\(E_{N}=I_{N}-\\bar{J}_{N}\\) and \\(E_{T}=I_{T}-\\bar{J}_{T}\\). This transformation sweeps the \\(\\mu_{i}\\) and \\(\\lambda_{t}\\) effects. In fact, \\(\\tilde{y}=Q y\\) has a typical element \\(\\tilde{y}_{i t}=\\left(y_{i t}-\\bar{y}_{i .}-\\bar{y}_{i t}+\\bar{y} . .\\right)\\) where \\(\\bar{y}_{. .}=\\sum_{i} \\sum_{t} y_{i t} /\\) \\(N T\\), and one would perform the regression of \\(\\tilde{y}=Q y\\) on \\(\\widetilde{X}=Q X\\) to get the Within estimator \\(\\widetilde{\\beta}=\\left(X^{\\prime} Q X\\right)^{-1} X^{\\prime} Q y\\) Note that by averaging the simple regression given in (2.8) over individuals, we get \\[\\begin{equation} \\bar{y}_{. t}=\\alpha+\\beta \\bar{x}_{. t}+\\lambda_{t}+\\bar{v}_{. t} \\end{equation}\\] (3.4) where we have utilized the restriction that \\(\\sum_i \\mu_i=0\\) to avoid the dummy variable trap. Similarly the averages defined in (2.9) and (2.11) still hold using \\(\\sum_t \\lambda_t=0\\) and one can deduce that \\[\\begin{equation} \\left(y_{i t}-\\bar{y}_{i .}-\\bar{y}_{. t}+\\bar{y}_{. .}\\right)=\\left(x_{i t}-\\bar{x}_{i .}-\\bar{x}_{. t}+\\bar{x}_{. .}\\right) \\beta+\\left(v_{i t}-\\bar{v}_{i .}-\\bar{v}_{. t}+\\bar{v}_{. .}\\right) \\end{equation}\\] (3.5) OLS on this model gives \\(\\widetilde {\\beta}\\) the Within estimator for the two-way model. Once again, the Within estimate of the intercept can be deduced from \\[\\widetilde {\\alpha}=\\bar{y}_..-\\widetilde {\\beta} \\bar{x} \\] and those of \\(\\mu_i\\) and \\(\\lambda_i\\) are given by \\[\\begin{equation} \\tilde{\\mu}_{i}=\\left(\\bar{y}_{i .}-\\bar{y}_{. .}\\right)-\\widetilde{\\beta}\\left(\\bar{x}_{i .}-\\bar{x}_{. .}\\right) \\end{equation}\\] (3.6) \\[\\begin{equation} \\tilde{\\lambda}_{t}=\\left(\\bar{y}_{. t}-\\bar{y}_{. .}\\right)-\\widetilde{\\beta}\\left(\\bar{x}_{. t}-\\bar{x}_{. .}\\right) \\end{equation}\\] (3.7) Note that the Within estimator cannot estimate the effect of time-invariant and individualinvariant variables because the Q transformation wipes out these variables. If the true model is a two-way fixed effects model as in (3.2), then OLS on (2.1) yields biased and inconsistent estimates of the regression coefficients. OLS ignores both sets of dummy variables, whereas the one-way fixed effects estimator considered in Chapter 2 ignores only the time dummies. If these time dummies are statistically significant, the one-way fixed effects estimator will also suffer from omission bias. 3.2.1 Testing for Fixed Effects As in the one-way error component model case, one can test for joint significance of the dummy variables: \\[ H_{0}: \\mu_{1}=\\ldots=\\mu_{N-1}=0 \\quad \\text { and } \\quad \\lambda_{1}=\\ldots=\\lambda_{T-1}=0 \\] The restricted residual sums of squares (RRSS) is that of pooled OLS and the unrestricted residual sums of squares (URSS) is that from the Within regression in (3.5). In this case, \\[\\begin{equation} F_{1}=\\frac{(\\mathrm{RRSS}-\\mathrm{URSS}) /(N+T-2)}{\\mathrm{URSS} /(N-1)(T-1)-K} \\stackrel{H_{0}}{\\sim} F_{(N+T-2),(N-1)(T-1)-K} \\end{equation}\\] (3.8) Next, one can test for the existence of individual effects allowing for time effects, i.e. \\(H_{2}: \\mu_{1}==u_{N}=0 \\quad\\) allowing \\(\\quad \\lambda \\neq 0\\) for \\(t=1 \\ldots \\quad T-1\\) The URSS is still the Within residual sum of squares. However, the RRSS is the regression with time-series dummies only, or the regression based upon \\[\\begin{equation} \\left(y_{i t}-\\bar{y}_{. t}\\right)=\\left(x_{i t}-\\bar{x}_{. t}\\right) \\beta+\\left(u_{i t}-\\bar{u}_{. t}\\right) \\end{equation}\\] (3.9) In this case the resulting \\(F\\) -statistic is \\(F_{2} \\stackrel{H_{0}}{\\sim} F_{(N-1),(N-1)(T-1)-K}\\). Note that \\(F_{2}\\) differs from \\(F_{0}\\) in ( \\(2.12\\) ) in testing for \\(\\mu_{i}=0\\). The latter tests \\(H_{0}: \\mu_{i}=0\\) assuming that \\(\\lambda_{t}=0\\), whereas the former tests \\(H_{2}: \\mu_{i}=0\\) allowing \\(\\lambda_{t} \\neq 0\\) for \\(t=1, \\ldots, T-1\\). Similarly, one can test for the existence of time effects allowing for individual effects, i.e. \\[\\begin{equation} H_{3}: \\lambda_{1}=\\ldots=\\lambda_{T-1}=0 \\quad \\text { allowing } \\quad \\mu_{i} \\neq 0 ; i=1, \\ldots,(N-1) \\end{equation}\\] The RRSS is given by the regression in (2.10), while the URSS is obtained from the regression (3.5). In this case, the resulting F-statistic is \\(F_{3} \\sim^{H_{0}} F_{(T-1),(N-1)(T-1)-K}\\) . Computational Warning As in the one-way model, \\(s^2\\) from the regression in (3.5) as obtained from any standard regression package has to be adjusted for loss of degrees of freedom. In this case, one divides by$ (N  1)(T  1)  K $ and multiplies by $(NT  K) $to get the proper variancecovariance matrix of the Within estimator. 3.3 THE RANDOM EFFECTS MODEL If \\(\\mu_{i} \\sim \\operatorname{IID}\\left(0, \\sigma_{\\mu}^{2}\\right), \\lambda_{t} \\sim \\operatorname{IID}\\left(0, \\sigma_{\\lambda}^{2}\\right)\\) and \\(v_{i t} \\sim \\operatorname{IID}\\left(0, \\sigma_{v}^{2}\\right)\\) independent of each other, then this is the two-way random effects model. In addition, \\(X_{i t}\\) is independent of \\(\\mu_{i}, \\lambda_{t}\\) and \\(v_{i t}\\) for all \\(i\\) and \\(t\\). Inference in this case pertains to the large population from which this sample was randomly drawn. From (3.2), one can compute the variance-covariance matrix \\[\\begin{equation} \\begin{aligned} \\Omega &amp;=E\\left(u u^{\\prime}\\right)=Z_{\\mu} E\\left(\\mu \\mu^{\\prime}\\right) Z_{\\mu}^{\\prime}+Z_{\\lambda} E\\left(\\lambda \\lambda^{\\prime}\\right) Z_{\\lambda}^{\\prime}+\\sigma_{v}^{2} I_{N T} \\\\ &amp;=\\sigma_{\\mu}^{2}\\left(I_{N} \\otimes J_{T}\\right)+\\sigma_{\\lambda}^{2}\\left(J_{N} \\otimes I_{T}\\right)+\\sigma_{v}^{2}\\left(I_{N} \\otimes I_{T}\\right) \\end{aligned} \\end{equation}\\] (3.10) The disturbances are homoskedastic with \\(\\operatorname{var}\\left(u_{i t}\\right)=\\sigma_{\\mu}^{2}+\\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\) for all \\(i\\) and \\(t\\), \\[\\begin{equation} \\begin{array}{rl} \\operatorname{cov}\\left(u_{i t}, u_{j s}\\right)=\\sigma_{\\mu}^{2} &amp; i=j, t \\neq s \\\\ =\\sigma_{\\lambda}^{2} &amp; i \\neq j, t=s \\end{array} \\end{equation}\\] (3.11) and zero otherwise. This means that the correlation coefficient \\[\\begin{equation} \\begin{aligned} \\operatorname{correl}\\left(u_{i t}, u_{j s}\\right) &amp;=\\sigma_{\\mu}^{2} /\\left(\\sigma_{\\mu}^{2}+\\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\right) &amp; &amp; i=j, t \\neq s \\\\ &amp;=\\sigma_{\\lambda}^{2} /\\left(\\sigma_{\\mu}^{2}+\\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\right) &amp; &amp; i \\neq j, t=s \\\\ &amp;=1 &amp; &amp; i=j, t=s \\\\ &amp;=0 &amp; &amp; i \\neq j, t \\neq s \\end{aligned} \\end{equation}\\] (3.12) In order to get \\(\\Omega^{-1}\\), we replace \\(J_{N}\\) by \\(N \\bar{J}_{N}, I_{N}\\) by \\(E_{N}+\\bar{J}_{N}, J_{T}\\) by \\(T \\bar{J}_{T}\\) and \\(I_{T}\\) by \\(E_{T}+\\bar{J}_{T}\\) and collect terms with the same matrices. This gives \\[\\begin{equation} \\Omega=\\sum_{i=1}^{4} \\lambda_{i} Q_{i} \\end{equation}\\] (3.13) where \\(\\lambda_{1}=\\sigma_{v}^{2}, \\lambda_{2}=T \\sigma_{\\mu}^{2}+\\sigma_{v}^{2}, \\lambda_{3}=N \\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\) and \\(\\lambda_{4}=T \\sigma_{\\mu}^{2}+N \\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\). Correspond- ingly, \\(Q_{1}=E_{N} \\otimes E_{T}, Q_{2}=E_{N} \\otimes \\bar{J}_{T}, Q_{3}=\\bar{J}_{N} \\otimes E_{T}\\) and \\(Q_{4}=\\bar{J}_{N} \\otimes \\bar{J}_{T}\\), respectively. The \\(\\lambda_{i}\\) are the distinct characteristic roots of \\(\\Omega\\) and the \\(Q_{i}\\) are the corresponding matrices of eigenprojectors. \\(\\lambda_{1}\\) is of multiplicity \\((N-1)(T-1), \\lambda_{2}\\) is of multiplicity \\((N-1), \\lambda_{3}\\) is of multiplicity \\((T-1)\\) and \\(\\lambda_{4}\\) is of multiplicity \\(1 .{ }^{1}\\) Each \\(Q_{i}\\) is symmetric and idempotent with its rank equal to its trace. Moreover, the \\(Q_{i}\\) are pairwise orthogonal and sum to the identity matrix. The advantages of this spectral decomposition are that \\[\\begin{equation} \\Omega^{r} =\\sum_{i=1}^{4} \\lambda_{i}^{r} Q_{i} \\end{equation}\\] (3.14) where \\(r\\) is an arbitrary scalar so that \\[\\begin{equation} \\sigma_{v} \\Omega^{-1 / 2}=\\sum_{i=1}^{4}\\left(\\sigma_{v} / \\lambda_{i}^{1 / 2}\\right) Q_{i} \\end{equation}\\] (3.15) and the typical element of \\(y^{*}=\\sigma_{\\nu} \\Omega^{-1 / 2} y\\) is given by \\[\\begin{equation} y_{i t}^{*}=y_{i t}-\\theta_{1} \\bar{y}_{i .}-\\theta_{2} \\bar{y}_{. t}+\\theta_{3} \\bar{y}_{. .} \\end{equation}\\] (3.16) where \\(\\theta_{1}=1-\\left(\\sigma_{v} / \\lambda_{2}^{1 / 2}\\right), \\theta_{2}=1-\\left(\\sigma_{v} / \\lambda_{3}^{1 / 2}\\right)\\) and \\(\\theta_{3}=\\theta_{1}+\\theta_{2}+\\left(\\sigma_{v} / \\lambda_{4}^{1 / 2}\\right)-1\\). As a result, GLS can be obtained as OLS of \\(y^{*}\\) on \\(Z^{*}\\), where \\(Z^{*}=\\sigma_{\\nu} \\Omega^{-1 / 2} Z\\). This transformation was first derived by Fuller and Battese (1974), see also Baltagi (1993). The best quadratic unbiased (BQU) estimators of the variance components arise naturally from the fact that \\(Q_{i} u \\sim\\left(0, \\lambda_{i} Q_{i}\\right) .\\) Hence, \\[\\begin{equation} \\widehat{\\lambda}_{i}=u^{\\prime} Q_{i} u / \\operatorname{tr}\\left(Q_{i}\\right) \\end{equation}\\] (3.17) is the BQU estimator of \\(\\lambda_{i}\\) for \\(i=1,2,3\\). These ANOVA estimators are minimum variance unbiased (MVU) under normality of the disturbances (see Graybill, 1961 ). As in the one-way error component model, one can obtain feasible estimates of the variance components by replacing the true disturbances by OLS residuals (see Wallace and Hussain, 1969 ). OLS is still an unbiased and consistent estimator under the random effects model, but it is inefficient and results in biased standard errors and \\(t\\) -statistics. Alternatively, one could substitute the Within residuals with \\(\\tilde{u}=y-\\widetilde{\\alpha} \\iota_{N T}-X \\widetilde{\\beta}\\), where \\(\\tilde{\\alpha}=\\bar{y}_{. .}-\\bar{X}_{. .}^{\\prime} \\tilde{\\beta}\\) and \\(\\widetilde{\\beta}\\) is obtained by the regression in (3.5). This is the method proposed by Amemiya (1971). In fact, Amemiya (1971) shows that the Wallace and Hussain (1969) estimates of the variance components have a different asymptotic distribution from that knowing the true disturbances, while the Amemiya (1971) estimates of the variance components have the same asymptotic distribution as that knowing the true disturbances: \\[\\begin{equation} \\left(\\begin{array}{c} \\sqrt{N T}\\left(\\widehat{\\sigma}_{v}^{2}-\\sigma_{v}^{2}\\right) \\\\ \\sqrt{N}\\left(\\widehat{\\sigma}_{\\mu}^{2}-\\sigma_{\\mu}^{2}\\right) \\\\ \\sqrt{T}\\left(\\widehat{\\sigma}^{2}-\\sigma_{\\mu}^{2}\\right) \\end{array}\\right) \\sim N\\left(0,\\left(\\begin{array}{ccc} 2 \\sigma_{v}^{4} &amp; 0 &amp; 0 \\\\ 0 &amp; 2 \\sigma_{\\mu}^{4} &amp; 0 \\\\ 0 &amp; 0 &amp; 2 \\sigma^{4} \\end{array}\\right)\\right) \\end{equation}\\] (3.18) SubstitutingOLSorWithin residuals instead of the true disturbances in (3.17) introduces bias in the corresponding estimates of the variance components. The degrees of freedom corrections that make these estimates unbiased depend upon traces of matrices that involve the matrix of regressors X. These corrections are given in Wallace and Hussain (1969) and Amemiya (1971), respectively. Alternatively, one can infer these correction terms from the more general unbalanced error component model considered in Chapter 9. Swamy and Arora (1972) suggest running three least squares regressions and estimating the variance components from the corresponding mean square errors of these regressions. The first regression corresponds to the Within regression which transforms the original model by \\(Q_1=E_N \\otimes E_T\\) . This is equivalent to the regression in (3.5), and yields the following estimate of \\(\\sigma_v^2\\) : \\[\\begin{equation} \\widehat{\\lambda}_{1}=\\widehat{\\sigma}_{v}^{2}=\\left[y^{\\prime} Q_{1} y-y^{\\prime} Q_{1} X\\left(X^{\\prime} Q_{1} X\\right)^{-1} X^{\\prime} Q_{1} y\\right] /[(N-1)(T-1)-K] \\end{equation}\\] (3.19) ( à arranger) The second regression is the Between individuals regression which transforms the original model by \\[Q_2=E_N \\otimes \\bar{J}_T \\] This is equivalent to the regression of \\[ (\\bar{y}_{i.}- \\bar{y}_{i..}) \\] on \\[ (\\bar{X}_{i.}- \\bar{X}_{i..}) \\] and yields the following estimate of $ _2=T_v^2 + _v^2 $: \\[\\begin{equation} \\widehat{\\lambda}_{2}=\\left[y^{\\prime} Q_{2} y-y^{\\prime} Q_{2} X\\left(X^{\\prime} Q_{2} X\\right)^{-1} X^{\\prime} Q_{2} y\\right] /[(N-1)-K] \\end{equation}\\] (3.20) from which one obtains \\(\\widehat{\\sigma}_{\\mu}^{2}=\\left(\\widehat{\\lambda}_{2}-\\widehat{\\sigma}_{v}^{2}\\right) / T .\\) The third regression is the Between time-periods regression which transforms the original model by \\(Q_{3}=\\bar{J}_{N} \\otimes E_{T}\\). This is equivalent to the regression of \\(\\left(\\bar{y}_{. t}-\\bar{y}_{. .}\\right)\\) on \\(\\left(\\bar{X}_{. t}-\\bar{X}_{. .}\\right)\\) and yields the following estimate of \\(\\lambda_{3}=N \\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\) \\[\\begin{equation} \\widehat{\\lambda}_{3}=\\left[y^{\\prime} Q_{3} y-y^{\\prime} Q_{3} X\\left(X^{\\prime} Q_{3} X\\right)^{-1} X^{\\prime} Q_{3} y\\right] /[(T-1)-K] \\end{equation}\\] (3.21) from which one obtains \\[ (\\widehat{\\widehat {\\sigma_\\lambda^2} }= \\widehat{\\widehat {\\lambda} }_3 - \\widehat{\\widehat {\\lambda} }_v)/N \\] Stacking the three transformed regressions just performed yields \\[\\begin{equation} \\left(\\begin{array}{l} Q_{1} y \\\\ Q_{2} y \\\\ Q_{3} y \\end{array}\\right)=\\left(\\begin{array}{l} Q_{1} X \\\\ Q_{2} X \\\\ Q_{3} X \\end{array}\\right) \\beta+\\left(\\begin{array}{l} Q_{1} u \\\\ Q_{2} u \\\\ Q_{3} u \\end{array}\\right) \\end{equation}\\] (3.22) since \\(Q_{i} \\iota_{N T}=0\\) for \\(i=1,2,3\\), and the transformed error has mean 0 and variance-covariance matrix given by \\(\\operatorname{diag}\\left[\\lambda_{i} Q_{i}\\right]\\) with \\(i=1,2,3 .\\) Problem \\(3.4\\) asks the reader to show that \\(\\mathrm{OLS}\\) on this system of \\(3 N T\\) observations yields the same estimator of \\(\\beta\\) as OLS on the pooled model (2.3). Also, GLS on this system of equations (3.22) yields the same estimator of \\(\\beta\\) as GLS on (2.3). In fact, \\[\\begin{equation} \\begin{aligned} \\widehat{\\beta}_{\\mathrm{GLS}}=&amp;\\left[\\left(X^{\\prime} Q_{1} X\\right) / \\sigma_{v}^{2}+\\left(X^{\\prime} Q_{2} X\\right) / \\lambda_{2}+\\left(X^{\\prime} Q_{3} X\\right) / \\lambda_{3}\\right]^{-1} \\\\ &amp; \\times\\left[\\left(X^{\\prime} Q_{1} y\\right) / \\sigma_{v}^{2}+\\left(X^{\\prime} Q_{2} y\\right) / \\lambda_{2}+\\left(X^{\\prime} Q_{3} y\\right) / \\lambda_{3}\\right] \\\\ =&amp;\\left[W_{X X}+\\phi_{2}^{2} B_{X X}+\\phi_{3}^{2} C_{X X}\\right]^{-1}\\left[W_{X y}+\\phi_{2}^{2} B_{X y}+\\phi_{3}^{2} C_{X y}\\right] \\end{aligned} \\end{equation}\\] (3.23) with \\(\\operatorname{var}\\left(\\widehat{\\beta}_{\\mathrm{GLS}}\\right)=\\sigma_{v}^{2}\\left[W_{X X}+\\phi_{2}^{2} B_{X X}+\\phi_{3}^{2} C_{X X}\\right]^{-1}\\). Note that \\(W_{X X}=X^{\\prime} Q_{1} X, B_{X X}=X^{\\prime} Q_{2} X\\) and \\(C_{X X}=X^{\\prime} Q_{3} X\\) with \\(\\phi_{2}^{2}=\\sigma_{v}^{2} / \\lambda_{2}, \\phi_{3}^{2}=\\sigma_{v}^{2} / \\lambda_{3} .\\) Also, the Within estimator of \\(\\beta\\) is \\(\\widetilde{\\beta}_{W}=\\) \\(W_{X X}^{-1} W_{X y}\\), the Between individuals estimator of \\(\\beta\\) is \\(\\widehat{\\beta}_{B}=B_{X X}^{-1} B_{X y}\\) and the Between timeperiods estimator of \\(\\beta\\) is \\(\\widehat{\\beta}_{C}=C_{X X}^{-1} C_{X y}\\). This shows that \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) is a matrix-weighted average of \\(\\widetilde{\\beta}_{W}, \\widehat{\\beta}_{B}\\) and \\(\\widehat{\\beta}_{C}\\). In fact, \\[\\begin{equation} \\widehat{\\beta}_{\\mathrm{GLS}}=W_{1} \\widetilde{\\beta}_{W}+W_{2} \\widehat{\\beta}_{B}+W_{3} \\widehat{\\beta}_{C} \\end{equation}\\] (3.24) where \\[\\begin{equation} \\begin{aligned} &amp;W_{1}=\\left[W_{X X}+\\phi_{2}^{2} B_{X X}+\\phi_{3}^{2} C_{X X}\\right]^{-1} W_{X X} \\\\ &amp;W_{2}=\\left[W_{X X}+\\phi_{2}^{2} B_{X X}+\\phi_{3}^{2} C_{X X}\\right]^{-1}\\left(\\phi_{2}^{2} B_{X X}\\right) \\\\ &amp;W_{3}=\\left[W_{X X}+\\phi_{2}^{2} B_{X X}+\\phi_{3}^{2} C_{X X}\\right]^{-1}\\left(\\phi_{3}^{2} C_{X X}\\right) \\end{aligned} \\end{equation}\\] This was demonstrated by Maddala (1971). Note that (i) if \\(\\sigma_{\\mu}^{2}=\\sigma_{\\lambda}^{2}=0, \\phi_{2}^{2}=\\phi_{3}^{2}=1\\) and \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) reduces to \\(\\widehat{\\beta}_{\\mathrm{OLs}}\\); (ii) as \\(T\\) and \\(N \\rightarrow \\infty, \\phi_{2}^{2}\\) and \\(\\phi_{3}^{2} \\rightarrow 0\\) and \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) tends to \\(\\widetilde{\\beta}_{W}\\); (iii) if \\(\\phi_{2}^{2} \\rightarrow \\infty\\) with \\(\\phi_{3}^{2}\\) finite, then \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) tends to \\(\\widehat{\\beta}_{B}\\); (iv) if \\(\\phi_{3}^{2} \\rightarrow \\infty\\) with \\(\\phi_{2}^{2}\\) finite, then \\(\\widehat{\\beta}_{\\text {GLS }}\\) tends to \\(\\widehat{\\beta}_{C}\\). Wallace and Hussain (1969) compare \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) and \\(\\widetilde{\\beta}_{\\text {Within }}\\) in the case of nonstochastic (repetitive) \\(X\\) and find that both are (i) asymptotically normal, (ii) consistent and unbiased and that \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) has a smaller generalized variance (i.e. more efficient) in finite samples. In the case of nonstochastic (nonrepetitive) \\(X\\) they find that both \\(\\widehat{\\beta}_{\\mathrm{GLS}}\\) and \\(\\widetilde{\\beta}_{\\text {Within }}\\) are consistent, asymptotically unbiased and have equivalent asymptotic variance-covariance matrices, as both \\(N\\) and \\(T \\rightarrow \\infty\\). The last statement can be proved as follows: the limiting variance of the GLS estimator is \\[\\begin{equation} \\frac{1}{N T} \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty}\\left(X^{\\prime} \\Omega^{-1} X / N T\\right)^{-1}=\\frac{1}{N T} \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty}\\left[\\sum_{i=1}^{3} \\frac{1}{\\lambda_{i}}\\left(X^{\\prime} Q_{i} X / N T\\right)\\right]^{-1} \\end{equation}\\] (3.25) but the limit of the inverse is the inverse of the limit, and \\[\\begin{equation} \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty} \\frac{X^{\\prime} Q_{i} X}{N T} \\quad \\text { for } i=1,2,3 \\end{equation}\\] (3.26) all exist and are positive semidefinite, since \\[\\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty}\\left(X^{\\prime} X / N T\\right)\\] is assumed finite and positive definite. Hence \\[ \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty} \\frac{1}{\\left(N \\sigma_{\\lambda}^{2}+\\sigma_{v}^{2}\\right)}\\left(\\frac{X^{\\prime} Q_{3} X}{N T}\\right)=0 \\] and \\[ \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty} \\frac{1}{\\left(T \\sigma_{\\mu}^{2}+\\sigma_{v}^{2}\\right)}\\left(\\frac{X^{\\prime} Q_{2} X}{N T}\\right)=0 \\] Therefore the limiting variance of the GLS estimator becomes \\[ \\frac{1}{N T} \\lim _{N \\rightarrow \\infty \\atop T \\rightarrow \\infty} \\sigma_{v}^{2}\\left(\\frac{X^{\\prime} Q_{1} X}{N T}\\right)^{-1} \\] which is the limiting variance of the Within estimator. One can extend Nerloves (1971a) method for the one-way model, by estimating \\(\\sigma_{\\mu}^{2}\\) as \\(\\sum_{i=1}^{N}\\left(\\widehat{\\mu}_{i}-\\widehat{\\mu}\\right)^{2} /(N-1)\\) and \\(\\sigma_{\\lambda}^{2}\\) as \\(\\sum_{t=1}^{T}\\left(\\widehat{\\lambda}_{t}-\\bar{\\lambda}\\right)^{2} /(T-1)\\) where the \\(\\widehat{\\mu}_{i}\\) and \\(\\widehat{\\lambda}_{t}\\) are obtained as coefficients from the least squares dummy variables regression (LSDV). \\(\\sigma_{v}^{2}\\) is estimated from the Within residual sums of squares divided by \\(N T\\). Baltagi (1995, appendix 3) develops two other methods of estimating the variance components. The first is Raos (1970) minimum norm quadratic unbiased estimation (MINQUE) and the second is Hendersons method III as described by Fuller and Battese (1973). These methods require more notation and development and may be skipped in a brief course on this subject. Chapter 9 studies these estimation methods in the context of an unbalanced error component model. Baltagi (1981a) performed a Monte Carlo study on a simple regression equation with twoway error component disturbances and studied the properties of the following estimators: OLS, the Within estimator and six feasible GLS estimators denoted by WALHUS, AMEMIYA, SWAR, MINQUE, FUBA and NERLOVE corresponding to the methods developed by Wallace and Hussain (1969), Amemiya (1971), Swamy and Arora (1972), Rao (1972), Fuller and Battese (1974) and Nerlove (1971a), respectively. The mean square error of these estimators was computed relative to that of true GLS, i.e. GLS knowing the true variance components. To review some of the properties of these estimators: OLS is unbiased, but asymptotically inefficient, and its standard errors are biased; see Moulton (1986) for the extent of this bias in empirical applications. In contrast, the Within estimator is unbiased whether or not prior information about the variance components is available. It is also asymptotically equivalent to the GLS estimator in case of weakly nonstochastic exogenous variables. Early in the literature, Wallace and Hussain (1969) recommended the Within estimator for the practical researcher, based on theoretical considerations but more importantly for its ease of computation. In Wallace and Hussains (1969, p. 66) words the covariance estimators come off with a surprisingly clear bill of health. True GLS is BLUE, but the variance components are usually not known and have to be estimated. All of the feasible GLS estimators considered are asymptotically efficient. In fact, Prucha (1984) showed that as long as the estimate of \\(\\sigma_{v}^{2}\\) is consistent, and the probability limits of the estimates \\(\\sigma_{\\mu}^{2}\\) and \\(\\sigma_{\\lambda}^{2}\\) are finite, the corresponding feasible GLS estimator is asymptotically efficient. Also, Swamy and Arora (1972) proved the existence of a family of asymptotically efficient two-stage feasible GLS estimators of the regression coefficients. Therefore, based on asymptotics only, one cannot differentiate among these twostage GLS estimators. This leaves undecided the question of which estimator is the best to use. Some analytical results were obtained by Swamy (1971) and Swamy and Arora (1972). These studies derived the relative efficiencies of (i) SWAR with respect to OLS, (ii) SWAR with respect to Within and (iii) Within with respect to OLS. Then, for various values of \\(N, T\\), the variance components, the Between groups, Between time-periods and Within groups sums of squares of the independent variable, they tabulated these relative efficiency values (see Swamy, 1971 , chapters II and III; Swamy and Arora, 1972, p. 272). Among their basic findings is the fact that, for small samples, SWAR is less efficient than OLS if \\(\\sigma_{\\mu}^{2}\\) and \\(\\sigma_{\\lambda}^{2}\\) are small. Also, SWAR is less efficient than Within if \\(\\sigma_{\\mu}^{2}\\) and \\(\\sigma_{\\lambda}^{2}\\) are large. The latter result is disconcerting, since Within, which uses only a part of the available data, is more efficient than SWAR, a feasible GLS estimator, which uses all of the available data. 3.4 REFERENCES "],["test-of-hypotheses-with-panel-data.html", "Chapter 4 Test of Hypotheses with Panel Data 4.1 TESTS FOR POOLABILITY OF THE DATA 4.2 TESTS FOR INDIVIDUAL AND TIME EFFECTS 4.3 HAUSMANS SPECIFICATION TEST", " Chapter 4 Test of Hypotheses with Panel Data 4.1 TESTS FOR POOLABILITY OF THE DATA The question of whether to pool the data or not naturally arises with panel data. The restricted model is the pooled model given by (2.3) representing a behavioral equation with the same parameters over time and across regions. The unrestricted model, however, is the same behavioral equation but with different parameters across time or across regions. For example, Balestra and Nerlove (1966) considered a dynamic demand equation for natural gas across 36 states over six years. In this case, the question of whether to pool or not to pool boils down to the question of whether the parameters of this demand equation vary from one year to the other over the six years of available data. One can have a behavioral equation whose parameters may vary across regions. For example, Baltagi and Griffin (1983) considered panel data on motor gasoline demand for 18 OECD countries. In this case, one is interested in testing whether the behavioral relationship predicting demand is the same across the 18 OECD countries, i.e. the parameters of the prediction equation do not vary from one country to the other. These are but two examples of many economic applications where time-series and crosssection data may be pooled. Generally, most economic applications tend to be of the first type, i.e. with a large number of observations on individuals, firms, economic sectors, regions, industries and countries but only over a few time periods. In what follows, we study the tests for the poolability of the data for the case of pooling across regions keeping in mind that the other case of pooling over time can be obtained in a similar fashion. For the unrestricted model, we have a regression equation for each region given by \\[\\begin{equation} y_i=Z_i \\delta _i + \\mu_i \\text{i=1,2,...,N} \\end{equation}\\] (4.1) where \\[ y^{\\prime}=(y_i1,...,y_{iT}) \\], \\(Z_i=[i_T,X_i]\\) ans \\(X_i\\) is $T K $ . The important thing to notice is that \\(\\delta_{1}\\) is different for every regional equation. We want to test the hypothesis \\(H_{0}: \\delta_{l}=\\delta\\) for all \\(i\\), so that undcr \\(H_{0}\\) we can write the restricted model given in \\((4.1)\\) as \\[\\begin{equation} y= Z \\delta + \\mu \\end{equation}\\] where \\[ Z^{\\prime}=\\left(Z_{1}^{\\prime} , Z_{2}^{\\prime}, \\ldots, Z_{N}^{\\prime}\\right) \\] and \\[ u^{\\prime}=\\left(u_{1}^{\\prime}, u_{2}^{\\prime}, \\ldots, u_{N}^{\\prime}\\right) \\] . The unrestricted model can also be written as \\[\\begin{equation} y=\\left(\\begin{array}{cccc} Z_{1} &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; Z_{2} &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; Z_{N} \\end{array}\\right)\\left(\\begin{array}{c} \\delta_{1} \\\\ \\delta_{2} \\\\ \\vdots \\\\ \\delta_{N} \\end{array}\\right)+u=Z^{*} \\delta^{*}+u \\end{equation}\\] where \\[\\delta^{* \\prime}=\\left(\\delta_{1}^{\\prime}, \\delta_{2}^{\\prime}, \\ldots, \\delta_{N}^{\\prime}\\right)\\] and \\[ Z=Z^{*} I^{*} \\] with \\[ I^{*}=\\left(\\iota_{N} \\otimes I_{K^{\\prime}}\\right)\\], an \\[ N K^{\\prime} \\times K^{\\prime}\\] matrix, with \\[K^{\\prime}=K+1 \\] . Hence the variables in Z are all linear combinations of the variables in \\[ Z^{*}\\] . 4.1.1 Test for Poolability under \\[ u \\sim N\\left(0, \\sigma^{2} I_{N T}\\right)\\] Assumption 4. 1 \\[\\mu \\sim N\\left(0, \\sigma^{2} I_{N T}\\right)\\] Under assumption 4.1, the minimum variance unbiased estimator for \\(\\delta\\) in equation (4.2) is \\[\\begin{equation} \\widehat{\\delta}_{\\mathrm{OLS}}=\\widehat{\\delta}_{m l e}=\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} y \\end{equation}\\] and therefore \\[\\begin{equation} y=Z \\widehat{\\delta}_{\\mathrm{OLS}}+e \\end{equation}\\] implying that \\[e=\\left(I_{NT}-Z \\left(Z Z^{\\prime}\\right)^{-1}Z^{\\prime}\\right)y =My=M\\left(Z\\delta +\\mu \\right)= M \\mu \\] since \\(MZ=0\\). Similarly, under assumption 4.1, the MVU for \\(\\delta_i\\) is given by \\[\\begin{equation} \\widehat{\\delta}_{i, \\mathrm{OLS}}=\\widehat{\\delta}_{i, m l e}=\\left(Z_{i}^{\\prime} Z_{i}\\right)^{-1} Z_{i}^{\\prime} y_{i} \\end{equation}\\] therefore \\[\\begin{equation} y_{i}=Z_{i} \\widehat{\\delta}_{i, \\mathrm{OLS}}+e_{i} \\end{equation}\\] implying that \\[ e_{i}=\\left(I_{T}-Z_{i}\\left(Z_{i}^{\\prime} Z_{i}\\right)^{-1} Z_{i}^{\\prime}\\right) y_{i}=M_{i} y_{i}=M_{i}\\left(Z_{i} \\delta_{i}+u_{i}\\right)=M_{i} u_{i}\\] since $M_{i} Z_{i}=0 $ and this is true for \\(i=1,2, \\ldots, N\\) . Also, let \\[ M^{*}=I_{N T}-Z^{*}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z^{* \\prime}=\\left(\\begin{array}{cccc} M_{1} &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; M_{2} &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; M_{N} \\end{array}\\right) \\] One can easily deduce that \\(y=Z^{*} \\widehat{\\delta}^{*}+e^{*}\\) with \\(e^{*}=M^{*} y=M^{*} u\\) and \\(\\widehat{\\delta}^{*}=\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z^{* \\prime} y\\). Note that both \\(M\\) and \\(M^{*}\\) are symmetric and idempotent with \\(M M^{*}=M^{*}\\). This easily follows since \\[\\begin{equation} \\begin{aligned} Z\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} Z^{*}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z^{* \\prime} &amp;=Z\\left(Z^{\\prime} Z\\right)^{-1} I^{* \\prime} Z^{* \\prime} Z^{*}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z^{* \\prime} \\\\ &amp;=Z\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} \\end{aligned} \\end{equation}\\] (Non Numéroté) This uses the fact that \\(Z=Z^{*} I^{*} .\\) Under assumption 4.1, \\(e^{\\prime} e-e^{* \\prime} e^{*}=u^{\\prime}\\left(M-M^{*}\\right) u\\) and \\(e^{* \\prime} e^{*}=u^{\\prime} M^{*} u\\) are independent since \\(\\left(M-M^{*}\\right) M^{*}=0 .\\) Also, both quadratic forms when divided by \\(\\sigma^{2}\\) are distributed as \\(\\chi^{2}\\) since \\(\\left(M-M^{*}\\right)\\) and \\(M^{*}\\) are idempotent. Dividing these quadratic forms by their respective degrees of freedom and taking their ratio leads to the following test statistic: \\({ }^{1}\\) \\[F_{obs}=\\frac{\\left(e^{\\prime}e-e^{* \\prime} \\right) / \\left( \\operatorname{tr}\\left(M\\right)-\\operatorname{tr}\\left(M^*\\right) \\right)} {e^{* \\prime}e^* / \\operatorname{tr}\\left(M\\right) } \\] \\[\\begin{equation} F_{obs}=\\frac{\\left(e_1^{\\prime}e-e_2^{\\prime}e_1 -e^{\\prime}e_2 -\\ldots -e_N^{\\prime}e \\right)/ \\left(N-1 \\right) K^{\\prime} } {\\left(e_1^{\\prime}e-e_2^{\\prime}e_1 -e^{\\prime}e_2 -\\ldots -e_N^{\\prime}e \\right)/ N \\left(T-K^{\\prime} \\right) } \\end{equation}\\] Under \\(H_0,F_{obs}\\) is distributed as an \\[F \\left(\\left(N-1 \\right)K^{\\prime}, N\\left(T-K^{\\prime}\\right)\\right) \\] Hence the critical region for this test is defined as \\[\\brace F_{obs}&gt; \\left(\\left(N-1 \\right)K^{\\prime}, NT-NK^{\\prime} ,\\alpha_0 \\right) \\] where \\(\\alpha_0\\) denotes the level of significance of the test. This is exactly the Chow test presented by Chow (1960) extended to the case of \\(N\\) linear regressions. Therefore if an economist has reason to believe that assumption 4.1 is true, and wants to pool his data across regions, then it is recommended that he or she test for the poolability of the data using the Chow test given in (4.8). However, for the variance component model \\(\\mu \\sim \\left(0,\\Omega \\right)\\) and not \\(\\left(0,\\sigma^2 I_{NT} \\right)\\) Therefore, even if we assume normality on the disturbances two questions remain: (1) is the Chow test still the right test to perform when \\(\\mu \\sim N\\left(0,\\Omega \\right)\\) ? and (2) does the Chow statistic still have an F-distribution when \\(\\mu \\sim N\\left(0,\\Omega \\right)\\) ? The answer to the first question is no, the Chow test given in (4.8) is not the right test to perform. However, as will be shown later, a generalized Chow test will be the right test to perform. As for the second question, it is still relevant to ask because it highlights the problem of economists using the Chow test assuming erroneously that \\(\\mu\\) is \\(N\\left(0,\\sigma^2 I_{NT} \\right)\\) when in fact it is not. For example, Toyoda (1974), in treating the case where the \\(\\mu_i\\) are heteroskedastic, found that the Chow statistic given by (4.8) has an approximate \\(F-\\text{Distribution}\\) where the degree of freedom of the denominator depends upon the true variances. Hence for specific values of these variances, Toyoda demonstrates how wrong it is to apply the Chow test in case of heteroskedastic variances. Having posed the two questions above, we can proceed along two lines: the first is to find the approximate distribution of the Chow statistic (4.8) in case \\(\\mu \\sim N \\left(0,\\Omega \\right)\\) and therefore show how erroneous it is to use the Chow test in this case (this is not pursued in this book). The second route, and the more fruitful, is to derive the right test to perform for pooling the data in case \\(\\mu \\sim N \\left(0,\\Omega \\right)\\) This is done in the next subsection. 4.1.2 Test for Poolability under the General Assumption \\[ u \\sim N\\left(0, \\Omega\\right)\\] Assumption 4.2 \\[ u \\sim N\\left(0, \\Omega\\right)\\] In case \\(\\Omega\\) is known up to a scalar factor, the test statistic employed for the poolability of the data would be simple to derive. All we need to do is transform our model (under both the null and alternative hypotheses) such that the transformed disturbances have a variance of \\(\\sigma^2 I_{NT}\\) , then apply the Chow test on the transformed model. The later step is legitimate because the transformed disturbances have homoskedastic variances and the analysis of the previous subsection applies in full. Given $=^2 $ , we premultiply the restricted model given in (4.2) by \\(\\sum^{-1/2}\\) and we call \\(\\sum^{-1/2}y=\\dot{y}\\) , \\(\\sum^{-1/2}Z=\\dot{Z}\\) and \\(\\sum^{-1/2} \\mu=\\dot{\\mu}\\) Hence \\[\\begin{equation} \\dot{y}=\\dot{Z}\\delta + \\dot{u} \\end{equation}\\] with \\[E\\left(\\dot{u}\\dot{u^{\\prime}}\\right) =\\sum^{-1/2} E\\left(u,u^{\\prime}\\right) \\sum^{-1/2 \\prime} =\\sigma^2 I_{NT} \\]. Similarly, we premultiply the unrestricted model given in (4.3) by \\(\\sum^{-1/2}\\) and we call \\(\\sum^{-1/2} Z^*=Z^*\\). Therefore \\[\\begin{equation} \\dot{y}=\\dot{Z}^{*} \\delta ^{*} + \\dot{u} \\end{equation}\\] with \\(E\\left(\\dot{u}\\dot{u}^{\\prime}\\right)=\\sigma^2 I_{NT}\\) . At this stage, we can test \\(H_0:\\delta_i =\\delta\\) for every \\(i=1,2,\\ldots, N\\) , simply by using the Chow statistic, only nowon the transformed models (4.9) and (4.10) since they satisfy assumption 4.1 of homoskedasticity of the normal disturbances. Note that \\[\\dot{Z}=\\dot{Z^*}I^* \\] , which is simply obtained from $Z=Z* I* $ by premultiplying by \\(\\sum^{-1/2}\\). Defining \\[\\dot{M}=I_{NT}-\\dot{Z}\\left(\\dot{Z^{\\prime}}\\dot{Z} \\right)^{-1} \\dot{Z^{\\prime}} \\] and \\[\\dot{M^*}=I_{NT}-\\dot{Z^*}\\left(\\dot{Z^{* \\prime}}\\dot{Z^*} \\right)^{-1} \\dot{Z^{* \\prime}} \\] it is easy to show that \\(\\dot{M}\\) and \\(\\dot{M^*}\\) are both symmetric, idempotent and such that \\[\\dot{M} \\dot{M^*}=\\dot{M^*} \\] Once again the conditions for lemma 2.2 of Fisher (1970) are satisfied, and the test statistic \\[\\begin{equation} \\dot{F}_{obs}=\\frac{\\left(\\dot{e}^{\\prime} \\dot{e}-\\dot{e}^{* \\prime} \\dot{e}^* \\right) / \\left(\\operatorname{tr}\\left(\\dot{M}\\right)-\\operatorname{tr}\\left(\\dot{M^*} \\right) \\right) } {\\dot{e}^{* \\prime} \\dot{e}^* / \\operatorname{tr}\\left( \\dot{M^*} \\right) } \\sim F \\left(\\left( N-1 \\right)K^{\\prime} , N \\left( T- K^{\\prime} \\right) \\right) \\end{equation}\\] \\[ \\begin{aligned} \\text{where } \\dot{e}=\\dot{y}-\\dot{Z} \\widehat{\\dot{\\delta}}_{OLS} \\text{ and } \\widehat{\\dot{\\delta}}_{OLS} =\\left(\\dot{Z^{\\prime}\\dot{Z}} \\right)^{-1} \\dot{Z^{\\prime}} \\dot{y} \\text{ implying that } \\dot{e}=\\dot{M}\\dot{y}=\\dot{M}\\dot{u} \\text{. Similarly,} \\dot{e^*}= \\\\ \\dot{y}-\\dot{Z^*}\\widehat{\\dot{\\delta}_{OLS}^*} \\text{ and } \\widehat{\\dot{\\delta}_{OLS}^*}=\\left(\\dot{Z^{* \\prime}} \\dot{Z} \\right)^{-1} \\dot{Z^{* \\prime}} \\dot{y} \\text{ implying that } \\dot{e^*}=\\dot{M^*}\\dot{y}= \\dot{M^*} \\dot{u} \\text{. Using the fact that } \\\\ \\dot{M} \\text{ and } \\dot{M^*} \\text{ are symmetric and idempotent, we can rewrite (4.11) as } \\end{aligned} \\] \\[ \\dot{F}_{obs}= \\frac{\\left(\\dot{y}^{\\prime}\\dot{M}\\dot{y}-\\dot{y}^{\\prime}\\dot{M^*} \\dot{y} \\right) / \\left(N-1\\right) K^{\\prime} } {\\dot{y}^{\\prime}\\dot{M}\\dot{y} / N \\left( T-K^{\\prime} \\right) }\\] \\[\\begin{equation} \\dot{F}_{obs}=\\frac{ \\left(y^{\\prime} \\sum^{-1/2}\\dot{M} \\sum^{-1/2}y -\\sum^{-1/2}\\dot{M^*} \\sum^{-1/2}y \\right) / \\left(N-1\\right) K^{\\prime} } { \\sum^{-1/2}\\dot{M^*} \\sum^{-1/2}y / N \\left(T-K^{\\prime} \\right) } \\end{equation}\\] But \\[ \\dot{M}=I_{NT}-\\sum^{-1/2} Z\\left(Z^{\\prime } \\sum^{-1} Z \\right)^{-1} Z^{\\prime} \\sum^{-1/2^{\\prime}} \\] And \\[ \\dot{M^*}=I_{NT}-\\sum^{-1/2} Z^{*}\\left(Z^{*} \\sum^{-1} Z \\right)^{-1} Z^{* \\prime} \\sum^{-1/2^{\\prime}} \\] so that \\[\\sum^{-1/2}\\dot{M}\\sum_{-1/2} =\\sum^{-1}-\\sum^{-1} Z \\left(Z^{\\prime}\\sum^{-1}Z \\right)^{-1} Z^{\\prime} \\] and \\[\\sum^{-1/2}\\dot{M^*}\\sum_{-1/2} =\\sum^{-1}-\\sum^{-1} Z^* \\left(Z^{*\\prime}\\sum^{-1}Z^* \\right)^{-1} Z^{*\\prime} \\sum^{-1} \\] Hence we can write (4.12) in the form 4.2 TESTS FOR INDIVIDUAL AND TIME EFFECTS 4.3 HAUSMANS SPECIFICATION TEST "],["methods.html", "Chapter 5 Methods", " Chapter 5 Methods We describe our methods in this chapter. Les données de panel, ou données longitudinales possèdent les deux dimensions précédentes (individuelle et temporelle). En effet, il est souvent intéressant didentifier leffet associé à chaque individu (un effet qui ne varie pas dans le temps, mais qui varie dun individu à un autre). Cet effet peut être fixe ou aléatoire. Par conséquent, le modèle en données de panel sécrit comme un modèle à double indice qui prend la forme suivante : \\[ Y_{it}= \\alpha_i\\sum_{k}\\beta_{ki}x_{ki}+ \\epsilon_{it} \\] avec \\[ i:1 \\rightarrow N \\] et \\[ t:1 \\rightarrow T \\] La double dimension quoffrent les données de panel est un atout majeur. En effet, si les données en séries temporelles permettent détudier lévolution des relations dans le temps, elles ne permettent pas de contrôler lhétérogénéité entre les individus. A linverse, les données en coupes transversales permettent danalyser lhétérogénéité entre les individus mais elles ne peuvent pas tenir compte des comportements dynamiques, puisque la dimension temporelle est exclue du champ danalyse. Ainsi, en utilisant des données de panel, on pourra exploiter les deux sources de variation de linformation statistique : - Temporelle où variabilité intra-individuelle (within) - et individuelle ou variabilité inter-individuelle (Between). "],["analyses.html", "Chapter 6 Analyses 6.1 Netoyage de la base des données 6.2 Agregation des données avec la méthode reduce 6.3 Analyse descriptive des Varariales", " Chapter 6 Analyses Nous faisons application des méthodes présentées dans le chapitre précédant pour lanalyse des données de pannel Avant de passefr à la modélisation, nous ferons une description de nos variables dinteret dune manière statique : nos prédicteurs et la variables réponses 6.1 Netoyage de la base des données Apperçue globale des données Table 6.1: Echantillon de la base des données N° Country/destination Year Goods Weight Taxe 1 AFRIQUE DU SUD 2011 GRUES SUR PNEUMATIQUE 13500 0 2 AFRIQUE DU SUD 2011 CAMION FAMIL 12000 0 3 AFRIQUE DU SUD 2011 CAMION SOMUL 24000 0 4 AFRIQUE DU SUD 2013 Café vert arabica k4 183 0 5 AFRIQUE DU SUD 2013 Café vert arabica k4 19520 264771 6 AFRIQUE DU SUD 2013 Café vert arabica k4 19520 272817 7 AFRIQUE DU SUD 2013 Café vert arabica k4 19520 283220 8 AFRIQUE DU SUD 2013 Café vert arabica k4 19520 264142 9 AFRIQUE DU SUD 2013 CAMION 24000 0 10 AFRIQUE DU SUD 2017 Instruments et appareils du n°90.15 654 0 Voici la structure de la base des données ## Rows: 3,310 ## Columns: 6 ## $ `N°` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1~ ## $ `Country/destination` &lt;chr&gt; &quot;AFRIQUE DU SUD&quot;, &quot;AFRIQUE DU SUD&quot;, &quot;AFRIQUE DU ~ ## $ Year &lt;dbl&gt; 2011, 2011, 2011, 2013, 2013, 2013, 2013, 2013, ~ ## $ Goods &lt;fct&gt; &quot;GRUES SUR PNEUMATIQUE&quot;, &quot;CAMION FAMIL&quot;, &quot;CAMION~ ## $ Weight &lt;dbl&gt; 13500, 12000, 24000, 183, 19520, 19520, 19520, 1~ ## $ Taxe &lt;dbl&gt; 0, 0, 0, 0, 264771, 272817, 283220, 264142, 0, 0~ Voici les modalités de la variabme Goods qui signifie Marchandises Table 6.2: Modalités de la variable Goods à limportation des donnees x 0 3Café vert arabica, en feve K3 Abats comestibles,congeles,de chevaux,anes,mulets,ovins ou caprins ABATS COMESTIBLES;CONGELES;DE CHEVAUX;ANES;MULETS;OVINS Accessoires de radio diffusion Accessoires de vehicules Accumulateurs electriques Acide acetique ages de 5 ans ou moins ages de plus de 5 ans Agés de plus de 5 ans ou moins Alcaloides du quinquina et leurs derives; ALCOOL ETYLIQUE NON DENATURE ambulance dune cylindree excedant 2500 cm3 Antennes Antennes et reflecteurs dantennes antennes et refleteurs Appareils declairage electriques Appareils declairage non electriques Appareils declairages electriques Appareils du n°84.14 Appareils electrothermiques pour la appareils pour la reception,la conversion et la transmission Art et materiel dathletisme Articles confectionnes en textiles Articles deconomie domestique,en Articles de bureau ARTICLES DE BUREAU Articles de bureau ou de la papeterie Articles de friperie ARTICLES DE FRIPERIE Articles et materiel dathletisme Ashok Layland ASPIRATEUR ET ACCESSOIRES Autes bois sciés AUTRE MACHINE ET APPAREIL A IMPRIMER AUTRE MINERAIS DE TITANE (Coltant) AUTRE PARTIE DE PLANTE AUTRE PEAUX AUTRE PREP ALIMENTAIRE Autre vehicules automobiles a usages speciaux Autres AUTRES Autres bois scies Autres abats comestibles frais ou refrigérés de chevaux,anes,mulets,ovins,caprins Autres abats comestibles,congeles,de chevaux,anes,mulets,ovins ou caprins Autres accessoires de tuyauterie en fonte Autres accumulateurs electriques Autres appareils elevateurs, a action continue pour marchandises Autres armes Autres art de bureau ou de papeterie en papier Autres Art de menage Autres articles deconomie domestique, en autres articles de bureau AUTRES ARTICLES DE BUREAU OU D PAPETERIE EN PAPIER OU CARTON Autres articles de bureau ou de papeterie en Autres articles de campement Autres articles de friperie Autres articles de menage ou déconomie en aluminium Autres articles de transport ou demballage Autres articles en toles emailles Autres babeurre,lait et creme Autres babeurre,lait et creme caillés,yoghourt autres baies fraiches Autres bois Autres bois plaques ou stratifies AUTRES BOIS ROPICAUX SCIES LONG Autres bois sciées Autres bois scies AUTRES BOIS SCIES autres bois sciés Autres bois sciés Autres bois tropicaux, a scies long, sciages d1 avives d1 epaisseur superieure a 50mm Autres bois tropicaux, scies lon, sciages avives d1 epaisseur superieure a 50mm Autres bois tropicaux,scies long AUTRES BOIS TROPICAUX,SCIES LONG-SCIAGES AVIVESD1 EPAISSEEUR SUPERIEURE A 50MM Autres bois tropicaux,scies long,sciages avivées Autres bois tropicaux,scies long,sciages avives Autres bois tropicaux,sciés longsciages avives Autres bois tropicauxmscies long Autres Caoutchouc Autres cassiterites Autres chariots y.c. chariots tracteurs,sans Autres chaussures Autres coltan Autres coltans Autres constructions Autres couvertures non chauffantes Autres dechets et debris Autres dechets et debris daciers alliés Autres déchets et débris daciers alliés Autres dechets et debris d aciers autres eaux, y compris leau douce Autres engins du n84 29 AUTRES EQUIPEMENTS DE PROTECTION Autres et debris de bois Autres etiquettes de tous genres, en papier Autres fours industriels ou de laboratoires Autres fours, cuisinieres, rechauds, grils Autres fours,cuisinières,rechauds AUTRES GROUPES ELECT Autres groupes electrogenes Autres haut-parleurs Autres huiles de pal;iste ou babassu Autres instrument et appareils du n°90,18 Autres instruments de musique a vent Autres instruments et appareils Autres instruments et appareils du n 90.18 Autres jus de tout autres fruit ou legume Autres legumes ¿ cosse secs, ¿coss¿s, m¿me Autres legumes, frais ou refrigeres Autres lunettes autres machines Autres machines Autres machines electriques a laver la Autres machines et appareils Autres machines et appareils de manutention du n,8428 Autres machines et appareils du n 8515 Autres medicaments AUTRES MEDICAMENTS Autres mélanges dépices Autres meubles et leurs parties AUTRES MINERAIS DE TANTALE AUTRES MINERALS DE TANTALE (Coltan) Autres montres (y.c. compteur de temps) Autres moteurs diésel Autres moteurs diésels Autres nattes en matières vegetales autres outils Autres outils agricoles Autres ouvrages en plomb Autres papiers non denommes ni compris ailleurs AUTRES PAPIERS NON DENOMMES NI COMPRIS AILLEURS Autres papiers non denommés ni compris ailleurs Autres parties davions ou dhelicopteres Autres parties de lampes portatives AUTRES PARTIES DE PLANTES EN MEDECINE AUTRES PARTIES DE PLANTES UTILISEES PRINCIPALEMENT EN MEDECINE Autres parties de plantes utilisés en medecine Autres parties des appareils des n 88.01 ou Autres parties des plantes utilisées principalement en medecine Autres parties et accesoires des vehicules Autres parties et accessoires de AUTRES PARTIES ET ACCESSOIRES DE VEHICULES DES n87.01 a 87.05 AUTRES PARTIES ET ACCESSOIRES DES VEHICULES autres parties et accessoires des vehicules des 8701 a 8705 Autres parties et accessoires des vehicules des 8701 a 8705 AUTRES PARTIES ET ACCESSOIRES DES VEHICULES DES N 87.01 A 87.05 Autres parties et accessoires des vehicules des n 8701 à 8705 Autres parties et accessoires des vehicules n 8701 à 8705 AUTRES PEAUX AUTRES PEAUX DES BOVINS Autres pierres autres piles et batteries Autres plantes Autres plantes, parties de plantes, AUTRES PLAQUES Autres plaques, feuilles,bandes en polymeres Autres pneumat autres qua crpn,a chvn en autr ctc synt Autres pneumatiques neuf en autres mat que les mat synth de type pour voiture tourisme AUTRES POMPES A DISPOSITIF MESUREUR OU CONCUES POUR EN COMPORTER UN Autres préparations alimentaires Autres preparations alimentaires n d c a Autres preparations homogenisees AUTRES PRODUITS DE BEAUTE Autres produits de beaute,de Autres produits du no 33.07,contenant de Autres recepteurs fixes de radiodiffusion AUTRES RECERVOIRS Autres refrigerateurs menagers Autres sacs,sachets,pochettes Autres salmonides entiers, Autres savons AUTRES TENTES Autres tissus de coton imprimé,200 g/m2 et Autres tracteurs Autres unites de machines automatiques de Autres vehicules autom a usages speciaux Autres vehicules automobiles a usages Autres vehicules automobiles a usages speciaux Autres vetements de dessous en coton, pour Autres vetements en bonneterie dautres Autres, en couleurs Babeurre,lait et creme cailles,yoghourt ou avec fruits ou cacao BACHES Baches et stores dexterieur de fibres Baches et stores dexterieur de fibres synth Baies fraiches bières de malt titrant moins de 6° BISCUIT Biscuits additionnes dedulcorants BITUMES Bois ciés BOIS RABOTES OU PONCES Bois scies BOIS SCIES Bois sciés Bois scies longitudinalement Bois topicaux, scie long..sciages avives d1 epaisseur supérieur à 50mm Bois tropicaux Bois tropicaux scies Bois tropicaux scies long,sciages avives dun epaisseur superieur à 50 mm Bois tropicaux,scies Bois tropicaux,scies long Bois tropicaux,scies long,sciages avives Bois tropicaux,scies long,sciages vives dun epaisseur supérieure à 50 mm Boites et cartonnages,pliants,en papier ou carton non ondulé Boites et caisses en papier ou carton ondule Boites et caisses en papier ou carton ondulé Boites et cartonnaages,pliants en papier ou carton non ondule Boites et cartonnages pliants en papier ou carton non ondule Boites et cartonnages, en papier ou carton non ondule Boites et cartonnages, pliants, en papier ou Boites et cartonnages, pliants, en papier ou caton non ondule Boites et cartonnages,pliants Boites et cartonnages,pliants , en papier ou carton Boites et cartonnages,pliants en papier ou carton non ondule BOITES ET CARTONNAGES,PLIANTS EN PAPIER OU CARTON NON ONDULE Boites et cartonnages,pliants,en papier Boites et cartonnages,pliants,en papier ou carton non ondule Boites et cartonnages,pliants,en papier ou carton non ondulé Boites et cartonnages,pliants,en papier ou carton non ondules Boites et cartonnagesmpliantsmen papier ou carton non ondule Boites et cartonnagres Boites et catonnages, pliants, en papier ou carton non ondule Boites, caisses et similaires en matieres plastiques Boites,caisses,casiers et similaires en matières plastiques Boites,caisses,casiers et similaires plastiques Brosses a dents y compris les brosses a Cables coaxiaux Cadres et conteneurs Cadres et conteneurs multimodaux y.c.conteneurs citernes et rervoirs CAF Vert arabica, en feves K4, Est Caf? vert arabica, en feves K4, Est Caf¿ vert arabica, en feves K3, Est CAFE Cafe arabica CAFE ARABICA Café arabica Café Arabica Café arabica k4 Café arabica k7 Café arabica, en feve K4 Café arabica,en feves k3,est Cafe arabica,en feves k4 Café arabica,en feves k4,est Cafe arabica,en feves k7 Café cert, en feves k4 (kivu4) Cafe non torrefie, non decaféiné Café non torrifié Café ver arabica k4 CAFE VERT Café vert Café vert arabica CAFE VERT AFRICA;EN FEVES K4 CAFE VERT ARAB K4 Cafe vert arabica CAFE VERT ARABICA Café vert arabica Café vert Arabica CAFE VERT ARABICA ;EN FEVES Cafe vert arabica en feves k4 café vert arabica en feve k4 Est Cafe vert arabica en feves café vert arabica en feves Cafe vert arabica en fèves Café vert arabica en fèves CAFE VERT ARABICA EN FEVES DE K4 Cafe vert arabica en feves k3 CAFE vert arabica en feves k3 Café vert arabica en feves k3 Café vert arabica en fèves K3 café vert arabica en feves k3 est Cafe vert arabica en fèves K3(kivu) cafe vert arabica en feves k4 Cafe vert arabica en feves k4 CAFE VERT ARABICA EN FEVES K4 Café vert arabica en feves k4 Café vert arabica en fèves K4 Cafe vert arabica en feves k4 est café vert arabica en feves k4 est Cafe vert arabica en fèves K4(kivu4) Cafe vert arabica en fèves k4,Est Cafe vert arabica en feves k7 Café vert arabica en feves,k3 Café vert arabica en feves,k4 Cafe vert arabica en fevesk4 Cafe vert arabica en fevres Cafe vert arabica feves k4 café vert arabica k Café vert arabica k CAFE VERT ARABICA K3 café vert arabica k3 Café vert arabica k3 Cafe vert arabica k4 CAFE VERT ARABICA k4 CAFE VERT ARABICA K4 café vert arabica k4 Café vert arabica k4 Café vert Arabica K4 Cafe vert arabica k7 CAFE VERT ARABICA K7 café vert arabica k7 Café vert arabica k7 Café vert arabica k8 Café vert arabica, en faves k4 Cafe vert arabica, en faveurs k4 Cafe vert arabica, en feve K3 Café vert arabica, en feve K3 Café vert arabica, en feve k3, est Cafe vert arabica, en feve k4 café vert arabica, en feve K4 Café vert arabica, en feve k4 Café vert arabica, en feve K4 Cafe vert arabica, en feves k3 Café vert arabica, en feves k3 Café vert arabica, en feves K3 (KIVU 3 ) Cafe vert arabica, en feves k3 (kivu 3) Cafe vert arabica, en feves k3 (kivu3) café vert arabica, en feves k3, Est Café vert arabica, en feves k3, Est Café vert arabica, en fèves K3, EST Cafe vert arabica, en feves k4 Café vert arabica, en feves k4 Cafe vert arabica, en feves k4 (kivu 4) Café vert arabica, en feves k4 (kivu 4) Café vert arabica, en feves K4 (KIVU 4) Cafe vert arabica, en feves k4 (kivu4) Café vert arabica, en feves K4 (KIVU4) Café vert arabica, en feves k4 est Cafe vert arabica, en feves K4, Est Café vert arabica, en feves k4, Est Café vert arabica, en feves K4, Est Café vert arabica, en fèves K4, EST Cafe vert arabica, en feves k5 (kivu 5) Cafe vert arabica, en feves k6 (kivu 6) Cafe vert arabica, en feves k7 (kivu 7) Café vert arabica, en feves K7 (KIVU7) Café vert arabica, en feves k7, Est Café vert arabica,en feve k4 est Café vert arabica,en feve k7 est Café vert arabica,en feves café vert arabica,en feves 4,est cafe vert arabica,en feves k3 Cafe vert arabica,en feves k3 Cafe vert arabica,en feves K3 café vert arabica,en feves k3 Café vert arabica,en feves k3 café vert arabica,en fèves k3 CAFE VERT ARABICA,EN FEVES K3(kivu) Café vert arabica,en feves K3, Est café vert arabica,en feves k3,Est Café vert arabica,en feves k3,Est Café vert arabica,en feves K3,Est cafe vert arabica,en feves k4 Cafe vert arabica,en feves k4 Café vert arabica,en feves k4 café vert arabica,en fèves k4 café vert arabica,en feves k4 est Café vert arabica,en feves k4 est Café vert arabica,en feves K4 EST CAFE VERT ARABICA,EN FEVES K4(kivi) CAFE VERT ARABICA,EN FEVES K4(kivu4) Café vert arabica,en feves k4(kivu4) Cafe vert arabica,en feves k4,est café vert arabica,en feves k4,Est café vert arabica,en feves K4,est Café vert arabica,en feves k4,est Café vert arabica,en feves K4,Est cafe vert arabica,en feves k5 Cafe vert arabica,en feves k7 Café vert arabica,en feves k7,est Café vert arabica,en feves k7,Est cafe vert arabira,en feves k3 CAFE VERT EN FEVE Cafe vert en fèves k4 Cafe vert en feves,k3 CAFE VERT K4 CAFE VERTS Cafévert arabica,en feves K4,Est cameras CAMION Camion Ben doccasion Camion Citerne doccasion CAMION FAMIL Camion renault kerax CAMION SOMUL CAOUTCHOU CAOUTCHOUC NATUREL CAOUTHOUX NATUREL Cartons vide Cartons vides CARTONS VIDES USAGES POUR RE-EMPLOIES CARTONS VIDES USAGES POUR RE-EMPLOIS casseroles en aluminium Casseterite Cassitérie Cassiteriet oxyde detain, 60-64% de Sn cassiterite Cassiterite CASSITERITE Cassitérite CASSITERITE DE 55-65 % DE SNO2 Cassiterite (oxyde detain) de 55-65% de Cassiterite de 55-65% Cassiterite de 55% - 65 % Cassiterite oxyde Cassiterite oxyde detain Cassiterite oxyde detain 55-65% Cassiterite oxyde détain de 55-59% de sn Cassitérite oxyde détain de 55-59% de Sn Cassiterite oxyde detain de 65-69 Cassiterite oxyde detain de 66-69% de Sn Cassitérite oxyde d étain de 55-59% de sn Cassiterite oxyde d étain de 65-69% de sn cassiterites Cassiterites Cassitérites oxyde detain de 55-59% de sn Cassiterites oxyde detain de 65-69% de Sn Cassiterites oxyde d etain de 55-59 CASSITERTE CASSITTERITTES CEFE VERT ARABICA Cerceuil Cereales autre que semences Cereales autres que semences Chargeurs autopropulses a chargement frontal Chars et automobiles blindees de combat et Chassis dautres vehicules automobiles des n 8701 à 8705 avec moteur CHAUDIERE ET SES ACCESSOIRES Chaudières Chocolat et préparations alimentaires du cacao Cigares(meme a bouts coupes) et CIMENT CITERNES EN ACIER coltan Coltan COLTAN Coltan-scorie stanique 20-25% Ta2O5 Coltan &amp; scorie stanique (concentres de Coltan et scorie stanique de 26-30% COLTANS COLTN Compacteurs autopropulses COMPACTEURS, NIVELEUSES compresseurs Conditionneur dair de mur puis. &lt; ou = Conserves de sprats et esprots entiers ou en morceaux non haches construction prefabriquees Constructions et parties de constructions en aluminium autre que 761010 Constructions prefabriquées Constructions prefabriques CONTAINER CUISINIERES Dechets et brisures de cafe vert arabica DECHETS ET BRISURES DE CAFE VERT ROBUSTA Dechets et brisures de café vert robusta Dechets et debris daciers allies Dechets et debris de bois Dechets et debris de fer ou dacier Dechets et debris de zirconium et ouvrages Demareurs,meme fonctionnant comme generatrice Desinfectant Désinfectants Dispositifs de fermeture Eaux conditionnées pour la table Eaux de vie de vin ou de marc de raisin EAUX MINERALES EAUX MINERALES ET EAUX GAZEIFIEES Eaux minerales et eaux gazeifiées ECAUSSINE ET AUTRES PIERRES CALCAIRES DE TAILLE OU CONSTRUCTION,ALBATRE ECHANTILLONS DES MATERIAUX GEOLOGIQUES ECORCE QUINQUINA ECORCE QUINQUINNA ECORCES DE QUINQUINA ECORCES QUINKINA ECORCES QUINQUINA EFFETS PERSONNELS Engins du n°84.29 Engrais du chapitre31 en tablettes ou emballages nexcedant pas 10 kg brut equipement concus pour la protection individuelles Equipements de sport ESCALIERS MECANIQUES ET TROTTOIRS ROULANTS Etuis,ecrins et similaires à surface exterieure en plastique ou textile Extincteurs meme charges Farine de froment (ble) ou de meteil Farine de froment ou de meteil FARINE DE MAIS Farine de moutarde et moutarde préparée Farine de moutarde preparée FARINES DE MAIS FILS DACETATE FILS DACETATE DE CELLULOSE FOURS,CUISINIERES,RECHAUDS,GRILS ET ROTISSOIRES ELECTRIQUES Goupes electrogene (diesel, semi- disiel) Groupes electrogene Groupes electrogene(diesel,semi-diesel)de puissance nexcedant pas 75kva Groupes electrogenes(diesel,semi-diesel) de GRUES SUR PNEUMATIQUE HARICOTS Haricots communs:de semences HUILE DE GRAISSAGE Huile de palme brute HUILES VEGETALES Huiles,graissages et fractions,non chimiquement modifiées Insecticides Instruments et appareils de photographie Instruments et appareils du n°90.15 IVECO TRAKKER D OCCAS LAIT EN POUDRE Lampes -reclames LAND CRUISER LAND CRUISER PRADO LAND ROVER Land rover defender 90 Levures vivantes limes,rapes et outils similaires Liqueurs titrant moins de 25? MACHINE A MELANGER MACHINE A RABOTER ET ACCESSOIRES MACHINE A SCIER ET ACCESSOIRES Machine et appareil pour la fabrication du tabac MACHINE POUR PREPARATION DU TABAC MACHINE POUR PREPARER DU TABAC MACHINE POUR TRANS DE TABAC Machines à meuler,poncer ou polir les matières du n°84.65 MACHINES A PERCER OU MORTAISER LES MATIRES DU N° 84.65 Machines et appareil Machines et appareils du n° 84.30 MACHINES ET APPAREILS POUR LA PREP OU TRANSFORMATION DU TABAC Machines et appareils pour le brochage ou la Machines geneatrices à courant alternatif de 75 kva Machines generatrices a courant alternatif Machines pr la transformation du tabac Machines qui assurent au moins deux des fonctions suivantes Machines qui assurent au moins deux des fonctions suivantes impression,copie ou transm Maghony,scies longitudinalement,sciages avives Mahagany, scies lingutudinalement, sciage avices dune epaisseur superieur a 50mm Mahagany, scies longitudinalement, sciage avives dune epaisseur inferieur à 50mm Mahogaany, scies lingitudinalement, sciagaes avives dune epaisseur superieur a 50mm Mahogany, scies lingitudianlement, sciages avives dune epaisseur superieure a 50mm Mahogany,scies lingitudinalement,sciages avives Mahogany,scies lingitudinalement,sciages avives d une epaisseur superieure a 50 mm Mahogany,scies longitudinalement Mahogany,sciés longitudinalement Mahogany,scies longitudinalement,sciages avives Mahogany,scies longitudinalement,sciages avives dune epaisseur Mahogany,scies longitudinalement,sciages avives dune épaisseur inf à 50 mm Mahogany,scies longitudinalement,sciages avives dune epaisseur inférieure à 50 mm Mahogany,scies longitudinalement,sciages avives d une epaisseur inferieure à 50 mm Malles,valises,mallettes à surface exter en plastique Margarine, a lexclusion de la margarine Margarine,a l exclusion de la margarine liquide marteaux et masses Matériel déchafaudage,de coffrage ou détayage Materiel dechafaudage,de coffrage ou detayge en fonte,fer ou acier MATERIEL POUR LA GYMNASTIQUE Materiels dechafaudage,de coffrage ou detayage en fonte,fer ou acier MATIERES COLORANTES Matières colorantes MEDICAMENTS Melanges depices MEUBLES ET LEURS PARTIES MIERAIS DE COLTAN ET SCORIE STANIQUE DE 26-30%TA05 Minerais autres que ceux des n 26171000 a 26179091 MINERAIS CASSITERITES Minerais detain et leurs concentré Minerais detain et leurs concentré dune teneur de 55 à 65 % en etain Minerais detain et leurs concentres minerais detain et leurs concentres dune teneur de 55 à 65 Minerais détain et leurs concentrés d une teneur de 55 à 65% en etain/cassiterite Minerais d étain et leurs concentrés d une teneur de 55 à 65% en étain Minerais d étain et leurs concentrés dune teneur de 55% à 65% MINERAIS DE COLTA § SCORIE STANIQUE DE + DE 35 Minerais de coltan MINERAIS DE COLTAN Minérais de coltan Minerais de coltan &amp; scorie de 26-30 % Tao5 Minerais de coltan &amp; scorie stanique Minerais de coltan &amp; scorie stanique de Minerais de Coltan &amp; scorie stanique de Minerais de coltan &amp; scorie stanique de 26-30 % Ta05 MINERAIS DE COLTAN &amp; SCORIE STANIQUE DE 26-30% Minerais de coltan &amp; scorie stanique de 26-30% Ta 05 Minerais de coltan &amp; scorie stanique de 26-30% Ta05 Minerais de coltan &amp; scorie stanique de 26-30% Tao5 Minerais de coltan &amp; scorie stanique de 26-30% TaO5 Minerais de Coltan &amp; scorie stanique de 26-30% TaO5 Minerais de coltan &amp; scorie stanique de 26-30%Ta05 Minerais de coltan &amp; scorie stanique de 4-14% Ta05 Minerais de coltan &amp; scories stanique de 26-30% MINERAIS DE COLTAN &amp;SCORIE STANIQUE DE 26-30 %Ta05 MINERAIS DE COLTAN &amp;SCORIE STANIQUE DE 26-30% Minerais de coltan &amp;scorie stanique de 26-30%TaO5 Minerais de coltan et scorie Minerais de coltan et scorie stanique MINERAIS DE COLTAN ET SCORIE STANIQUE Minerais de coltan et scorie stanique de 26-30% Minerais de coltan et scorie stanique de 26-30% Ta05 Minerais de coltan et scorie stanique de 26-30%Ta05 Minerais de coltan et scorie stanique de 4-14% ta05 Minérais de coltan et scories MINERAIS DE COLTAN&amp;SCORIE STANIQUE Minerais de coltant &amp; scorie stanique de 26-30% Ta05 MINERAIS DE TANTALE Minérais de tantale Minerais de tantale et leurs concentres Minerais de tantale teneur 26-30% Minerais de tantale teneur 26-30% tantale et 40-59% oxyde niobium ou colombite MINERAIS DE TANTALE TENEUR 26-30% TANTALE ET 40-59% OXYDE NIOBIUM OUCOLOM Minerais de tantale teneur 26-30%tentale et 40-59% oxyde niobium ou colombite Minerais de tantale teneure 26-30% et 40-59% oxyde niobium ou colombite Minerais et leurs concentres Minerais et leurs concentrés d une teneur de 55 à 65% en etain/cassiterite Mineraus de coltant &amp; scorie stanique de 26-30% Ta05 Mitraille de fer Mitrailles de fer Mitralles de fer Mohogany, scies longitudinalement, sciages avives dune epaisseur inferieur à 50mm Moteur Aviation Moteurs à explosion pour vehicule du chapitre 87 Moteurs universels de puissance excedant 37,5w MOTOCYCLES Motos et lubrifiant Navets,betteraves,celeris-raves et similaires frais refrigeres NISSAN VANETTE Niveleuses autopropulsées Non decafeine OBJET DE DEMEN OBJET DE DEMENAGEMENT ufs Or a usages non monetaires dexploitation OR ARTISANAL Oxygene Palettes simples, palettes-caisses et autres PAPIER A CIGARETTE EN ROULEAUX DUNE LARGEUR NEXCEDANT PAS 5 CM Papier a cigarettes en rouleaux dune Papiers ndnca Papiers non denommes ni compris ailleurs Parfuns et eaux de toillette titrant - de 50 Parties de machines PARTIES DES MACHINES ET APPAREILS DU N 84.74 Parties des machines et appareils du n° 84.53 PARTIES ET ACC VEHICULE Parties et accessoires PARTIES ET ACCESSOIRES DES MACHINES Parties et accessoires des vehicules des 87.01 à 87,05 Parties et accessoires des vehicules des n° 87.11 à 87.13 Parties et accessoires des vehicules des n°87.01 à 87.05 PARTIES RECONNAISSABLES DES MACHINES DES N°85.01 OU 85.02 PC PEAUX PEAUX DE BETTES Peaux de bovins PEAUX DES BEBES Peaux des betes PEAUX DES BETTES Petrol lampant sans biodiesel photocopie Pieres synthétiques ou reconstituees Pieres synthetiques ou reconstituées Pierres de couleur Pierres de taille ou construction Pierres synth ou reconstituées PIOCHES,PICS,HOUES,BINETTES,RATEAUX ET RACLOIRS Plantes médecinales Plantes medicales Plaques,feuilles,bandes,rubans,pellicules Pneumatiques Pneumatiques rechapes pour auto-bus ou camion Poivre non broye ni pulverise Pompe à vide pompes a carburant Pompes à injection pour moteurs à explosion PREPARATIONS ALIMENTAIRES NDCA Préparations alimentaires ndca Preparations pour enfants,conditionnees pour Préparations soufflées ou grillées a base de cereales Préparations,pour sauces,sauces,condiments et assaisonnements PRODUIT DE BEAUTE Produits de beaute Produits de beaute,de maquillage,solaires ou pour la peau propulseurs a reaction autres que les turboreacteurs quinquina Quinquina QUINQUINA Recipients pour gaz comprimes,liquefiés,en fonte,fer ou acier REFRIGERATEUR Reservoirs, futs, tambours, bidons, dune Reservoirs,foudres,cuves et autres(+300)en matières plastiques Riz semi-blanchi ou blanchi, meme poli ou Sacs et sachets de mballage en autres synthétiques ou articles Sacs et sachets demballage en autres sythetiques ou artificiels Sacs et sachets en bonneterie de jute ou autres fibres Sacs,sachets,pochettes,cornets en polyethylene Salmonides entiers,congeles SCORIE Scorpio 4x4 Mihindra Sel SEL IODE Slips et caleçons pour hommes ou garçonnets en bonneterie de coton Sucres de canne Sucs et extraits végétaux de houblon Tabac partiellement ecotes TABACS TABACS ECOTES TABACS PARTIELLEMENT ECOTES The noir (fermente), en emballage TILAPIAS Tissus Imprimés Tours et pylones en fontes Toyota ambulance TOYOTA DINA Agés de plus de 5 ans Toyota Hilux Toyota hilux surf TOYOTA ID Toyota Land cruiser TOYOTA LAND CRUISER Toyota pick up Toyota Prado tx TOYOTA RAV4 DOCCASION Treuils cabestans Treuils cabestants TROTTINETTES(CHUKUDU) Tungstene vaisselle et art de tables et de cuisine en mat plastiques Vaisselle et art de tables et de cuisine en matières plast Véhicule VEHICULE AUTO Vehicule automobile VEHICULE KIA SORENTO VEHICULE LAND ROVER Vehicules automobiles a usages speciaux Vetements neuf vetements usage Vis et boulons filetes en fonte,fer ou acier WOLFRAM WOLFRAMI Wolframite WOLFRAMITE La variables Goods a 740 modalités Faison la cactérisation des niveaux des marchandises dont lencodage fait défaut class(taxe_df$Goods) ## [1] &quot;character&quot; Usage de tm et Stringr ## Warning: Unknown levels in `f`: equipements protection ## [1] &quot;Autres Marchandises&quot; ## [2] &quot;Bois&quot; ## [3] &quot;Machines et appareils domestique&quot; ## [4] &quot;Médicaments et plantes médécinales&quot; ## [5] &quot;Poissaons, viande et oeufs&quot; ## [6] &quot;Matériels de construction&quot; ## [7] &quot;Materiel Informatique et Electroniques&quot; ## [8] &quot;Véhicules,camions,Motos et acc&quot; ## [9] &quot;Vetements,tissus et acc et chaussure&quot; ## [10] &quot;boissons, bières et limonades&quot; ## [11] &quot;Machine us Ingsutriel&quot; ## [12] &quot;Article Menange et Campement&quot; ## [13] &quot;sacs, sachetsn emballages&quot; ## [14] &quot;Papiers et fournitures de bureaux&quot; ## [15] &quot;Produits alimentaires,prep et huiles&quot; ## [16] &quot;caféarabica&quot; ## [17] &quot;Minérais et dérivés&quot; ## [18] &quot;engins et tracteurs&quot; ## [19] &quot;Cigarette et papier cigarettes et tabac&quot; ## [20] &quot;constructionprefabriquees&quot; ## [21] &quot;cadreset conteneurs&quot; ## [22] &quot;Pièces de Réchange appareils&quot; ## [23] &quot;Générateurs,baterie et piles&quot; ## [24] &quot;etuis en plastique ou textile&quot; ## [25] &quot;Pétrole et dérivées et huile de graissage&quot; ## [26] &quot;boissons, bières,liqueurs et limonades&quot; ## [27] &quot;produits beaute&quot; ## [28] &quot;peauxdes betes&quot; ## n % val% ## Autres Marchandises 160 4.8 4.8 ## Bois 140 4.2 4.2 ## Machines et appareils domestique 30 0.9 0.9 ## Médicaments et plantes médécinales 34 1.0 1.0 ## Poissaons, viande et oeufs 12 0.4 0.4 ## Matériels de construction 27 0.8 0.8 ## Materiel Informatique et Electroniques 32 1.0 1.0 ## Véhicules,camions,Motos et acc 113 3.4 3.4 ## Vetements,tissus et acc et chaussure 100 3.0 3.0 ## boissons, bières et limonades 15 0.5 0.5 ## Machine us Ingsutriel 54 1.6 1.6 ## Article Menange et Campement 37 1.1 1.1 ## sacs, sachetsn emballages 6 0.2 0.2 ## Papiers et fournitures de bureaux 24 0.7 0.7 ## Produits alimentaires,prep et huiles 68 2.1 2.1 ## caféarabica 1303 39.4 39.5 ## Minérais et dérivés 1053 31.8 31.9 ## engins et tracteurs 18 0.5 0.5 ## Cigarette et papier cigarettes et tabac 14 0.4 0.4 ## constructionprefabriquees 7 0.2 0.2 ## cadreset conteneurs 1 0.0 0.0 ## Pièces de Réchange appareils 6 0.2 0.2 ## Générateurs,baterie et piles 15 0.5 0.5 ## etuis en plastique ou textile 1 0.0 0.0 ## Pétrole et dérivées et huile de graissage 4 0.1 0.1 ## boissons, bières,liqueurs et limonades 2 0.1 0.1 ## produits beaute 10 0.3 0.3 ## peauxdes betes 14 0.4 0.4 ## NA 10 0.3 NA netoyyage de la variable country_desti qui est un facteur dans le quel nous retrouvons les niveaux rédondants (sur lidentifiant des pays) ## Warning: Unknown levels in `f`: République Populaire démocra ## [1] &quot;AFRIQUE DU SUD&quot; ## [2] &quot;ALGERIE&quot; ## [3] &quot;ALLEMAGNE&quot; ## [4] &quot;AMERIQUE LATINE&quot; ## [5] &quot;GRANDE BRATAGNE&quot; ## [6] &quot;ANGOLA&quot; ## [7] &quot;ARABIE&quot; ## [8] &quot;ASIE&quot; ## [9] &quot;AUSTRALIE&quot; ## [10] &quot;BELGIQUE&quot; ## [11] &quot;BURUNDI&quot; ## [12] &quot;CANADA&quot; ## [13] &quot;CHINE&quot; ## [14] &quot;CHYPRE&quot; ## [15] &quot;CONGO BRAZA&quot; ## [16] &quot;REP TCHEQUE&quot; ## [17] &quot;NA&quot; ## [18] &quot;EMIRATES ARABES UNIES&quot; ## [19] &quot;ESPAGNE&quot; ## [20] &quot;FRANCE&quot; ## [21] &quot;GABON&quot; ## [22] &quot;GRECE&quot; ## [23] &quot;HONG KONG&quot; ## [24] &quot;ILE MAURICE&quot; ## [25] &quot;INDE&quot; ## [26] &quot;ITALIE&quot; ## [27] &quot;JAPON&quot; ## [28] &quot;KENYA&quot; ## [29] &quot;KP - Corée, République Populaire démocra&quot; ## [30] &quot;LIBAN&quot; ## [31] &quot;LUXEMBOURG&quot; ## [32] &quot;MALAISIE&quot; ## [33] &quot;MAROC&quot; ## [34] &quot;NERLAND&quot; ## [35] &quot;PAYS BAS&quot; ## [36] &quot;NIGERIA&quot; ## [37] &quot;NOUVELLE ZELANDE&quot; ## [38] &quot;OUGANDA&quot; ## [39] &quot;PANAMA&quot; ## [40] &quot;PHILLIPINE&quot; ## [41] &quot;POLOGNE&quot; ## [42] &quot;PORTUGAL&quot; ## [43] &quot;ROYAUME UNI&quot; ## [44] &quot;RDC&quot; ## [45] &quot;USA&quot; ## [46] &quot;RWANDA&quot; ## [47] &quot;SINGAPOUR&quot; ## [48] &quot;SUISSE&quot; ## [49] &quot;SENEGAL&quot; ## [50] &quot;SOMALIE&quot; ## [51] &quot;SOUDAN&quot; ## [52] &quot;SUD SOUDAN&quot; ## [53] &quot;SUEDE&quot; ## [54] &quot;Swaziland&quot; ## [55] &quot;TANZANIE&quot; ## [56] &quot;TCHAD&quot; ## [57] &quot;THAILANDE&quot; ## [58] &quot;UNION EUROPEENNE&quot; ## [59] &quot;ZAMBIE&quot; Dans la base des données il y a des entreprises que lon a enregistré à la place des pays. ces genre des cas ont été traité par remplacement avec le NA pour Not Available et ces dernier on été élargués de la base des données, car nous avions jugé qu aucune méthode dimputation nest applicable pour ce genre de situation. Nous avions fait la même chose pour les variables tels que Les marchandises. 6.1.1 Nouvelle base de données pour les analyses Regroupement des variables pour la synthèse pour rendre la base des données simple à exploiter, éliminer les NA dans les observations telsque les pays et les valeurs pour les marchandises et les taxes. DBase &lt;- taxe_df %&gt;% rename(&quot;Destination_Country&quot;=`Country/destination`) DBase &lt;- DBase %&gt;% select(Year,Destination_Country,Goods,Weight,Taxe) %&gt;% group_by(Year,Destination_Country,Goods) %&gt;% summarise(Weight=sum(Weight),Taxe=sum(Taxe),.groups = &quot;drop&quot;) %&gt;% drop_na() correlate(DBase) %&gt;% kable(caption = &quot;Table de corrélation entre les variables quantitatives&quot;) Table 6.3: Table de corrélation entre les variables quantitatives var1 var2 coef_corr Weight Year -0.1727414 Taxe Year -0.1965648 Year Weight -0.1727414 Taxe Weight 0.6699457 Year Taxe -0.1965648 Weight Taxe 0.6699457 #plot_correlate(DBase) df &lt;- pdata.frame(DBase,index = c(&quot;Year&quot;,&quot;Destination_Country&quot;)) ## Warning in pdata.frame(DBase, index = c(&quot;Year&quot;, &quot;Destination_Country&quot;)): duplicate couples (id-time) in resulting pdata.frame ## to find out which, use, e.g., table(index(your_pdataframe), useNA = &quot;ifany&quot;) DF &lt;- df %&gt;% pivot_wider(names_from = Goods, values_from = c(Taxe,Weight)) DF &lt;- DF %&gt;% mutate_all(~replace(.,is.na(.),0)) ## Warning in `[&lt;-.factor`(`*tmp*`, list, value = 0): invalid factor level, NA ## generated ## Warning in `[&lt;-.factor`(`*tmp*`, list, value = 0): invalid factor level, NA ## generated View(DF) 6.2 Agregation des données avec la méthode reduce DF_aggreg &lt;- DF %&gt;% mutate(Tax=reduce(select(.,starts_with(&quot;Taxe&quot;)),`+`)) DF_aggreg &lt;- DF_aggreg %&gt;% mutate(W=reduce(select(.,starts_with(&quot;Weight&quot;)),`+`)) DF_aggreg &lt;- DF_aggreg %&gt;% select(-starts_with(&quot;Taxe&quot;),-starts_with(&quot;Weight&quot;)) DF_aggreg &lt;- DF_aggreg %&gt;% rename(&quot;Taxes&quot;=&quot;Tax&quot;,&quot;Weight&quot;=&quot;W&quot;) p_DF_aggreg &lt;- DF_aggreg %&gt;% pdata.frame(index = c(&quot;Destination_Country&quot;,&quot;Year&quot;)) 6.3 Analyse descriptive des Varariales Conversion des données en modèle des panels des données DBase$Destination_Country &lt;- DBase$Destination_Country %&gt;% factor() DBase$Destination_Country %&gt;% levels() ## [1] &quot;AFRIQUE DU SUD&quot; ## [2] &quot;ALGERIE&quot; ## [3] &quot;ALLEMAGNE&quot; ## [4] &quot;AMERIQUE LATINE&quot; ## [5] &quot;GRANDE BRATAGNE&quot; ## [6] &quot;ANGOLA&quot; ## [7] &quot;ARABIE&quot; ## [8] &quot;ASIE&quot; ## [9] &quot;AUSTRALIE&quot; ## [10] &quot;BELGIQUE&quot; ## [11] &quot;BURUNDI&quot; ## [12] &quot;CANADA&quot; ## [13] &quot;CHINE&quot; ## [14] &quot;CHYPRE&quot; ## [15] &quot;CONGO BRAZA&quot; ## [16] &quot;REP TCHEQUE&quot; ## [17] &quot;NA&quot; ## [18] &quot;EMIRATES ARABES UNIES&quot; ## [19] &quot;ESPAGNE&quot; ## [20] &quot;FRANCE&quot; ## [21] &quot;GABON&quot; ## [22] &quot;GRECE&quot; ## [23] &quot;HONG KONG&quot; ## [24] &quot;ILE MAURICE&quot; ## [25] &quot;INDE&quot; ## [26] &quot;ITALIE&quot; ## [27] &quot;JAPON&quot; ## [28] &quot;KENYA&quot; ## [29] &quot;KP - Corée, République Populaire démocra&quot; ## [30] &quot;LUXEMBOURG&quot; ## [31] &quot;MALAISIE&quot; ## [32] &quot;MAROC&quot; ## [33] &quot;NERLAND&quot; ## [34] &quot;PAYS BAS&quot; ## [35] &quot;NIGERIA&quot; ## [36] &quot;NOUVELLE ZELANDE&quot; ## [37] &quot;OUGANDA&quot; ## [38] &quot;PANAMA&quot; ## [39] &quot;PHILLIPINE&quot; ## [40] &quot;POLOGNE&quot; ## [41] &quot;PORTUGAL&quot; ## [42] &quot;ROYAUME UNI&quot; ## [43] &quot;RDC&quot; ## [44] &quot;USA&quot; ## [45] &quot;RWANDA&quot; ## [46] &quot;SINGAPOUR&quot; ## [47] &quot;SUISSE&quot; ## [48] &quot;SENEGAL&quot; ## [49] &quot;SOMALIE&quot; ## [50] &quot;SOUDAN&quot; ## [51] &quot;SUD SOUDAN&quot; ## [52] &quot;SUEDE&quot; ## [53] &quot;Swaziland&quot; ## [54] &quot;TANZANIE&quot; ## [55] &quot;TCHAD&quot; ## [56] &quot;THAILANDE&quot; ## [57] &quot;UNION EUROPEENNE&quot; ## [58] &quot;ZAMBIE&quot; "],["modele-poooling.html", "Chapter 7 Modele poooling", " Chapter 7 Modele poooling Modèle à polaire, considérant les deux éffets ## Pooling Model ## ## Call: ## plm(formula = Taxes ~ Weight, data = p_DF_aggreg, model = &quot;pooling&quot;) ## ## Unbalanced Panel: n = 58, T = 1-10, N = 228 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -76127299 -3525554 -38253 0 822609 135485918 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) -8.7251e+05 1.3302e+06 -0.6559 0.5125 ## Weight 4.1859e+01 2.8796e+00 14.5367 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 1.37e+17 ## Residual Sum of Squares: 7.0803e+16 ## R-Squared: 0.48321 ## Adj. R-Squared: 0.48092 ## F-statistic: 211.315 on 1 and 226 DF, p-value: &lt; 2.22e-16 Modele à éffet aléatoire ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = Taxes ~ Weight, data = p_DF_aggreg, model = &quot;within&quot;) ## ## Unbalanced Panel: n = 58, T = 1-10, N = 228 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -69551626 -1715042 0 0 1537006 114103679 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## Weight 39.6087 3.8914 10.178 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 9.7404e+16 ## Residual Sum of Squares: 6.0386e+16 ## R-Squared: 0.38005 ## Adj. R-Squared: 0.16728 ## F-statistic: 103.601 on 1 and 169 DF, p-value: &lt; 2.22e-16 Le modèle à éffets aléatoire ## Oneway (individual) effect Between Model ## ## Call: ## plm(formula = Taxes ~ Weight, data = p_DF_aggreg, model = &quot;between&quot;) ## ## Unbalanced Panel: n = 58, T = 1-10, N = 228 ## Observations used in estimation: 58 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -17710356 -1366186 300948 0 907729 22825793 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) -9.2150e+05 9.0924e+05 -1.0135 0.3152 ## Weight 3.8816e+01 3.8643e+00 10.0449 3.886e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 5.1437e+15 ## Residual Sum of Squares: 1.8358e+15 ## R-Squared: 0.64309 ## Adj. R-Squared: 0.63671 ## F-statistic: 100.9 on 1 and 56 DF, p-value: 3.886e-14 "],["references-1.html", "References", " References "],["final-words.html", "Chapter 8 Final Words", " Chapter 8 Final Words We have finished a nice book. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
